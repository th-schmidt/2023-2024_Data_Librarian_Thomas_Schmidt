{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18b32f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a013b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.datacite.org/application/vnd.datacite.datacite+json/\"\n",
    "doi = \"10.5281/zenodo.5563162\"\n",
    "full_url = base_url + doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab571362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = urllib.request.urlopen(full_url)\n",
    "testing.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e66abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = urllib.request.urlretrieve(full_url, 'jsonpd.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dee4a4f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(raw_data[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/json/_json.py:784\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/json/_json.py:975\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 975\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[1;32m    978\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[1;32m    979\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/json/_json.py:1001\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    999\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1001\u001b[0m     obj \u001b[38;5;241m=\u001b[39m FrameParser(json, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/json/_json.py:1134\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse()\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/json/_json.py:1319\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1316\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[1;32m   1320\u001b[0m         loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     )\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1323\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1324\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1325\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1326\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(raw_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc3d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_json_dataset = urllib.request.urlopen(full_url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c298941",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_dataset = json.loads(doi_json_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8343ae06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'https://doi.org/10.5281/zenodo.5563162',\n",
       " 'doi': '10.5281/ZENODO.5563162',\n",
       " 'url': 'https://zenodo.org/record/5563162',\n",
       " 'types': {'ris': 'DATA',\n",
       "  'bibtex': 'misc',\n",
       "  'citeproc': 'dataset',\n",
       "  'schemaOrg': 'Dataset',\n",
       "  'resourceTypeGeneral': 'Dataset'},\n",
       " 'creators': [{'name': 'Banda, Juan M.',\n",
       "   'givenName': 'Juan M.',\n",
       "   'familyName': 'Banda',\n",
       "   'affiliation': [{'name': 'Georgia State University'}],\n",
       "   'nameIdentifiers': [{'schemeUri': 'https://orcid.org',\n",
       "     'nameIdentifier': 'https://orcid.org/0000-0001-8499-824X',\n",
       "     'nameIdentifierScheme': 'ORCID'}]},\n",
       "  {'name': 'Tekumalla, Ramya',\n",
       "   'givenName': 'Ramya',\n",
       "   'familyName': 'Tekumalla',\n",
       "   'affiliation': [{'name': 'Georgia State University'}],\n",
       "   'nameIdentifiers': [{'schemeUri': 'https://orcid.org',\n",
       "     'nameIdentifier': 'https://orcid.org/0000-0002-1606-4856',\n",
       "     'nameIdentifierScheme': 'ORCID'}]},\n",
       "  {'name': 'Wang, Guanyu',\n",
       "   'givenName': 'Guanyu',\n",
       "   'familyName': 'Wang',\n",
       "   'affiliation': [{'name': 'University of Missouri'}]},\n",
       "  {'name': 'Yu, Jingyuan',\n",
       "   'givenName': 'Jingyuan',\n",
       "   'familyName': 'Yu',\n",
       "   'affiliation': [{'name': 'Universitat Autònoma de Barcelona'}]},\n",
       "  {'name': 'Liu, Tuo',\n",
       "   'givenName': 'Tuo',\n",
       "   'familyName': 'Liu',\n",
       "   'affiliation': [{'name': 'Carl von Ossietzky Universität Oldenburg'}]},\n",
       "  {'name': 'Ding, Yuning',\n",
       "   'givenName': 'Yuning',\n",
       "   'familyName': 'Ding',\n",
       "   'affiliation': [{'name': 'Universität Duisburg-Essen'}]},\n",
       "  {'name': 'Artemova, Katya',\n",
       "   'givenName': 'Katya',\n",
       "   'familyName': 'Artemova',\n",
       "   'affiliation': [{'name': 'NRU HSE'}]},\n",
       "  {'name': 'Tutubalina, Elena',\n",
       "   'givenName': 'Elena',\n",
       "   'familyName': 'Tutubalina',\n",
       "   'affiliation': [{'name': 'KFU'}]},\n",
       "  {'name': 'Chowell, Gerardo',\n",
       "   'givenName': 'Gerardo',\n",
       "   'familyName': 'Chowell',\n",
       "   'affiliation': [{'name': 'Georgia State University'}],\n",
       "   'nameIdentifiers': [{'schemeUri': 'https://orcid.org',\n",
       "     'nameIdentifier': 'https://orcid.org/0000-0003-2194-2251',\n",
       "     'nameIdentifierScheme': 'ORCID'}]}],\n",
       " 'titles': [{'title': 'A large-scale COVID-19 Twitter chatter dataset for open scientific research - an international collaboration'}],\n",
       " 'publisher': 'Zenodo',\n",
       " 'container': {'type': 'DataRepository',\n",
       "  'identifier': 'https://zenodo.org/communities/biohackathon',\n",
       "  'identifierType': 'URL'},\n",
       " 'subjects': [{'subject': 'social media'},\n",
       "  {'subject': 'twitter'},\n",
       "  {'subject': 'nlp'},\n",
       "  {'subject': 'covid-19'},\n",
       "  {'subject': 'covid19'}],\n",
       " 'contributors': [],\n",
       " 'dates': [{'date': '2021-10-10', 'dateType': 'Issued'}],\n",
       " 'publicationYear': 2021,\n",
       " 'language': 'en',\n",
       " 'identifiers': [{'identifier': 'https://github.com/thepanacealab/covid19_twitter',\n",
       "   'identifierType': 'URL'},\n",
       "  {'identifier': 'https://zenodo.org/record/5563162',\n",
       "   'identifierType': 'URL'}],\n",
       " 'sizes': [],\n",
       " 'formats': [],\n",
       " 'version': '83',\n",
       " 'rightsList': [{'rights': 'Open Access',\n",
       "   'rightsUri': 'info:eu-repo/semantics/openAccess'}],\n",
       " 'descriptions': [{'description': '<em><strong>Version 83 of the dataset. The peer-reviewed publication for this dataset has now been published in Epidemiologia an MDPI journal, and can be accessed here: https://doi.org/10.3390/epidemiologia2030024. Please cite this when using the dataset.</strong></em> <strong>Due to the relevance of the COVID-19 global pandemic, we are releasing our dataset of tweets acquired from the Twitter Stream related to COVID-19 chatter. Since our first release we have received additional data from our new collaborators, allowing this resource to grow to its current size. Dedicated data gathering started from March 11th yielding over 4 million tweets a day. We have added additional data provided by our new collaborators from January 27th to March 27th, to provide extra longitudinal coverage. Version 10 added ~1.5 million tweets in the Russian language collected between January 1st and May 8th, gracefully provided to us by: Katya Artemova (NRU HSE) and Elena Tutubalina (KFU). From version 12 we have included daily hashtags, mentions and emoijis and their frequencies the respective zip files. From version 14 <em>we</em> have included the tweet identifiers and their respective language for the clean version of the dataset. Since version 20 we have included language and place location for all tweets.</strong> <strong>The data collected from the stream captures all languages, but the higher prevalence are: English, Spanish, and French. We release all tweets and retweets on the full_dataset.tsv file (1,222,981,652 unique tweets), and a cleaned version with no retweets on the full_dataset-clean.tsv file (313,558,225 unique tweets). There are several practical reasons for us to leave the retweets, tracing important tweets and their dissemination is one of them. For NLP tasks we provide the top 1000 frequent terms in frequent_terms.csv, the top 1000 bigrams in frequent_bigrams.csv, and the top 1000 trigrams in frequent_trigrams.csv. Some general statistics per day are included for both datasets in the full_dataset-statistics.tsv and full_dataset-clean-statistics.tsv files. For more statistics and some visualizations visit: http://www.panacealab.org/covid19/ </strong> <strong>More details can be found (and will be updated faster at: https://github.com/thepanacealab/covid19_twitter) and our pre-print about the dataset (https://arxiv.org/abs/2004.03688) </strong> <strong>As always, the tweets distributed here are only tweet identifiers (with date and time added) due to the terms and conditions of Twitter to re-distribute Twitter data ONLY for research purposes. They need to be hydrated to be used.</strong>',\n",
       "   'descriptionType': 'Abstract'},\n",
       "  {'description': 'This dataset will be updated bi-weekly at least with additional tweets, look at the github repo for these updates. Release: We have standardized the name of the resource to match our pre-print manuscript and to not have to update it every week.',\n",
       "   'descriptionType': 'Other'}],\n",
       " 'geoLocations': [],\n",
       " 'fundingReferences': [],\n",
       " 'relatedIdentifiers': [{'relationType': 'IsContinuedBy',\n",
       "   'relatedIdentifier': 'http://www.panacealab.org/covid19/',\n",
       "   'resourceTypeGeneral': 'Other',\n",
       "   'relatedIdentifierType': 'URL'},\n",
       "  {'relationType': 'IsSupplementTo',\n",
       "   'relatedIdentifier': 'https://arxiv.org/abs/2004.03688',\n",
       "   'resourceTypeGeneral': 'Preprint',\n",
       "   'relatedIdentifierType': 'URL'},\n",
       "  {'relationType': 'IsVersionOf',\n",
       "   'relatedIdentifier': '10.5281/zenodo.3723939',\n",
       "   'relatedIdentifierType': 'DOI'},\n",
       "  {'relationType': 'IsPartOf',\n",
       "   'relatedIdentifier': 'https://zenodo.org/communities/biohackathon',\n",
       "   'relatedIdentifierType': 'URL'},\n",
       "  {'relationType': 'IsPartOf',\n",
       "   'relatedIdentifier': 'https://zenodo.org/communities/covid-19',\n",
       "   'relatedIdentifierType': 'URL'},\n",
       "  {'relationType': 'IsPartOf',\n",
       "   'relatedIdentifier': 'https://zenodo.org/communities/zenodo',\n",
       "   'relatedIdentifierType': 'URL'}],\n",
       " 'relatedItems': [],\n",
       " 'schemaVersion': 'http://datacite.org/schema/kernel-4',\n",
       " 'providerId': 'cern',\n",
       " 'clientId': 'cern.zenodo',\n",
       " 'agency': 'datacite',\n",
       " 'state': 'findable'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffdc9d0",
   "metadata": {},
   "source": [
    "## keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f63fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'doi', 'url', 'types', 'creators', 'titles', 'publisher', 'container', 'subjects', 'contributors', 'dates', 'publicationYear', 'language', 'identifiers', 'sizes', 'formats', 'version', 'rightsList', 'descriptions', 'geoLocations', 'fundingReferences', 'relatedIdentifiers', 'relatedItems', 'schemaVersion', 'providerId', 'clientId', 'agency', 'state'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7daa1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get *publicationYear*\n",
    "doi_dataset['publicationYear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24ab999f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large-scale COVID-19 Twitter chatter dataset for open scientific research - an international collaboration'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get *title*\n",
    "doi_dataset['titles'][0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dbcecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large-scale COVID-19 Twitter chatter dataset for open scientific research - an international collaboration\n"
     ]
    }
   ],
   "source": [
    "print(doi_dataset['titles'][0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba79a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Banda, Juan M.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get *creators*\n",
    "doi_dataset['creators'][0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665e1d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doi_dataset['creators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c333364d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doi_dataset['creators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d16ddd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banda, Juan M.\n",
      "Tekumalla, Ramya\n",
      "Wang, Guanyu\n",
      "Yu, Jingyuan\n",
      "Liu, Tuo\n",
      "Ding, Yuning\n",
      "Artemova, Katya\n",
      "Tutubalina, Elena\n",
      "Chowell, Gerardo\n"
     ]
    }
   ],
   "source": [
    "for creator in doi_dataset['creators']:\n",
    "    print(creator['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea18956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name', 'givenName', 'familyName', 'affiliation', 'nameIdentifiers'])\n",
      "dict_keys(['name', 'givenName', 'familyName', 'affiliation', 'nameIdentifiers'])\n",
      "dict_keys(['name', 'givenName', 'familyName', 'affiliation'])\n",
      "dict_keys(['name', 'givenName', 'familyName', 'affiliation'])\n",
      "dict_keys(['name', 'givenName', 'familyName', 'affiliation'])\n",
      "dict_keys(['name', 'givenName', 'familyName', 'affiliation'])\n",
      "dict_keys(['name', 'givenName', 'familyName', 'affiliation'])\n",
      "dict_keys(['name', 'givenName', 'familyName', 'affiliation'])\n",
      "dict_keys(['name', 'givenName', 'familyName', 'affiliation', 'nameIdentifiers'])\n"
     ]
    }
   ],
   "source": [
    "for creator in doi_dataset['creators']:\n",
    "    print(creator.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0aacb",
   "metadata": {},
   "source": [
    "## Übung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6cee027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste mit DOIs\n",
    "# Titel, DOI, Publisher ausgeben\n",
    "\n",
    "dois = [\"10.6084/m9.figshare.155613\",\n",
    "        \"10.6084/m9.figshare.153821.v1\",\n",
    "        \"10.7490/f1000research.1115338.1\",\n",
    "        \"10.5281/zenodo.2599866\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "522a01a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI: 10.6084/M9.FIGSHARE.155613\n",
      "Title: git repository for paper on git and reproducible science\n",
      "Publisher: figshare\n",
      "\n",
      "DOI: 10.6084/M9.FIGSHARE.153821.V1\n",
      "Title: git can facilitate greater reproducibility and increased transparency in science\n",
      "Publisher: figshare\n",
      "\n",
      "DOI: 10.7490/F1000RESEARCH.1115338.1\n",
      "Title: The role of the German National Library for Life Sciences ZB MED in the approach to a FAIR Research Data Infrastructure in Agricultural Science embedded in the Life Sciences\n",
      "Publisher: F1000 Research Limited\n",
      "\n",
      "DOI: 10.5281/ZENODO.2599866\n",
      "Title: Nachnutzbare Awarenessmaterialien für Forschungsdatenmanagement (FDM)\n",
      "Publisher: Zenodo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doi in dois:\n",
    "    # Get full_url\n",
    "    full_url = base_url + doi\n",
    "    \n",
    "    # Parse full_url\n",
    "    doi_json_dataset = urllib.request.urlopen(full_url).read()\n",
    "    doi_dataset = json.loads(doi_json_dataset)\n",
    "    \n",
    "    # Print doi, title, publisher\n",
    "    print(f\"DOI: {doi_dataset['doi']}\")\n",
    "    print(f\"Title: {doi_dataset['titles'][0]['title']}\")\n",
    "    print(f\"Publisher: {doi_dataset['publisher']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4ba08ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.datacite.org/application/vnd.datacite.datacite+json/10.6084/m9.figshare.155613\n",
      "https://api.datacite.org/application/vnd.datacite.datacite+json/10.6084/m9.figshare.153821.v1\n",
      "https://api.datacite.org/application/vnd.datacite.datacite+json/10.7490/f1000research.1115338.1\n",
      "https://api.datacite.org/application/vnd.datacite.datacite+json/10.5281/zenodo.2599866\n"
     ]
    }
   ],
   "source": [
    "for doi in dois:\n",
    "    full_url = base_url + doi\n",
    "    print(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cc22b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
