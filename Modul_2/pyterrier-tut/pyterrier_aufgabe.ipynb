{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Librarian Modul 2: **PyTerrier** Aufgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "import pyterrier as pt\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tools import littlehelper as lh\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"./jdk/Contents/Home/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from [bibsonomy.org](https://www.bibsonomy.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'makerspace'. Number of items retrieved: 187\n",
      "Query: 'virtual reality'. Number of items retrieved: 1335\n",
      "Query: 'mixed reality'. Number of items retrieved: 1071\n",
      "Query: 'augmented reality'. Number of items retrieved: 1256\n",
      "\n",
      "Saved data to \"bibsonomy_raw_data/makerspaces_vr.json\"\n"
     ]
    }
   ],
   "source": [
    "queries = ['makerspace', 'virtual reality', 'mixed reality', 'augmented reality']\n",
    "\n",
    "df = lh.df_from_bibsonomy_query(queries, \n",
    "                                to_json=True,\n",
    "                                limit=1000,\n",
    "                                output_dir='./bibsonomy_raw_data/',\n",
    "                                output_filename='makerspaces_vr.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set json_file to load\n",
    "json_infile = './bibsonomy_raw_data/makerspaces_vr.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "      <th>intraHash</th>\n",
       "      <th>label</th>\n",
       "      <th>user</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "      <th>changeDate</th>\n",
       "      <th>count</th>\n",
       "      <th>...</th>\n",
       "      <th>citeulike-linkout-3</th>\n",
       "      <th>opac</th>\n",
       "      <th>status</th>\n",
       "      <th>oai-id</th>\n",
       "      <th>oai-set</th>\n",
       "      <th>unit</th>\n",
       "      <th>details</th>\n",
       "      <th>documenturl</th>\n",
       "      <th>review</th>\n",
       "      <th>journaltitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/a10ba61e8b7f7ca6...</td>\n",
       "      <td>[Unsortierte_Lesezeichen]</td>\n",
       "      <td>a10ba61e8b7f7ca6db55da2670090ef2</td>\n",
       "      <td>Attraktor - Der Makerspace in Hamburg</td>\n",
       "      <td>robingarcia</td>\n",
       "      <td>Der gemeinnützige Attraktor e. V. betreibt den...</td>\n",
       "      <td>2017-03-17 10:34:46</td>\n",
       "      <td>2017-04-13 09:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/6093e51d381e6c5c...</td>\n",
       "      <td>[educação, maker]</td>\n",
       "      <td>6093e51d381e6c5c2a7d5ee954f7c1ee</td>\n",
       "      <td>Why Makerspaces are Changing the World – Betab...</td>\n",
       "      <td>christianoavila</td>\n",
       "      <td>While in college I learned about engineering i...</td>\n",
       "      <td>2017-05-09 12:48:55</td>\n",
       "      <td>2017-05-09 12:48:55</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/90a1ee85e2acf284...</td>\n",
       "      <td>[thinking, design, makerspace, makers, learnin...</td>\n",
       "      <td>90a1ee85e2acf284765373ea8756d255</td>\n",
       "      <td>Design Thinking Process and UDL Planning Tool ...</td>\n",
       "      <td>yish</td>\n",
       "      <td>If there is a makerspace in your school, it ma...</td>\n",
       "      <td>2017-12-25 16:06:35</td>\n",
       "      <td>2017-12-25 16:06:35</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/f1009e9d37cb7bfd...</td>\n",
       "      <td>[labs, tesis, maker]</td>\n",
       "      <td>f1009e9d37cb7bfdf0208ac6d7f052b4</td>\n",
       "      <td>Albemarle County Schools’ Journey From a Maker...</td>\n",
       "      <td>djimenezsanchez</td>\n",
       "      <td></td>\n",
       "      <td>2016-08-04 09:40:49</td>\n",
       "      <td>2016-08-04 09:40:49</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/5d1439dd75497437...</td>\n",
       "      <td>[makerspace, makers, israel, education, maker]</td>\n",
       "      <td>5d1439dd7549743700296c739da928ee</td>\n",
       "      <td>XLN - Anything is possible. Anyone Can.</td>\n",
       "      <td>yish</td>\n",
       "      <td></td>\n",
       "      <td>2017-06-08 14:45:40</td>\n",
       "      <td>2017-06-08 14:45:40</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                                 id  \\\n",
       "0  Bookmark  https://www.bibsonomy.org/url/a10ba61e8b7f7ca6...   \n",
       "1  Bookmark  https://www.bibsonomy.org/url/6093e51d381e6c5c...   \n",
       "2  Bookmark  https://www.bibsonomy.org/url/90a1ee85e2acf284...   \n",
       "3  Bookmark  https://www.bibsonomy.org/url/f1009e9d37cb7bfd...   \n",
       "4  Bookmark  https://www.bibsonomy.org/url/5d1439dd75497437...   \n",
       "\n",
       "                                                tags  \\\n",
       "0                          [Unsortierte_Lesezeichen]   \n",
       "1                                  [educação, maker]   \n",
       "2  [thinking, design, makerspace, makers, learnin...   \n",
       "3                               [labs, tesis, maker]   \n",
       "4     [makerspace, makers, israel, education, maker]   \n",
       "\n",
       "                          intraHash  \\\n",
       "0  a10ba61e8b7f7ca6db55da2670090ef2   \n",
       "1  6093e51d381e6c5c2a7d5ee954f7c1ee   \n",
       "2  90a1ee85e2acf284765373ea8756d255   \n",
       "3  f1009e9d37cb7bfdf0208ac6d7f052b4   \n",
       "4  5d1439dd7549743700296c739da928ee   \n",
       "\n",
       "                                               label             user  \\\n",
       "0              Attraktor - Der Makerspace in Hamburg      robingarcia   \n",
       "1  Why Makerspaces are Changing the World – Betab...  christianoavila   \n",
       "2  Design Thinking Process and UDL Planning Tool ...             yish   \n",
       "3  Albemarle County Schools’ Journey From a Maker...  djimenezsanchez   \n",
       "4            XLN - Anything is possible. Anyone Can.             yish   \n",
       "\n",
       "                                         description                date  \\\n",
       "0  Der gemeinnützige Attraktor e. V. betreibt den... 2017-03-17 10:34:46   \n",
       "1  While in college I learned about engineering i... 2017-05-09 12:48:55   \n",
       "2  If there is a makerspace in your school, it ma... 2017-12-25 16:06:35   \n",
       "3                                                    2016-08-04 09:40:49   \n",
       "4                                                    2017-06-08 14:45:40   \n",
       "\n",
       "            changeDate  count  ... citeulike-linkout-3  opac status oai-id  \\\n",
       "0  2017-04-13 09:59:00      1  ...                None  None   None   None   \n",
       "1  2017-05-09 12:48:55      1  ...                None  None   None   None   \n",
       "2  2017-12-25 16:06:35      1  ...                None  None   None   None   \n",
       "3  2016-08-04 09:40:49      1  ...                None  None   None   None   \n",
       "4  2017-06-08 14:45:40      1  ...                None  None   None   None   \n",
       "\n",
       "  oai-set  unit details documenturl review journaltitle  \n",
       "0    None  None    None        None   None         None  \n",
       "1    None  None    None        None   None         None  \n",
       "2    None  None    None        None   None         None  \n",
       "3    None  None    None        None   None         None  \n",
       "4    None  None    None        None   None         None  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(json_infile)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3849, 128)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "search_string\n",
       "virtual reality      1335\n",
       "augmented reality    1256\n",
       "mixed reality        1071\n",
       "makerspace            187\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['search_string'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"496.685095pt\" height=\"325.986375pt\" viewBox=\"0 0 496.685095 325.986375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-01-29T11:17:42.145978</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.4, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 325.986375 \n",
       "L 496.685095 325.986375 \n",
       "L 496.685095 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 120.085938 288.430125 \n",
       "L 477.205938 288.430125 \n",
       "L 477.205938 22.318125 \n",
       "L 120.085938 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 120.085938 271.798125 \n",
       "L 167.727414 271.798125 \n",
       "L 167.727414 238.534125 \n",
       "L 120.085938 238.534125 \n",
       "z\n",
       "\" clip-path=\"url(#p3ed08ef3e4)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 120.085938 205.270125 \n",
       "L 392.941668 205.270125 \n",
       "L 392.941668 172.006125 \n",
       "L 120.085938 172.006125 \n",
       "z\n",
       "\" clip-path=\"url(#p3ed08ef3e4)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 120.085938 138.742125 \n",
       "L 440.07361 138.742125 \n",
       "L 440.07361 105.478125 \n",
       "L 120.085938 105.478125 \n",
       "z\n",
       "\" clip-path=\"url(#p3ed08ef3e4)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 120.085938 72.214125 \n",
       "L 460.200223 72.214125 \n",
       "L 460.200223 38.950125 \n",
       "L 120.085938 38.950125 \n",
       "z\n",
       "\" clip-path=\"url(#p3ed08ef3e4)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m69cddf1cf3\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m69cddf1cf3\" x=\"120.085938\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(116.904687 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m69cddf1cf3\" x=\"171.039389\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(161.495639 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m69cddf1cf3\" x=\"221.99284\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(212.44909 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m69cddf1cf3\" x=\"272.946291\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(263.402541 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m69cddf1cf3\" x=\"323.899742\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 800 -->\n",
       "      <g transform=\"translate(314.355992 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m69cddf1cf3\" x=\"374.853193\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(362.128193 303.028562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m69cddf1cf3\" x=\"425.806644\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1200 -->\n",
       "      <g transform=\"translate(413.081644 303.028562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m69cddf1cf3\" x=\"476.760095\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1400 -->\n",
       "      <g transform=\"translate(464.035095 303.028562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- number of titles -->\n",
       "     <g transform=\"translate(258.754531 316.706687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"63.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"126.757812\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" x=\"224.169922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"287.646484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"349.169922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"390.283203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"422.070312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-66\" x=\"483.251953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"518.457031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"550.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"589.453125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"617.236328\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"656.445312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"684.228516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"745.751953\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m2afeed31ff\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2afeed31ff\" x=\"120.085938\" y=\"255.166125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- makerspace -->\n",
       "      <g transform=\"translate(52.1875 258.965344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-61\" x=\"97.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-6b\" x=\"158.691406\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-65\" x=\"212.976562\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-72\" x=\"274.5\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-73\" x=\"315.613281\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-70\" x=\"367.712891\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-61\" x=\"431.189453\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-63\" x=\"492.46875\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-65\" x=\"547.449219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2afeed31ff\" x=\"120.085938\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- mixed reality -->\n",
       "      <g transform=\"translate(47.71875 192.437344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-69\" x=\"97.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-78\" x=\"125.195312\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-65\" x=\"181.25\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-64\" x=\"242.773438\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-20\" x=\"306.25\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-72\" x=\"338.037109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-65\" x=\"376.900391\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-61\" x=\"438.423828\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-6c\" x=\"499.703125\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-69\" x=\"527.486328\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-74\" x=\"555.269531\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-79\" x=\"594.478516\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2afeed31ff\" x=\"120.085938\" y=\"122.110125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- augmented reality -->\n",
       "      <g transform=\"translate(20.878125 125.909344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-75\" x=\"61.279297\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-67\" x=\"124.658203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-6d\" x=\"188.134766\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-65\" x=\"285.546875\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-6e\" x=\"347.070312\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-74\" x=\"410.449219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-65\" x=\"449.658203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-64\" x=\"511.181641\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-20\" x=\"574.658203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-72\" x=\"606.445312\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-65\" x=\"645.308594\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-61\" x=\"706.832031\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-6c\" x=\"768.111328\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-69\" x=\"795.894531\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-74\" x=\"823.677734\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-79\" x=\"862.886719\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2afeed31ff\" x=\"120.085938\" y=\"55.582125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- virtual reality -->\n",
       "      <g transform=\"translate(46.373437 59.381344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-69\" x=\"59.179688\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-72\" x=\"86.962891\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-74\" x=\"128.076172\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-75\" x=\"167.285156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-61\" x=\"230.664062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-6c\" x=\"291.943359\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-20\" x=\"319.726562\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-72\" x=\"351.513672\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-65\" x=\"390.376953\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-61\" x=\"451.900391\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-6c\" x=\"513.179688\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-69\" x=\"540.962891\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-74\" x=\"568.746094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-79\" x=\"607.955078\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- bibsonomy query string -->\n",
       "     <g transform=\"translate(14.798437 214.798344) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-62\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" x=\"91.259766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"154.736328\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"206.835938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"268.017578\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"331.396484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"392.578125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"489.990234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"549.169922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"580.957031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"644.433594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"707.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"769.335938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"810.449219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"869.628906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"901.416016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"953.515625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"992.724609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"1033.837891\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"1061.621094\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"1125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 120.085938 288.430125 \n",
       "L 120.085938 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 477.205938 288.430125 \n",
       "L 477.205938 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 120.085938 288.430125 \n",
       "L 477.205938 288.430125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 120.085938 22.318125 \n",
       "L 477.205938 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- title distribution for bibsonomy query strings -->\n",
       "    <g transform=\"translate(164.319063 16.318125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"39.208984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"66.992188\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"106.201172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"133.984375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"195.507812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"227.294922\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"290.771484\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"318.554688\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"370.654297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"409.863281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"450.976562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-62\" x=\"478.759766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"542.236328\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"605.615234\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"644.824219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"672.607422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"733.789062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"797.167969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-66\" x=\"828.955078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"864.160156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"925.341797\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"966.455078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-62\" x=\"998.242188\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"1061.71875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-62\" x=\"1089.501953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1152.978516\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1205.078125\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1266.259766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1329.638672\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1390.820312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-79\" x=\"1488.232422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1547.412109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-71\" x=\"1579.199219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"1642.675781\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1706.054688\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"1767.578125\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-79\" x=\"1808.691406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1867.871094\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1899.658203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"1951.757812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"1990.966797\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"2032.080078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"2059.863281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"2123.242188\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"2186.71875\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p3ed08ef3e4\">\n",
       "   <rect x=\"120.085938\" y=\"22.318125\" width=\"357.12\" height=\"266.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['search_string'].value_counts().sort_values(ascending=True).plot(kind='barh',\n",
    "                                                                    xlabel='number of titles',\n",
    "                                                                    ylabel='bibsonomy query string',\n",
    "                                                                    title='title distribution for bibsonomy query strings');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "      <th>intraHash</th>\n",
       "      <th>label</th>\n",
       "      <th>user</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "      <th>changeDate</th>\n",
       "      <th>count</th>\n",
       "      <th>...</th>\n",
       "      <th>citeulike-linkout-3</th>\n",
       "      <th>opac</th>\n",
       "      <th>status</th>\n",
       "      <th>oai-id</th>\n",
       "      <th>oai-set</th>\n",
       "      <th>unit</th>\n",
       "      <th>details</th>\n",
       "      <th>documenturl</th>\n",
       "      <th>review</th>\n",
       "      <th>journaltitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/a10ba61e8b7f7ca6...</td>\n",
       "      <td>[Unsortierte_Lesezeichen]</td>\n",
       "      <td>a10ba61e8b7f7ca6db55da2670090ef2</td>\n",
       "      <td>Attraktor - Der Makerspace in Hamburg</td>\n",
       "      <td>robingarcia</td>\n",
       "      <td>Der gemeinnützige Attraktor e. V. betreibt den...</td>\n",
       "      <td>2017-03-17 10:34:46</td>\n",
       "      <td>2017-04-13 09:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/6093e51d381e6c5c...</td>\n",
       "      <td>[educação, maker]</td>\n",
       "      <td>6093e51d381e6c5c2a7d5ee954f7c1ee</td>\n",
       "      <td>Why Makerspaces are Changing the World – Betab...</td>\n",
       "      <td>christianoavila</td>\n",
       "      <td>While in college I learned about engineering i...</td>\n",
       "      <td>2017-05-09 12:48:55</td>\n",
       "      <td>2017-05-09 12:48:55</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/90a1ee85e2acf284...</td>\n",
       "      <td>[thinking, design, makerspace, makers, learnin...</td>\n",
       "      <td>90a1ee85e2acf284765373ea8756d255</td>\n",
       "      <td>Design Thinking Process and UDL Planning Tool ...</td>\n",
       "      <td>yish</td>\n",
       "      <td>If there is a makerspace in your school, it ma...</td>\n",
       "      <td>2017-12-25 16:06:35</td>\n",
       "      <td>2017-12-25 16:06:35</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                                 id  \\\n",
       "0  Bookmark  https://www.bibsonomy.org/url/a10ba61e8b7f7ca6...   \n",
       "1  Bookmark  https://www.bibsonomy.org/url/6093e51d381e6c5c...   \n",
       "2  Bookmark  https://www.bibsonomy.org/url/90a1ee85e2acf284...   \n",
       "\n",
       "                                                tags  \\\n",
       "0                          [Unsortierte_Lesezeichen]   \n",
       "1                                  [educação, maker]   \n",
       "2  [thinking, design, makerspace, makers, learnin...   \n",
       "\n",
       "                          intraHash  \\\n",
       "0  a10ba61e8b7f7ca6db55da2670090ef2   \n",
       "1  6093e51d381e6c5c2a7d5ee954f7c1ee   \n",
       "2  90a1ee85e2acf284765373ea8756d255   \n",
       "\n",
       "                                               label             user  \\\n",
       "0              Attraktor - Der Makerspace in Hamburg      robingarcia   \n",
       "1  Why Makerspaces are Changing the World – Betab...  christianoavila   \n",
       "2  Design Thinking Process and UDL Planning Tool ...             yish   \n",
       "\n",
       "                                         description                date  \\\n",
       "0  Der gemeinnützige Attraktor e. V. betreibt den... 2017-03-17 10:34:46   \n",
       "1  While in college I learned about engineering i... 2017-05-09 12:48:55   \n",
       "2  If there is a makerspace in your school, it ma... 2017-12-25 16:06:35   \n",
       "\n",
       "            changeDate  count  ... citeulike-linkout-3  opac status oai-id  \\\n",
       "0  2017-04-13 09:59:00      1  ...                None  None   None   None   \n",
       "1  2017-05-09 12:48:55      1  ...                None  None   None   None   \n",
       "2  2017-12-25 16:06:35      1  ...                None  None   None   None   \n",
       "\n",
       "  oai-set  unit details documenturl review journaltitle  \n",
       "0    None  None    None        None   None         None  \n",
       "1    None  None    None        None   None         None  \n",
       "2    None  None    None        None   None         None  \n",
       "\n",
       "[3 rows x 128 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "      <th>intraHash</th>\n",
       "      <th>label</th>\n",
       "      <th>user</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "      <th>changeDate</th>\n",
       "      <th>count</th>\n",
       "      <th>...</th>\n",
       "      <th>citeulike-linkout-3</th>\n",
       "      <th>opac</th>\n",
       "      <th>status</th>\n",
       "      <th>oai-id</th>\n",
       "      <th>oai-set</th>\n",
       "      <th>unit</th>\n",
       "      <th>details</th>\n",
       "      <th>documenturl</th>\n",
       "      <th>review</th>\n",
       "      <th>journaltitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>Publication</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2f9ab9ad1d6ef...</td>\n",
       "      <td>[dblp]</td>\n",
       "      <td>f9ab9ad1d6efa6e13d5c9f1d656a01d1</td>\n",
       "      <td>Mobile augmented reality for browsing physical...</td>\n",
       "      <td>dblp</td>\n",
       "      <td></td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>2018-11-07 15:22:31</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>Publication</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2d9c365efe0da...</td>\n",
       "      <td>[dblp]</td>\n",
       "      <td>d9c365efe0daa197ca15113c2edb8cc0</td>\n",
       "      <td>A Projection-based Medical Augmented Reality S...</td>\n",
       "      <td>dblp</td>\n",
       "      <td></td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>2018-11-07 13:34:32</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>Publication</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/268bdd48b5861...</td>\n",
       "      <td>[dblp]</td>\n",
       "      <td>68bdd48b58619875e909f73a0a019b81</td>\n",
       "      <td>Occlusion registration in video-based augmente...</td>\n",
       "      <td>dblp</td>\n",
       "      <td></td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>2018-11-07 14:20:38</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             type                                                 id    tags  \\\n",
       "3846  Publication  https://www.bibsonomy.org/bibtex/2f9ab9ad1d6ef...  [dblp]   \n",
       "3847  Publication  https://www.bibsonomy.org/bibtex/2d9c365efe0da...  [dblp]   \n",
       "3848  Publication  https://www.bibsonomy.org/bibtex/268bdd48b5861...  [dblp]   \n",
       "\n",
       "                             intraHash  \\\n",
       "3846  f9ab9ad1d6efa6e13d5c9f1d656a01d1   \n",
       "3847  d9c365efe0daa197ca15113c2edb8cc0   \n",
       "3848  68bdd48b58619875e909f73a0a019b81   \n",
       "\n",
       "                                                  label  user description  \\\n",
       "3846  Mobile augmented reality for browsing physical...  dblp               \n",
       "3847  A Projection-based Medical Augmented Reality S...  dblp               \n",
       "3848  Occlusion registration in video-based augmente...  dblp               \n",
       "\n",
       "           date           changeDate  count  ... citeulike-linkout-3  opac  \\\n",
       "3846 2018-11-06  2018-11-07 15:22:31      1  ...                None  None   \n",
       "3847 2018-11-06  2018-11-07 13:34:32      1  ...                None  None   \n",
       "3848 2018-11-06  2018-11-07 14:20:38      1  ...                None  None   \n",
       "\n",
       "     status oai-id oai-set  unit details documenturl review journaltitle  \n",
       "3846   None   None    None  None    None        None   None         None  \n",
       "3847   None   None    None  None    None        None   None         None  \n",
       "3848   None   None    None  None    None        None   None         None  \n",
       "\n",
       "[3 rows x 128 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns) # column count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful columns with lower value counts that **should not** be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'abstract' column entry count: 531\n"
     ]
    }
   ],
   "source": [
    "print(f\"'abstract' column entry count: {len(df.query('abstract.notna()')[['id', 'abstract']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns with less than 500 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "series            506\n",
       "abstract          531\n",
       "number            664\n",
       "journal           878\n",
       "volume           1223\n",
       "editors          1352\n",
       "editor           1352\n",
       "isbn             1645\n",
       "publisher        2038\n",
       "booktitle        2095\n",
       "ee               2342\n",
       "pages            2626\n",
       "author           3112\n",
       "authors          3112\n",
       "pub-type         3149\n",
       "bibtexKey        3149\n",
       "year             3149\n",
       "interHash        3149\n",
       "type             3849\n",
       "count            3849\n",
       "changeDate       3849\n",
       "date             3849\n",
       "description      3849\n",
       "user             3849\n",
       "label            3849\n",
       "intraHash        3849\n",
       "tags             3849\n",
       "id               3849\n",
       "url              3849\n",
       "search_string    3849\n",
       "dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_counts = df.count()\n",
    "col_500 = col_counts[col_counts > 500]\n",
    "col_500.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "      <th>intraHash</th>\n",
       "      <th>label</th>\n",
       "      <th>user</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "      <th>changeDate</th>\n",
       "      <th>count</th>\n",
       "      <th>...</th>\n",
       "      <th>pages</th>\n",
       "      <th>abstract</th>\n",
       "      <th>isbn</th>\n",
       "      <th>bibtexKey</th>\n",
       "      <th>journal</th>\n",
       "      <th>series</th>\n",
       "      <th>volume</th>\n",
       "      <th>number</th>\n",
       "      <th>ee</th>\n",
       "      <th>search_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/a10ba61e8b7f7ca6...</td>\n",
       "      <td>[Unsortierte_Lesezeichen]</td>\n",
       "      <td>a10ba61e8b7f7ca6db55da2670090ef2</td>\n",
       "      <td>Attraktor - Der Makerspace in Hamburg</td>\n",
       "      <td>robingarcia</td>\n",
       "      <td>Der gemeinnützige Attraktor e. V. betreibt den...</td>\n",
       "      <td>2017-03-17 10:34:46</td>\n",
       "      <td>2017-04-13 09:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/6093e51d381e6c5c...</td>\n",
       "      <td>[educação, maker]</td>\n",
       "      <td>6093e51d381e6c5c2a7d5ee954f7c1ee</td>\n",
       "      <td>Why Makerspaces are Changing the World – Betab...</td>\n",
       "      <td>christianoavila</td>\n",
       "      <td>While in college I learned about engineering i...</td>\n",
       "      <td>2017-05-09 12:48:55</td>\n",
       "      <td>2017-05-09 12:48:55</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bookmark</td>\n",
       "      <td>https://www.bibsonomy.org/url/90a1ee85e2acf284...</td>\n",
       "      <td>[thinking, design, makerspace, makers, learnin...</td>\n",
       "      <td>90a1ee85e2acf284765373ea8756d255</td>\n",
       "      <td>Design Thinking Process and UDL Planning Tool ...</td>\n",
       "      <td>yish</td>\n",
       "      <td>If there is a makerspace in your school, it ma...</td>\n",
       "      <td>2017-12-25 16:06:35</td>\n",
       "      <td>2017-12-25 16:06:35</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                                 id  \\\n",
       "0  Bookmark  https://www.bibsonomy.org/url/a10ba61e8b7f7ca6...   \n",
       "1  Bookmark  https://www.bibsonomy.org/url/6093e51d381e6c5c...   \n",
       "2  Bookmark  https://www.bibsonomy.org/url/90a1ee85e2acf284...   \n",
       "\n",
       "                                                tags  \\\n",
       "0                          [Unsortierte_Lesezeichen]   \n",
       "1                                  [educação, maker]   \n",
       "2  [thinking, design, makerspace, makers, learnin...   \n",
       "\n",
       "                          intraHash  \\\n",
       "0  a10ba61e8b7f7ca6db55da2670090ef2   \n",
       "1  6093e51d381e6c5c2a7d5ee954f7c1ee   \n",
       "2  90a1ee85e2acf284765373ea8756d255   \n",
       "\n",
       "                                               label             user  \\\n",
       "0              Attraktor - Der Makerspace in Hamburg      robingarcia   \n",
       "1  Why Makerspaces are Changing the World – Betab...  christianoavila   \n",
       "2  Design Thinking Process and UDL Planning Tool ...             yish   \n",
       "\n",
       "                                         description                date  \\\n",
       "0  Der gemeinnützige Attraktor e. V. betreibt den... 2017-03-17 10:34:46   \n",
       "1  While in college I learned about engineering i... 2017-05-09 12:48:55   \n",
       "2  If there is a makerspace in your school, it ma... 2017-12-25 16:06:35   \n",
       "\n",
       "            changeDate  count  ... pages abstract  isbn bibtexKey journal  \\\n",
       "0  2017-04-13 09:59:00      1  ...  None     None  None      None    None   \n",
       "1  2017-05-09 12:48:55      1  ...  None     None  None      None    None   \n",
       "2  2017-12-25 16:06:35      1  ...  None     None  None      None    None   \n",
       "\n",
       "  series volume number    ee search_string  \n",
       "0   None   None   None  None    makerspace  \n",
       "1   None   None   None  None    makerspace  \n",
       "2   None   None   None  None    makerspace  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df.filter(col_500.index)\n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean `year` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2b6e28fc1c8cd...</td>\n",
       "      <td>2019, to appear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>https://www.bibsonomy.org/bibtex/29e5bb6d85acc...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2d62f6d7ab928...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3101</th>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2d62f6d7ab928...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id             year\n",
       "694   https://www.bibsonomy.org/bibtex/2b6e28fc1c8cd...  2019, to appear\n",
       "1016  https://www.bibsonomy.org/bibtex/29e5bb6d85acc...            2021 \n",
       "3098  https://www.bibsonomy.org/bibtex/2d62f6d7ab928...            2021 \n",
       "3101  https://www.bibsonomy.org/bibtex/2d62f6d7ab928...            2021 "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing year values with np.nan\n",
    "df_clean.loc[:, 'year'] = df_clean['year'].fillna(np.nan)\n",
    "\n",
    "# Check for incorrect year values\n",
    "df_clean[(df_clean['year'].notna()) & (df_clean['year'].str.contains('\\D+', regex=True))][['id', 'year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [year]\n",
       "Index: []"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean\n",
    "df_clean.loc[:, 'year'] = df_clean['year'].str.strip().replace(r'\\D+', '', regex=True)\n",
    "\n",
    "# Check\n",
    "df_clean[~(df_clean['year'].notna()) & (df_clean['year'].str.match('\\d{4}'))][['year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean `isbn` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [isbn]\n",
       "Index: []"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean\n",
    "df_clean['isbn'] = df_clean['isbn'].str.replace(r'\\D+', '', regex=True)\n",
    "\n",
    "# Check\n",
    "df_clean[(df_clean['isbn'].notna()) & (df_clean['isbn'].str.contains(r'[^\\d]', regex=True))][['isbn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean `abstract` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"You may have heard the term makerspace and wondered what it meant. Makerspaces are, simply put, places where people gather to make things. Although that may sound like a simplistic definition, the things that can be created in a makerspace vary a great deal. Makerspaces can be high tech, low tech, and everything in between. A makerspace's offerings revolve around the needs of the community it serves, but the one thing all have in common is that they bring people together to share ideas. Typically, the first thing that comes to mind when thinking about mak- erspaces is 3D printing, but when it comes to what’s going on in makerspaces around the world, that’s just the tip of the iceberg. Makers create things, ideas, and concepts. Makers work in metal, wood, plastic, fabric, paper, and digital forms. From robotics to crocheting, there are no limits to your makerspace. Let your imagination run wild. In this chapter, we’ll provide the information and ideas to get your maker-spaces up and running based on your unique populations and budgets. You’ll find a myriad of ways to create your makerspace. You’ll also discover ways to ensure your makerspace is fun and functional.\",\n",
       " 'A growing body of research focuses on what outcomes to assess in makerspaces, and appropriate formats for capturing those outcomes (e.g. reflections, surveys, and portfolios). Linguistic analysis as a data mining technique holds promise for revealing different dimensions of learning exhibited by students in makerspaces. In this study, student reflections on makerspace projects were gathered in 2 formats over 2 years: private written assessments captured in the 3D GameLab gamification platform, and semi-public video-recorded assessments posted in the more social FlipGrid platform. Transcripts of student assessments were analyzed using Linguistic Inquiry Word Count (LIWC) to generate 4 summary variables thought to inform makerspace outcomes of interest (i.e. analytical thinking, authenticity, clout, and emotional tone). Comparative findings indicate that written assessments may elicit more analytical thinking about maker projects compared with less analytical conversation in videos, while video assessments may elicit somewhat higher clout scores as evidence of social scaffolding along with a much more positive emotional tone. Recommendations are provided for layering assessment approaches to maximize the potential benefits of each format, including reflective writing for social spaces, in social groups, and about design processes and procedures.',\n",
       " \"The Makerspace Librarian's Sourcebook is an essential all-in-one guidebook to the maker realm written specifically for librarians. This practical volume is an invaluable resource for librarians seeking to learn about the major topics, tools, and technologies relevant to makerspaces today. Jam-packed with instruction and advice from the field's most tech-savvy innovators, this one-stop handbook will inspire readers through practical projects that they can implement in their libraries right now.\",\n",
       " 'Step-by-step instructions to guide you through exciting projects for makers of all skill levels. As a bonus, find useful info on how to customize and use these projects for outreach and promotion of your makerspace, your library, or your institution.',\n",
       " 'This book is a guidebook jam-packed with resources, advice, and information to help you develop and fund your own makerspace from the ground up. Learn what other libraries are making, building, and doing in their makerspaces and how you can, too. Readers are introduced to makerspace equipment, new technologies, models for planning and assessing projects, and useful case studies that will equip them with the knowledge to implement their own library makerspaces. This expanded second edition features eighteen brand new library makerspace profiles providing advice and inspiration for how to create your own library makerspace, over twenty new images and figures illustrating maker tools and trends as well as library makerspaces in action and new lists of actual grant and funding sources for library makerspaces.',\n",
       " 'Makerspaces, sometimes also referred to as hackerspaces, hackspaces, and fablabs are creative, DIY spaces where people can gather to create, invent, and learn. Discover how you can create a makerspace within your own library though this step-by-step guidebook.',\n",
       " \"Makerspaces and maker activities have evolved from a shiny new trend in libraries to an acknowledged and valued conduit for partnering with library patrons in the production process and a potent means to provide STEM and critical thinking skills to people of all ages. In a 2017 Library Journal survey of 7,000 public libraries, it was determined that the vast majority of them---89 percent---currently offer maker programming for their patrons. Makerspaces in Practice: Successful Models for Implementation is an advanced guidebook to library makerspaces written from a perspective derived from years of practical experience. Written nearly half a decade after The Makerspace Librarian's Sourcebook was published, this book strives to be of use not only to librarians who are strategizing how to get started but also to those who are actively running makerspaces and maker programming in their libraries. This handbook offers advice from seasoned practitioners based on what has worked for them as well as which programs and tools don't resonate with library patrons. This essential handbook will answer these questions and more\",\n",
       " 'The present study investigates makerspaces in Nigerian academic libraries: perceived benefits and challenges. Four objectives guided the study: what constituted makerspace in academic library, the level of awareness of makerspace by academic library benefits and challenges of adopting makerspace in libraries. The descriptive survey design was adopted and questionnaire was used for data collection. The population of the study comprised of professional and paraprofessional librarians of Ambrose University and University of Benin, from which a sample size of 119 was drawn using total enumeration sampling technique. Out of the 119 copies of questionnaire administered, 94 were retrieved and analyzed using simple percentage and frequency tables. Findings revealed that, the respondents were aware of what constitute makerspace which are library space, 3D printers, computers and projector. It was also discovered that, the respondents had a high level of awareness of makerspace in the library. Some of the benefits associated with the use of makerspace are: it facilitates group interaction, it improves knowledge and provides access to wide varieties of tools and technology. Some of the challenges encountered in the adoption of makerspace are training of academic library staff, security of makerspace gadgets, poor funding, erratic power supply, high cost and maintenance of equipment. It was however recommended that; librarians should make deliberate effort to explore the potentials in makerspace in the enhancement of their services and training should be conducted regularly to enhance librarian’s skills in the use of ICTs Kingsley Efe OSAWARU, Angela Ishioma DIME and Emordi Herbert OKONJO 2020. The Right Time for Makerspaces in Nigerian Academic Libraries: Perceived Benefits and Challenges. International Journal on Integrated Education. 3, 10 (Oct. 2020), 103-115. DOI:https://doi.org/10.31149/ijie.v3i10.694 Pdf Url : https://journals.researchparks.org/index.php/IJIE/article/view/694/654 Paper Url : https://journals.researchparks.org/index.php/IJIE/article/view/694',\n",
       " 'Purpose The purpose of this paper is to start exploring the possibilities for makerspaces to function as a new learning space within academic library services in higher education (HE). This original research study ask two key questions: How is learning achieved and supported in makerspaces? What can academic library services bring to the effective organisation and support of makerspaces? Design/methodology/approach An extensive literature review is followed by a template analysis (King, 2012) of data from an online forum of three professionals operating makerspaces in academic library services in the USA and a discussion incorporating relevant educational theory and philosophy. Findings The three overarching learning themes found were: experiential learning (Dewey, 1909; Kolb, 1984), communities of practice (Lave and Wenger, 1991) and self-efficacy through social learning (Bandura, 1997). Research limitations/implications The one-week forum of three professional library staff provided detailed and informative data. Substantial field work with students will also be required to see how far this professional lens has provided insight into how students are learning and supported in these and other makerspaces. Social implications The wider cultural implications are examined, including the potential social value of makerspaces as transformative creative spaces empowering communities and individuals. Originality/value This is the first study to date on the potential educational value of makerspaces within HE, and the specific support academic library services can offer if they choose to host a makerspace (including teaching information, digital and critical literacies).',\n",
       " 'Every good adaptation is also an innovation poster represents Makerspace PATS SAU in National library of Lithuania. It is an open access and free of charge service for schoolers that brings the change to learning experience. Poster represents benefits (advantages) that Makerspace brings to National Library, Public libraries and most importantly to the users. The aim of the poster is to present Makerspace as a bridge that connects Traditional Library and emerging technologies such as 3D printing, Coding, sustainable design, Virtual Reality and etc. Makerspace is a successful service from National Library of Lithuania that proved to be a good way to attract new users and strengthen library‘s community. In almost 3 years‘ time 10000 users have used our services and discovered library in a new and exiting way. IFLA WLIC 2019 main topic “Dialogue for change“ was exactly what we wanted to communicate through bringing our expertise to the conference. Our successful service is a great way to encourage other libraries to take on courageous initiatives, unconventional solutions and open the libraries‘ door to meet technological changes that are forming new way of learning, communicating, sharing and creating. Short movies about Makerspace that were used along with the printed poster: https://www.youtube.com/playlist?list=PLZHTxpVgsgcOTYh3e6OxyQzfjt9Plre\\\\_9',\n",
       " 'Makerspaces have permeated public libraries for a few years now -- a trend that has been fundamental to encouraging community building and enabling technical and practical education for all. The public libraries in the Central district of Berlin are embracing this important means of training by not only offering two permanent Makerspaces in two of their branches, but also by introducing a mobile MakerBus which will bring different formats to the community where needed. This new expanded service will serve our users in different ways: one Makerspace will focus further on digital skills by offering drone building classes, 3D printing and robotics; the second one will focus on more manual competences like sewing, stitching and hand-lettering but also embrace technology by offering a low-level introduction to photo and film making. The MakerBus will combine the best of both Makerspaces, offering modules in 3D printing, programming, sewing, and on top a small mobile workshop for fixing bikes and small electrical items, encouraging responsible consumption and less waste. We truly believe that by investing in and expanding these services, the Central district of Berlin will present its library users with the best in digital literacy education resting on three solid pillars of learning.',\n",
       " 'Abstract (deutsch) Die Master-Thesis untersucht das Potential von Urban Gardening Makerspaces für Öffentliche Bibliotheken. Die Arbeit steht im Kontext der Neuentwicklung bibliothekarischer Dienstleistungen in der digitalen Gesellschaft. Ausgangspunkt der Thesis stellen die aktuellen gesellschaftlichen Entwicklungen zum Do-it-Yourself (DIY), zum Sharing und der Maker-Bewegung dar. Diese Entwicklungen üben großen Einfluss auf die Debatte um den Wandel von Bibliotheken aus und legen die Grundlage, Makerspaces und Community Building als zentrale zukünftige Arbeitsfelder für Öffentliche Bibliothek zu betrachten. Dem wird der Trend zum Urban Gardening gegenübergestellt. Zentral ist die These, dass Urban Gardening keine Abkehr von der digitalen Gesellschaft darstellt, sondern vielmehr eine Ausprägung des DIY und der Maker-Bewegung ist. Diese Sichtweise ermöglicht es, Parallelen zwischen Urban Gardening und Makerspaces abzuleiten, was die Grundlage für die Einbindung von Urban Gardening Makerspaces in eine bibliothekarische Praxis darstellt. Urban Gardening Makerspaces werden zudem mit der Idee der grünen Bibliothek sowie der sustainable library verknüpft, wofür die Thesis eine Begriffserweiterung zur ökologisch und sozial nachhaltigen Bibliotheksarbeit anbietet. Anschließend werden die Potentiale von Urban Gardening und Öffentlicher Bibliotheken im Rahmen aktueller Tendenzen der Stadtentwicklung beleuchtet. Ein abschließender Leitfaden soll Öffentlichen Bibliotheken als Vorlage dienen, eigene Ideen zu entwickeln und Urban Gardening Makerspaces einzurichten. Schlagworte: Urban Gardening, Makerspace, Stadtentwicklung, grüne Bibliothek Abstract (english) In the context of the development of new library services in the digital age, this Master’s thesis investigates the potential of Urban Gardening Makerspaces in public libraries. It is argued that current trends in society like DIY, sharing or the maker movement have huge influence on the discussion of public library services in the digital age. They also provide the groundwork for makerspaces and community building as one of the main parts of future library services. This development is combined with the urban gardening trend. One main aspect of this thesis is the understanding, that urban gardening is an essential part of the digital age and not its counterpart. Urban gardening has lots of characteristics of the DIY and maker movement. Regarding urban gardening and community gardens as makerspaces gives the basis to combine them with library services. Urban gardening makerspaces are embedded in the ideas of the green library and the sustainable library. Therefore, this thesis proposes a definition for ecological and societal sustainable library services. Afterwards the potentials of urban gardening and public libraries and their role in urban development are highlighted. A guideline for public libraries for creating their own urban gardening makerspaces completes this thesis.',\n",
       " 'Urban Gardening ist mehr als ein Trend. Öffentliche Bibliotheken sollten sich diesem in der Suche nach einem neuen Selbstverständnis und ihrer Rolle in einer digitalen Gesellschaft nicht verschließen. In der Stadtbibliothek Bad Oldesloe wurde eine ganze Veranstaltungsreihe rund um dieses Thema organisiert und in Form eines Makerspaces mit modernen Formen des gemeinschaftlichen Lernens verknüpft. So konnte sich die Stadtbibliothek als moderner und innovativer Lernort positionieren und neue Initiativen in der Stadt anstoßen. Urban Gardening is more than a trend. Public Libraries should not ignore this in their search for a new understanding and their role in the digital age. The Public Library of Bad Oldesloe created a series of events around that topic and combined it with a makerspace with new forms of learning in communities. By that, the Public Library presented itself as an innovative learning space and helped to engage some initiatives in the city itself.',\n",
       " 'As the maker movement continues to grow, new ideas and applications are being applied in public, academic, and K--12 libraries. Looking at where we started and where we are headed is essential to applying new knowledge and creating spaces that meet the needs of our users. The maker movement got its first push in 2006. Maker Media launched the maker movement as we know it today and brought it to the public with Maker Faires.',\n",
       " 'The Complete Guide is a road map for libraries of any size, with any budget, seeking to redesign or repurpose space or develop maker style programming. This book covers developing makerspaces, writing grant proposals, and helping staff and administrators learn about the technologies and processes involved.',\n",
       " 'Public libraries in Germany are currently under pressure, with many closures recently due to reduced municipal budgets. Public libraries hence need to demonstrate relevance to their communities, sponsors and other accountable bodies to avoid the continuance of this trend. This paper shows how libraries can link into debates in society and politics by adopting the urban gardening trend as well as developing new services around this green movement. Within the debate on public libraries’ transformation, urban gardening and the idea of the green library provide an opportunity to develop new green and sustainable library services. By combining makerspaces and community building with urban gardening and foodsharing, the public library of Bad Oldesloe developed a series of events that turned the library into a modern and creative learning space. At the same time, the library offered its physical space as a community platform for networking, cooperation and creativity.',\n",
       " \"The needs and functions of library buildings have certainly changed over the last decade, but the necessity for planning intelligently and thoroughly hasn't. In fact, tighter budgets and the complex demands of both library users and staff call for careful preparation now more than ever. Whether you're building from the ground up or simply remodeling, the success of your project hinges on planning, coordination, and communication. This new update of Sannwald's classic guide will help you stay prepared and organized for every phase of your undertaking from conception through the dedication ceremony. Using a popular checklist format that ensures no detail is overlooked, this planner covers crucial considerations like Americans with Disabilities Act (ADA) factors, structured to match the federal code; sustainable design features, including sensors that save energy and water; designing makerspaces, digital media labs, or leased library enterprises; disaster and recovery planning; creating quiet spaces; collaborative collections and materials-handling efficiency; and important virtual presence aspects to bear in mind during physical space decisions. Library managers, administrators, and facilities staff will find this book a matchless tool for any construction project regardless of size or complexity.\",\n",
       " 'Zusammenfassung Es wird wieder gebastelt, geschraubt, gelötet, repariert. Werkstätten erleben heutzutage eine unerwartete Renaissance. Der Sammelbegriff „offene Werkstätten“ umfasst verschiedene Formen von Infrastrukturen zur gemeinsamen Nutzung von Mitteln für materielle Produktion und den offenen Austausch darüber. Konkret handelt es sich um gemeinwohlorientierte Repair Cafés, Siebdruckwerkstätten und Fahrradreparaturinitiativen, aber auch um gewerblich orientierte FabLabs und Hackerspaces. Die gewachsene Bedeutung und die gestiegene Zahl der offenen Werkstätten sind Ausdruck und Ergebnis sich verändernder Produktions- und Innovationsbedingungen in der Wirtschaft. Dezentralisierungsprozesse der stofflichen Produktion, die durch die technisch avancierten Ausprägungen offener Werkstätten vorangetrieben werden, können sich auch transformativ auf andere Branchen auswirken. Entsprechend dieser Dynamiken wurden offene Werkstätten als vielschichtige Orte untersucht, um sowohl Nachhaltigkeits- als auch Innovationspotenziale in den Blick zu nehmen. Im Sommer 2015 richteten wir eine Befragung an 453 offene Werkstätten in Deutschland, um deren Innovations- und Arbeitsprozesse genauer in den Blick zu nehmen. Auf der Basis eines Datensamples von 103 Antwortsätzen (Rücklaufquote 23 %) rekonstruiert die vorliegende Studie die sozialen, materiellen und ökonomischen Wirkungsprozesse in diesen Werkstätten. Die starke Dynamik des Phänomens zeigt sich unter anderem daran, dass die meisten Antwortenden der offenen Werkstatt zwischen 2013 und 2015 beigetreten sind. Über die Hälfte hat ihre Werkstatt selbst mitbegründet. Der überwiegende Teil besitzt eine naturwissenschaftliche Qualifikation. Gemeinschaftsorientierung – nicht das materielle Endergebnis oder die Herstellung eines Gegenstandes – ist der Hauptgrund für das Engagement in offenen Werkstätten. Die persönliche Motivation der Aktiven liegt weniger im ökonomischen Bereich als vielmehr in der Vermittlung von Wissen, dem praktischen Arbeiten und in einer gesellschaftlichen Transformation. Das Erproben neuer sozialer Wege des Lernens, der Wissensvermittlung und des Zusammenarbeitens stehen im Mittelpunkt alltäglicher Praktiken in offenen Werkstätten. Summary Lately, there is a noticeable trend that people start doing handicrafts again. And thus open workshops experience an unexpected renaissance. The term “open workshops” comprises different kinds of infrastructures for the shared use of resources for material production and an open exchange about it. This includes non-commercial repair cafés, screen printing workshops and initiatives for bicycle repair as well as commercially oriented fab labs and hackerspaces. The growing number and importance of open workshops are the result of changing economic conditions for production and innovation. The decentralization of physical production, which is promoted by the technically advanced types of open workshops, may also have transformative effects on other sectors. Multiple aspects of open workshops were investigated to assess the potentials of these dynamics for sustainability and innovation. To investigate the work and innovation activivties, 453 open workshops in Germany were addressed by an empirical survey in the summer 2015. Based on a data sample of 103 observations (which corresponds to a response rate of 23 %), the social, material and economic processes within open workshops have been reconstructed. The strong dynamics of this phenomenon is demonstrated by the fact that the majority of the respondents joined the open workshops between 2013 and 2015. More than half of them are cofounders of the workshop. Predominantly, respondents have an educational background in natural sciences. Community-orientation – not the material end result or the fabrication of an object – is the principal reason for engaging in open workshops. The personal motivations of the actors are not primarily economic, but rather directed towards the transfer of knowledge, practical working experiences and societal transformation. Experimentation with new social forms of learning, knowledge transfer and collaboration is central to the everyday practice in open workshops.',\n",
       " 'Leicht verständlich werden die Grundlagen der Informatik und Informationstechnik vermittelt, die zum Verständnis der Anwendungen im bibliothekarischen Alltag benötigt werden. Grundlagen der Codierung, Datenmodellierung, Netzwerktechnik, Digitalisierung, Discovery-Systeme, Linked Data und Semantic-Web-Konzepte, Datensicherheit, Cloud-Systeme, RFID und Makerspaces werden mit hohem Praxisbezug und Beispielen aus dem bibliothekarischen Kontext eingehend erklärt und schaffen so eine umfassende Kenntnis der Terminologie und ein Verständnis für die technischen Zusammenhänge.',\n",
       " 'Etwas selbst zu machen, anstatt zu kaufen, ist in den letzten Jahren immer beliebter geworden. Öffentliche Bibliotheken werden auch auf diesem Gebiet immer mehr zum Kommunikationsort und gesellschaftlichen Raum.',\n",
       " 'Enterprises across the People’s Republic of China (China) seek to gain the benefits of electronic business, but very few of their electronic commerce (e-commerce) initiatives have been successful to date. A recent multiple case study examined some of the exceptions. This paper illustrates the distinctive nature of e-commerce with Chinese characteristics by profiling an online retailer, a traditional B2B intermediary, and an electronic marketplace or marketspace. Their key success factors were found to include the abilities to leverage core capabilities and to overcome institutional deficiencies through relationship building. The prospects for e-commerce in mainland China are related to the development of the rules and infrastructure that are fundamental to a modern market economy.',\n",
       " 'Neue, sich kontinuierlich weiterentwickelnde Anforderungen seitens Studium, Forschung und Lehre und technologisch getriebene – teilweise disruptive – Prozesse stellen die wissenschaftlichen Bibliotheken im digitalen Zeitalter vor große Herausforderungen. Als wissenschaftliche Bibliothekarinnen und Bibliothekare nehmen wir diese Herausforderungen nicht nur als selbstverständliche Aufgabe an, sondern sehen uns in der Verantwortung für eine aktive Mitgestaltung von Forschungsprozessen über den traditionellen Bereich der Informations- und Literaturversorgung hinaus. Perspektivisch entwickeln sich Bibliotheken zu virtuellen Arbeitsumgebungen, die wissenschaftliches Arbeiten in Forschung, Lehre und Studium fachspezifisch mit attraktiven Infrastrukturdiensten und Werkzeugen unterstützen und für Fach-Communities die Voraussetzungen für Interaktion und Kollaboration schaffen. In der mittelfristigen Perspektive bis 2025 haben folgende Handlungsfelder einen zentralen Stellenwert: (1) Open Access und neue Formen der Lizenzierung (2) Publikationsdienstleistungen (3) Management von Forschungsdaten (4) Überregionale Informationsversorgung für Fachcommunities (5) Langfristige Nutzbarkeit digitaler Ressourcen (6) Digitalisierung von Quellen des kulturellen Erbes (7) Etablierung von Kreativräumen (Cultural Labs, community-orientierte Makerspaces) (8) Förderung digitaler Medien- und Informationskompetenz',\n",
       " 'We regularly read and hear exhortations for women to take up positions in STEM. The call comes from both government and private corporate circles, and it also emanates from enthusiasts for free and open source software (FOSS), i.e. software that anyone is free to use, copy, study, and change in any way. Ironically, rate of participation in FOSS-related work is far lower than in other areas of computing. A 2002 European Union study showed that fewer than 2 percent of software developers in the FOSS world were women. How is it that an intellectual community of activists so open in principle to one and all -a community that prides itself for its enlightened politics and its commitment to social change - should have such a low rate of participation by women? This book is an ethnographic investigation of efforts to improve the diversity in software and hackerspace communities, with particular attention paid to gender diversity advocacy.',\n",
       " 'Digital assistants may well be the next revolution in computing. This free ebook explains what you need to know about Alexa, Cortana, Siri, and Google Assistant. There has been a subtle yet significant change in the way human beings interface with their computing devices. The traditional standard of keyboard and mouse is not the only way we have to communicate with our computers anymore. We have entered the age of the personal digital assistant, which can deliver results with nothing but a voice command. Some of the largest technology companies in the world (Microsoft, Google, Apple, and Amazon) believe that the digital agent will eventually be the primary interface we use to communicate with our computerized gadgets, the internet, and our network of interconnected devices (Internet of Things). With that belief in mind, these companies are attempting to make their version of a digital agent the de facto standard for most consumers. Each company is looking to establish a foothold in this marketspace because they believe digital agent technology is the key that will hold consumers in their respective digital ecosystems. In other words, once you use a particular digital agent and establish a connection with the software, it will be difficult to switch to a different digital agent. Essentially, if you were to change, you would have to start all over again.',\n",
       " 'To assess the content validity and concurrent validity of a haptically (force feedback) rendered, virtual reality simulation of temporal bone surgery.Eleven naive surgical trainees were given a 1-hour lesson on the operation, cortical mastoidectomy, in the virtual environment with the trainer on a networked simulator and then asked to perform this procedure on a real temporal bone.The simulator was found to be a convincing representation of temporal bone drilling and could be said to exhibit face validity. The simulator was an effective means of teaching both the surgical anatomy and the surgical approach as judged by oral assessments made before and after the virtual reality training session. The trainees were successful in identifying most surgical landmarks during their first temporal bone dissection, and over two thirds found the landmarks at the correct time during the procedure. Some trainees exhibited acceptable or better technique with the drill despite this being their first temporal bone dissection. Subjective assessments indicated a high level of acceptance of simulated surgery for training, and there was perceived value in specific enhancements of the virtual environment that facilitated learning. Particular enhancements of value were the networked simulation, the option to make the model semitransparent to reveal anatomic relationships, and error reporting when the sigmoid sinus or facial nerve was injured.Virtual reality simulation of temporal bone surgery was an effective method for teaching surgical anatomy and planning and was well accepted by trainees.',\n",
       " 'To assess the content validity and concurrent validity of a haptically (force feedback) rendered, virtual reality simulation of temporal bone surgery.Eleven naive surgical trainees were given a 1-hour lesson on the operation, cortical mastoidectomy, in the virtual environment with the trainer on a networked simulator and then asked to perform this procedure on a real temporal bone.The simulator was found to be a convincing representation of temporal bone drilling and could be said to exhibit face validity. The simulator was an effective means of teaching both the surgical anatomy and the surgical approach as judged by oral assessments made before and after the virtual reality training session. The trainees were successful in identifying most surgical landmarks during their first temporal bone dissection, and over two thirds found the landmarks at the correct time during the procedure. Some trainees exhibited acceptable or better technique with the drill despite this being their first temporal bone dissection. Subjective assessments indicated a high level of acceptance of simulated surgery for training, and there was perceived value in specific enhancements of the virtual environment that facilitated learning. Particular enhancements of value were the networked simulation, the option to make the model semitransparent to reveal anatomic relationships, and error reporting when the sigmoid sinus or facial nerve was injured.Virtual reality simulation of temporal bone surgery was an effective method for teaching surgical anatomy and planning and was well accepted by trainees.',\n",
       " \"Interaction in conversational interfaces strongly relies on the system's capability to interpret the user's references to objects via deictic expressions. Deictic gestures, especially pointing gestures, provide a powerful way of referring to objects and places, e.g., when communicating with an Embodied Conversational Agent in a Virtual Reality Environment. We highlight results drawn from a study on pointing and draw conclusions for the implementation of pointing-based conversational interactions in partly immersive Virtual Reality.\",\n",
       " 'This paper explores the idea that the universe is a virtual reality created by information processing, and relates this strange idea to the findings of modern physics about the physical world. The virtual reality concept is familiar to us from online worlds, but our world as a virtual reality is usually a subject for science fiction rather than science. Yet logically the world could be an information simulation running on a multi-dimensional space-time screen. Indeed, if the essence of the universe is information, matter, charge, energy and movement could be aspects of information, and the many conservation laws could be a single law of information conservation. If the universe were a virtual reality, its creation at the big bang would no longer be paradoxical, as every virtual system must be booted up. It is suggested that whether the world is an objective reality or a virtual reality is a matter for science to resolve. Modern information science can suggest how core physical properties like space, time, light, matter and movement could derive from information processing. Such an approach could reconcile relativity and quantum theories, with the former being how information processing creates space-time, and the latter how it creates energy and matter.',\n",
       " 'Augmented reality is an innovation which allows a user to the computer simulated environment, regardless of whether that condition is a reproduction of this present reality or a conjured up universe. It is the way to encountering, feeling and contacting the past, present and whats to come. It is the medium of making our very own reality, our own customized reality. It could go from making a computer game to having a virtual walk around the universe, from strolling through our very own fantasy house to encountering a stroll on an outsider planet. With computer generated reality, we can encounter the scariest and exhausting circumstances by playing safe and with a learning point of view. Not many individuals, be that as it may, truly realize what VR is, the thing that it is fundamental standards and its open issues are. In this paper a chronicled outline of computer generated reality is displayed, essential wording and classes of VR frameworks are recorded. A savvy investigation of normal VR frameworks is done and finds the difficulties of Virtual Reality. Augmented reality, in which virtual substance is reliably planned with grandstands of genuine scenes, is a creating zone of natural arrangement. With the climb of individual cellphones prepared for making charming augmented reality conditions, the tremendous ability of AR has begun to be examined. This paper audits the present forefront in expanded reality. It delineates work performed in different application territories and clears up the leaving issues experienced when building extended reality applications considering the ergonomic besides, specialized confinements of cell phones. Pratibha Jha | Sapna Yadav \"Virtual and Augmented Reality: An Overview\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-3 , April 2019, URL: https://www.ijtsrd.com/papers/ijtsrd23351.pdf',\n",
       " 'In this paper, we propose the use of specific system architecture, based on mobile device, for navigation in urban environments. The aim of this work is to assess how virtual and augmented reality interface paradigms can provide enhanced location based services using real-time techniques in the context of these two different technologies. The virtual reality interface is based on faithful graphical representation of the localities of interest, coupled with sensory information on the location and orientation of the user, while the augmented reality interface uses computer vision techniques to capture patterns from the real environment and overlay additional way-finding information, aligned with real imagery, in real-time. The knowledge obtained from the evaluation of the virtual reality navigational experience has been used to inform the design of the augmented reality interface. Initial results of the user testing of the experimental augmented reality system for navigation are presented.',\n",
       " 'This paper explores how Virtual Reality VR systems have been used as a rehabilitation tool for disabled population. Reviews were done in applications of virtual reality in patients with neurological disorders, visual impairment, psychiatric problems, children with physical disability and neurodevelopment disorders. This article mainly focused on the current status and use of virtual reality for children with autism. Literature was reviewed and the important findings are discussed in this paper. The virtual reality systems and designs, interventions method, treatment intensity and its effectiveness in target population were analyzed. The following skills emotion recognition, contextual processing, social attribution and executive function of analogical reasoning, navigation performance, safety skills, social interaction, motor and cognitive skills, conversational understanding were found to be improved in children with autism spectrum disorder by using VR. The paper also detailed the studies done in India using virtual reality in the disability field, mainly in autistic population. Sinitha. K. M | Stephy Jacob | Dr. Maria Grace Treasa \"Virtual Reality as a Promising Tool for Autism Intervention\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https://www.ijtsrd.compapers/ijtsrd42523.pdf Paper URL: https://www.ijtsrd.comother-scientific-research-area/other/42523/virtual-reality-as-a-promising-tool-for-autism-intervention/sinitha-k-m',\n",
       " 'The paper presents a model Virtual Reality framework for learning of activity in a clever industrial facility, working as per the idea of Industry 4.0. The Smart Factory labâ€”illustration of some portion of savvy processing plantâ€”is depicted. The VR model framework and cycle of its structure is additionally introduced, beginning from digitalization of the genuine brilliant plant, through rationale programming also, association of fringe VR gadgets. Elements of the preparation framework are introduced, alongside bearings of future investigations and advancement. Akash Parmar | Mohit Singh Tomar | Dr. Ritu Shivastava \"Virtual Reality Training in Smart Factory: A Perspective View\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-6 , October 2021, URL: https://www.ijtsrd.com/papers/ijtsrd47482.pdf Paper URL : https://www.ijtsrd.com/engineering/computer-engineering/47482/virtual-reality-training-in-smart-factory-a-perspective-view/akash-parmar',\n",
       " \"Virtual reality simulation is increasingly being incorporated into surgical training and may have a role in temporal bone surgical education. Here we test whether metrics generated by a virtual reality surgical simulation can differentiate between three levels of experience, namely novices, otolaryngology residents, and experienced qualified surgeons.Cohort study.Royal Victorian Eye and Ear Hospital.Twenty-seven participants were recruited. There were 12 experts, six residents, and nine novices. After orientation, participants were asked to perform a modified radical mastoidectomy on the simulator. Comparisons of time taken, injury to structures, and forces exerted were made between the groups to determine which specific metrics would discriminate experience levels.Experts completed the simulated task in significantly shorter time than the other two groups (experts 22 minutes, residents 36 minutes, and novices 46 minutes; P = 0.001). Novices exerted significantly higher average forces when dissecting close to vital structures compared with experts (0.24 Newton N vs 0.13 N, P = 0.002). Novices were also more likely to injure structures such as dura compared to experts (23 injuries vs 3 injuries, P = 0.001). Compared with residents, the experts modulated their force between initial cortex dissection and dissection close to vital structures. Using the combination of these metrics, we were able to correctly classify the participants' level of experience 90 percent of the time.This preliminary study shows that measurements of performance obtained from within a virtual reality simulator can differentiate between levels of users' experience. These results suggest that simulator training may have a role in temporal bone training beyond foundational training.\",\n",
       " 'In this paper, we present a virtual audience simulation system for Virtual Reality (VR). The system implements an audience perception model controlling the nonverbal behaviors of virtual spectators, such as facial expressions or postures. Groups of virtual spectators are animated by a set of nonverbal behavior rules representing a particular audience attitude (e.g., indifferent or enthusiastic). Each rule specifies a nonverbal behavior category: posture, head movement, facial expression and gaze direction as well as three parameters: type, frequency and proportion. In a first user-study, we asked participants to pretend to be a speaker in VR and then create sets of nonverbal behaviour parameters to simulate different attitudes. Participants manipulated the nonverbal behaviours of single virtual spectator to match a specific levels of engagement and opinion toward them. In a second user-study, we used these parameters to design different types of virtual audiences with our nonverbal behavior rules and evaluated their perceptions. Our results demonstrate our system’s ability to create virtual audiences with three types of different perceived attitudes: indifferent, critical, enthusiastic. The analysis of the results also lead to a set of recommendations and guidelines regarding attitudes and expressions for future design of audiences for VR therapy and training applications.',\n",
       " 'Surgical training has traditionally been one of apprenticeship, where the surgical trainee learns to perform surgery under the supervision of a trained surgeon. This is time consuming, costly, and of variable effectiveness. Training using a virtual reality simulator is an option to supplement standard training.To determine whether virtual reality training can supplement or replace conventional laparoscopic surgical training (apprenticeship) in surgical trainees with limited or no prior laparoscopic experience.We searched The Cochrane Hepato-Biliary Group Controlled Trials Register, the Cochrane Central Register of Controlled Trials (CENTRAL) in The Cochrane Library, MEDLINE, EMBASE, Science Citation Index Expanded, and grey literature until March 2008.We included all randomised clinical trials comparing virtual reality training versus other forms of training including video trainer training, no training, or standard laparoscopic training in surgical trainees with little or no prior laparoscopic experience. We also included trials comparing different methods of virtual reality training.We collected the data on the characteristics of the trial, methodological quality of the trials, mortality, morbidity, conversion rate, operating time, and hospital stay. We analysed the data with both the fixed-effect and the random-effects models using RevMan Analysis. For each outcome we calculated the standardised mean difference with 95% confidence intervals based on intention-to-treat analysis.We included 23 trials with 612 participants. Four trials compared virtual reality versus video trainer training. Twelve trials compared virtual reality versus no training or standard laparoscopic training. Four trials compared virtual reality, video trainer training and no training, or standard laparoscopic training. Three trials compared different methods of virtual reality training. Most of the trials were of high risk of bias. In trainees without prior surgical experience, virtual reality training decreased the time taken to complete a task, increased accuracy, and decreased errors compared with no training; virtual reality group was more accurate than video trainer training group. In the participants with limited laparoscopic experience, virtual reality training reduces operating time and error better than standard in the laparoscopic training group; composite operative performance score was better in the virtual reality group than in the video trainer group.Virtual reality training can supplement standard laparoscopic surgical training of apprenticeship and is at least as effective as video trainer training in supplementing standard laparoscopic training. Further research of better methodological quality and more patient-relevant outcomes are needed.',\n",
       " \"The first of two articles discusses virtual reality (VR) and online databases; the second one reports on an interview with Thomas A. Furness III, who defines VR and explains work at the Human Interface Technology Laboratory (HIT). Sidebars contain a glossary of VR terms and a conversation with Toni Emerson, the HIT lab's librarian. (LRW)\",\n",
       " 'Virtual reality (VR) offers an appealing experimental framework for studying visual performances of insects under highly controlled conditions. In the case of the honeybee Apis mellifera, this possibility may fill the gap between behavioural analyses in free-flight and cellular analyses in the laboratory. Using automated, computer-controlled systems, it is possible to generate virtual stimuli or even entire environments that can be modified to test hypotheses on bee visual behaviour. The bee itself can remain tethered in place, making it possible to record neural activity while the bees is performing behavioural tasks. Recent studies have examined visual navigation and attentional processes in VR on flying or walking tethered bees, but experimental paradigms for examining visual learning and memory are only just emerging. Behavioural performances of bees under current experimental conditions are often lower in VR than in natural environments, but further improvements on current experimental protocols seem possible. Here we discuss current developments and conclude that it is essential to tailor the specifications of the VR simulation to the visual processing of honeybees to improve the success of this research endeavour.',\n",
       " 'Tracking user’s visual attention is a fundamental aspect in novel human-computer interaction paradigms found in Virtual Reality. For example, multimodal interfaces or dialogue-based communications with virtual and real agents greatly benefit from the analysis of the user’s visual attention as a vital source for deictic references or turn-taking signals. Current approaches to determine visual attention rely primarily on monocular eye trackers. Hence they are restricted to the interpretation of two-dimensional fixations relative to a defined area of projection. The study presented in this article compares precision, accuracy and application performance of two binocular eye tracking devices. Two algorithms are compared which derive depth information as required for visual attention-based 3D interfaces. This information is further applied to an improved VR selection task in which a binocular eye tracker and an adaptive neural network algorithm is used during the disambiguation of partly occluded objects.',\n",
       " 'The authors present the experiences at the Electronic Visualization Laboratory (EVL) in introducing computational scientists to the use of virtual reality as a research tool. They describe the virtual environment, the CAVE. They then describe several applications currently being developed at EVL using the CAVE and conclude with a discussion on possible research paths to follow in making virtual reality an effective tool for visualization.&lt;<ETX>&gt;</ETX>',\n",
       " 'Virtual reality exposure therapy (VRET) is an altered form of behavioral therapy and may be a possible alternative to standard in vivo exposure. Virtual reality integrates real-time computer graphics, body tracking devices, visual displays, and other sensory input devices to immerse patients in a computer-generated virtual environment. Research on this type of treatment for anxiety disorders is discussed in this article, and the mediating and moderating variables that influence VR treatment effectiveness as well. Evidence is found that VRET is effective for participants with fear of heights and of flying. For other phobias, research to date is not conclusive. More randomized clinical trials in which VRET is compared with standard exposure are required. Furthermore, studies are needed in which VRET is not just a component of the treatment package evaluated, but in which VRET should be assessed as a stand-alone treatment.',\n",
       " 'A low latency is a fundamental timeliness requirement to reduce the potential risks of cyber sickness and to increase effectiveness, efficiency, and user experience of Virtual Reality Systems. The effects of uniform latency degradation based on mean or worst-case values are well researched. In contrast, the effects of latency jitter, the distribution pattern of latency changes over time has largely been ignored so far although today\\\\u0027s consumer VR systems are extremely vulnerable in this respect. We investigate the applicability of the Walsh, generalized ESD, and the modified z-score test for the detection of outliers as one central latency distribution aspect. The tests are applied to well defined test cases mimicking typical timing behavior expected from concurrent architectures of today. We introduce accompanying graphical visualization methods to inspect, analyze and communicate the latency behavior of VR systems beyond simple mean or worst-case values. As a result, we propose a stacked modified z-score test for more detailed analysis.',\n",
       " 'A low latency is a fundamental timeliness requirement to reduce the potential risks of cyber sickness and to increase effectiveness, efficiency, and user experience of Virtual Reality Systems. The effects of uniform latency degradation based on mean or worst-case values are well researched. In contrast, the effects of latency jitter, the distribution pattern of latency changes over time has largely been ignored so far although today\\\\u0027s consumer VR systems are extremely vulnerable in this respect. We investigate the applicability of the Walsh, generalized ESD, and the modified z-score test for the detection of outliers as one central latency distribution aspect. The tests are applied to well defined test cases mimicking typical timing behavior expected from concurrent architectures of today. We introduce accompanying graphical visualization methods to inspect, analyze and communicate the latency behavior of VR systems beyond simple mean or worst-case values. As a result, we propose a stacked modified z-score test for more detailed analysis.',\n",
       " 'Objective: We evaluated the evidence supporting the use of virtual reality among patients in acute inpatient medical settings. Method: We conducted a systematic review of randomized controlled trials conducted that examined virtual reality applications in inpatient medical settings between 2005 and 2015. We used PsycINFO, PubMed, and Medline databases to identify studies using the keywords virtual reality, VR therapy, treatment, and inpatient.Results: We identified 2,024 citations, among which 11 met criteria for inclusion. Studies addressed three general areas: pain management, eating disorders, and cognitive and motor rehabilitation. Studies were small and heterogeneous and utilized different designs and measures. Virtual reality was generally well tolerated by patients, and a majority of studies demonstrated clinical efficacy. Studies varied in quality, as measured by an evaluation metric developed by Reisch, Tyson, and Mize (average quality score=0.87; range=0.78-0.96). Conclusion: Virtual reality is a promising intervention with several potential applications in the inpatient medical setting. Studies to date demonstrate some efficacy, but there is a need for larger, well-controlled studies to show clinical and cost-effectiveness.',\n",
       " 'To validate the VOXEL-MAN TempoSurg simulator for temporal bone dissection.Prospective international study.Otolaryngology departments of 2 academic health care institutions in the United Kingdom and United States.Eighty-five subjects were recruited consisting of an experienced and referent group. Participants performed a standardized familiarization session and temporal bone dissection task. Realism, training effectiveness, and global impressions were evaluated across 21 domains using a 5-point Likert-type scale. A score of 4 was the minimum threshold for acceptability.The experienced group comprised 25 otolaryngology trainers who had performed 150 mastoid operations. The referent group comprised 60 trainees (mean otolaryngology experience of 2.9 years). Familiarization took longer in the experienced group (P = .01). User-friendliness was positively rated (mean score 4.1). Seventy percent of participants rated anatomical appearance as acceptable. Trainers rated drill ergonomics worse than did trainees (P = .01). Simulation temporal bone training scored highly (mean score 4.3). Surgical anatomy, drill navigation, and hand-eye coordination accounted for this. Trainees were more likely to recommend temporal bone simulation to a colleague than were trainers (P = .01). Transferability of skills to the operating room was undecided (mean score 3.5).Realism of the VOXEL-MAN virtual reality temporal bone simulator is suboptimal in its current version. Nonetheless, it represents a useful adjunct to existing training methods and is particularly beneficial for novice surgeons before performing cadaveric temporal bone dissection. Improvements in realism, specifically drill ergonomics and visual-spatial perception during deeper temporal bone dissection, are warranted.',\n",
       " \"This article presents a novel method for controlling a virtual audience system (VAS) in Virtual Reality (VR) application, called STAGE, which has been originally designed for supervised public speaking training in university seminars dedicated to the preparation and delivery of scientific talks. We are interested in creating pedagogical narratives: narratives encompass affective phenomena and rather than organizing events changing the course of a training scenario, pedagogical plans using our system focus on organizing the affects it arouses for the trainees. Efficiently controlling a virtual audience towards a specific training objective while evaluating the speaker's performance presents a challenge for a seminar instructor: the high level of cognitive and physical demands required to be able to control the virtual audience, whilst evaluating speaker's performance, adjusting and allowing it to quickly react to the user’s behaviors and interactions. It is indeed a critical limitation of a number of existing systems that they rely on a Wizard of Oz approach, where the tutor drives the audience in reaction to the user’s performance. We address this problem by integrating with a VAS a high-level control component for tutors, which allows using predefined audience behavior rules, defining custom ones, as well as intervening during run-time for finer control of the unfolding of the pedagogical plan. At its core, this component offers a tool to program, select, modify and monitor interactive training narratives using a high-level representation. The STAGE offers the following features: i) a high-level API to program pedagogical narratives focusing on a specific public speaking situation and training objectives, ii) an interactive visualization interface iii) computation and visualization of user metrics, iv) a semi-autonomous virtual audience composed of virtual spectators with automatic reactions to the speaker and surrounding spectators while following the pedagogical plan V) and the possibility for the instructor to embody a virtual spectator to ask questions or guide the speaker from within the Virtual Environment. We present here the design, implementation of the tutoring system and its integration in STAGE, and discuss its reception by end-users.\",\n",
       " 'Motion perception in immersive virtual reality environments significantly differs from the real world. For example, previous work has shown that users tend to underestimate travel distances in immersive virtual environments (VEs). As a solution to this problem, some researchers propose to scale the mapped virtual camera motion relative to the tracked real-world movement of a user until real and virtual motion appear to match. In such a way, real-world movements could be mapped with a larger gain to the VE in order to compensate for the underestimation. Although this approach usually results in more accurate self-motion judgments by users, introducing discrepancies between real and virtual motion can become a problem, in particular, due to misalignments of both worlds and distorted space cognition. In this paper we describe a different approach that introduces apparent self-motion illusions by manipulating optic flow fields during movements in VEs. These manipulations can affect self-motion perception in VEs, but omit a quantitative discrepancy between real and virtual motions. We introduce four illusions and show in experiments that optic flow manipulation can significantly affect users’ self-motion judgments. Furthermore, we show that with such manipulation of optic flow fields the underestimation of travel distances can be compensated.',\n",
       " 'To assess whether practice on a virtual-reality (VR) temporal bone simulator improves acquisition of technical skills in mastoid surgery.Prospective blinded study.Using a previously validated objective structured assessment of technical skills (OSATS) tool, performance was assessed in 12 residents for two tasks of cortical mastoidectomy: 1) identifying and defining the tegmen and 2) defining the sigmoid sinus and sinodural angle. These surgical tasks were chosen as key steps in mastoid dissection because they were of intermediate complexity. Videos of virtual dissections were captured at baseline and again after practicing each task four to six times.OSATS scores for the tegmen task increased from 2.125 � 1.25 to 3.1 � 0.85 (P = .026), whereas for the sigmoid task scores increased from 2 � 0.45 to 2.75 � 1.125 (P = .0098). The time to complete the tasks decreased from 8.37 � 4.78 minutes to 5.39 � 3.06 minutes (P = .018) for the tegmen task and from 8.99 � 6.7 minutes to 8.68 � 5.98 minutes (P = .594) for the sigmoid task. There was a decline in number of injuries from 0.5 � 1.5 to 0 � 0.5 (P = .594) for the tegmen task and from 2.5 � 4 to 0.5 � 1 (P = .029) for the sigmoid task.Technical skills in mastoidectomy surgery can be acquired during even brief practice on the VR temporal bone simulator. It is anticipated that longer periods of practice presented within the fundamentals of comprehensive curriculum will facilitate procedural learning. Further studies are required to elucidate evidence of transference of these skills to the operating room and to procedures of greater complexity.',\n",
       " 'In this paper we propose the concepts of virtual reflections, lights and shadows to enhance immersion in mixed reality (MR) environments, which focus on merging the real and the virtual world seamlessly. To improve immersion, we augment the virtual objects with real world information regarding the virtual reality (VR) system environment, e.g., CAVE, workbench etc. Real-world objects such as input devices or light sources as well as the position and posture of the user are used to simulate global illumination phenomena, e.g., users can see their own reflections and shadows on virtual objects. Besides the concepts and the implementation of this approach, we describe the system setup. and an example application for this kind of advanced MR system environment.',\n",
       " 'Surgical simulation is becoming an increasingly common training tool in residency programs. The first objective was to implement real-time soft-tissue deformation and cutting into a virtual reality myringotomy simulator. The second objective was to test the various implemented incision algorithms to determine which most accurately represents the tympanic membrane during myringotomy.Descriptive and face-validity testing.A deformable tympanic membrane was developed, and three soft-tissue cutting algorithms were successfully implemented into the virtual reality myringotomy simulator. The algorithms included element removal, direction prediction, and Delaunay cutting. The simulator was stable and capable of running in real time on inexpensive hardware. A face-validity study was then carried out using a validated questionnaire given to eight otolaryngologists and four senior otolaryngology residents. Each participant was given an adaptation period on the simulator, was blinded to the algorithm being used, and was presented the three algorithms in a randomized order.A virtual reality myringotomy simulator with real-time soft-tissue deformation and cutting was successfully developed. The simulator was stable, ran in real time on inexpensive hardware, and incorporated haptic feedback and stereoscopic vision. The Delaunay cutting algorithm was found to be the most realistic algorithm representing the incision during myringotomy (P < .05). The Likert and visual analog scales had strong correlations, suggesting good internal reliability.The first virtual reality myringotomy simulator is being developed and now integrates a real-time deformable tympanic membrane that appears to have face validity. Further development and validation studies are necessary before the simulator can be studied with respect to training efficacy and clinical impact.',\n",
       " 'Aerial photographs play a major role in current remote sensing applications. Traditionally those photographs were acquired using airplanes or satellites, and later manually processed in order to generate a virtual environment. Recently, Miniature Unmanned Aerial Vehicles (MUAVs) equipped with digital cameras have been proposed as inexpensive and more flexible alternative to acquire aerial images. The industrial research project Avionic Digital Service Platform (AVIGLE) explores novel approaches to remote sensing by using a swarm of MUAVs, which are equipped with different sensing and network technologies. The acquired data will be sent in quasi-real time to a flight ground control station, where a virtual environment will be generated automatically. While this approach has the potential to provide an efficient alternative to the traditional remote sensing processes, it opens up numerous research and engineering challenges. In this paper we describe a virtual reality-based simulation framework, which has been developed in the scope of the AVIGLE project. Since the project involves a lot of risks, the simulation of the entire system in a VR-based environment is essential to create a controlled virtual environment that is independent of hardware, flight permissions and influences like weather and lighting conditions. During the project the use of VR is not limited to presentation purposes, but also serves as testbed to simulate and evaluate different hardware setups and software algorithms.',\n",
       " 'We report on the initial phase of an ongoing, multi-stage investigation of how to incorporate Virtual Reality (VR) technology in teaching introductory astronomy concepts. Our goal was to compare the efficacy of VR vs. conventional teaching methods using one specific topic, Moon phases and eclipses. After teaching this topic to an ASTRO 101 lecture class, students were placed into three groups to experience one of three additional activities: supplemental lecture, \"hands-on\" activity, or VR experience. All students were tested before and after their learning activity. Although preliminary, our results can serve as a useful guide to expanding the role of VR in the astronomy classroom.',\n",
       " 'In this paper, we describe an experimental method to investigate the effects of reduced social information and behavioral channels in immersive virtual environments with full-body avatar embodiment. We compared physical-based and verbal-based social interactions in real world (RW) and virtual reality (VR). Participants were represented by abstract avatars that did not display gaze, facial expressions or social cues from appearance. Our results show significant differences in terms of presence and physical performance. However, differences in effectiveness in the verbal task were not present. Participants appear to efficiently compensate for missing social and behavioral cues by shifting their attentions to other behavioral channels.',\n",
       " 'Involving users in the early stages of design has implications for the development, usability, acceptance and implementation of new computer systems. A project exploring the practical application of virtual reality to stroke assessment recently commenced at the University of Nottingham, with an emphasis on user centred design. A consortium of stroke therapists and researchers has guided the direction of the project through their involvement at the early planning stage. The consortium has provided broad guidelines for design, potential applications and identified barriers to this technology being routinely used in stroke assessment. This paper describes the process of introducing stroke therapists to virtual reality and presents their views on how it could be applied to stroke assessment.',\n",
       " 'In this paper, we describe an experimental method to investigate the effects of reduced social information and behavioral channels in immersive virtual environments with full-body avatar embodiment. We compared physical-based and verbal-based social interactions in real world (RW) and virtual reality (VR). Participants were represented by abstract avatars that did not display gaze, facial expressions or social cues from appearance. Our results show significant differences in terms of presence and physical performance. However, differences in effectiveness in the verbal task were not present. Participants appear to efficiently compensate for missing social and behavioral cues by shifting their attentions to other behavioral channels.',\n",
       " 'In this paper, we describe an experimental method to investigate the effects of reduced social information and behavioral channels in immersive virtual environments with full-body avatar embodiment. We compared physical-based and verbal-based social interactions in real world (RW) and virtual reality (VR). Participants were represented by abstract avatars that did not display gaze, facial expressions or social cues from appearance. Our results show significant differences in terms of presence and physical performance. However, differences in effectiveness in the verbal task were not present. Participants appear to efficiently compensate for missing social and behavioral cues by shifting their attentions to other behavioral channels.',\n",
       " 'The use of immersive virtual reality (VR) systems in muse-ums is a recent trend, as the development of new interactive technologies has inevitably impacted the more traditional sciences and arts. This is more evident in the case of novel interactive technologies that fascinate the broad public, as has always been the case with virtual reality. The increas-ing development of VR technologies has matured enough to expand research from the military and scientific visuali-zation realm into more multidisciplinary areas, such as edu-cation, art and entertainment. This paper analyzes the inter-active virtual environments developed at an institution of informal education and discusses the issues involved in de-veloping immersive interactive virtual archaeology projects for the broad public.',\n",
       " 'Tracking user\\\\u0027s visual attention is a fundamental aspect in novel human-computer interaction paradigms found in Virtual Reality. For example, multimodal interfaces or dialogue-based communications with virtual and real agents greatly benefit from the analysis of the user\\\\u0027s visual attention as a vital source for deictic references or turn-taking signals. Current approaches to determine visual attention rely primarily on monocular eye trackers. Hence they are restricted to the interpretation of two-dimensional fixations relative to a defined area of projection. The study presented in this article compares precision, accuracy and application performance of two binocular eye tracking devices. Two algorithms are compared which derive depth information as required for visual attention-based 3D interfaces. This information is further applied to an improved VR selection task in which a binocular eye tracker and an adaptive neural network algorithm is used during the disambiguation of partly occluded objects.',\n",
       " \"Tracking user's visual attention is a fundamental aspect in novel human-computer interaction paradigms found in Virtual Reality. For example, multimodal interfaces or dialogue-based communications with virtual and real agents greatly benefit from the analysis of the user's visual attention as a vital source for deictic references or turn-taking signals. Current approaches to determine visual attention rely primarily on monocular eye trackers. Hence they are restricted to the interpretation of two-dimensional fixations relative to a defined area of projection. The study presented in this article compares precision, accuracy and application performance of two binocular eye tracking devices. Two algorithms are compared which derive depth information as required for visual attention-based 3D interfaces. This information is further applied to an improved VR selection task in which a binocular eye tracker and an adaptive neural network algorithm is used during the disambiguation of partly occluded objects.\",\n",
       " 'To evaluate construct validity of the Voxelman TempoSurg Virtual Reality (VR) temporal bone simulator by determining whether generated objective metrics can distinguish experienced otologic surgeons from intermediate and novice surgeons.Prospective assessment study.Two university-affiliated teaching hospitals.Sixty-five participants were recruited; 40 novice surgeons, 15 trainees in otolaryngology, and 10 experienced otolaryngology consultants with a specialist interest in otology were individually assessed on a standardized simulated temporal bone task. The task involved identification and delineation of the sigmoid sinus in a virtual left-sided temporal bone.Objective data were produced using a scoring matrix incorporated into the VOXEL MAN TempoSurg software. The simulator measured the total time taken to complete the task, the volume and efficiency of bone removal and error data for excessive force or injury to the facial nerve, dura, and sigmoid sinus.Experts and intermediates outperformed novices with respect to the total time taken to complete the task (expert versus novice: p < 0.001; intermediate versus novice: p < 0.001), total volume of bone removed (p < 0.001 and p = 0.03), efficiency of bone removal (p < 0.001 and p < 0.001), time spent with the drill tip obscured (p = 0.002 and p < 0.001), and number of injuries to the sigmoid sinus (p < 0.001 and p < 0.001). The intermediate group injured the sigmoid sinus on more occasions than the experts (p = 0.008) and were less efficient than experienced surgeons (p = 0.005).Simulator-generated objective metrics can be used to differentiate individuals of differing levels of experience using a standardized temporal bone task. VR simulation has potential as a training tool and may have a role in both formative and summative assessment.',\n",
       " 'This paper describes the methodological aspects of the application of various established and new graphics techniques in virtual reality applications, in order to visually enrich conventional walkthroughs and extend the common capabilities of virtual environment visualization platforms. The paper describes these techniques and goes to the extent of explaining various practical implementation issues. Examples and application case studies are provided to demonstrate the enhancements.',\n",
       " 'We describe an augmented reality (AR) system that allows multiple participants to interact with 2D and 3D data using tangible user interfaces. The system features face-to-face communication, collaborative viewing and manipulation of 3D models, and seamless access to 2D desktop applications within the shared 3D space. All virtual content, including 3D models and 2D desktop windows, is attached to tracked physical objects in order to leverage the efficiencies of natural two-handed manipulation. The presence of 2D desktop space within 3D facilitates data exchange between the two realms, enables control of 3D information by 2D applications, and generally increases productivity by providing access to familiar tools. We present a general concept for a collaborative tangible AR system, including a comprehensive set of interaction techniques, a distributed hardware setup, and a component-based software architecture that can be flexibly configured using XML. We show the validity of our concept with an implementation of an application scenario from the automotive industry.',\n",
       " \"Virtual reality (VR) systems utilize additional input and output channels in order to make interaction in virtual environments (VEs) more intuitive and to increase the user's immersion into the virtual world. When developing VR applications, developers should be able to focus on modeling advanced interaction and system behavior instead of rendering issues. Many systems and tools for developing virtual reality applications have been proposed to achieve this goal. However, no de facto standard is available. In this paper we present Virtual Reality VRS (VR2S), a generic VR software system, which is an extension of the high-level rendering system VRS. The system provides flexibility in terms of the rendering system and the user interface toolkit. Thus, with using VR2SÂ rendering can be performed with several low-level rendering APIs such as OpenGL, RenderMan or ray-tracing systems, and the interface can be implemented by arbitrary user interface toolkits to support both desktop- and VR-based interaction. The proposed system meets the demands of VR developers as well as users and has demonstrated its potential in different planning and exploration applications.\",\n",
       " \"Virtual Reality Systems is an accessible introduction to the underlying technologies: real-time computer graphics, color displays and simulation software used to create today's virtual environment systems. It provides a balanced coverage of both hardware and software issues. Optional explanations of the underlying mathematical algorithms and techniques are included.\",\n",
       " \"During the past decades, the virtual reality community has based its development on a synthesis of earlier work in interactive 3D graphics, user interfaces, and visual simulation. Currently, the VR field is transitioning into work influenced by video games. Because much of the research and development being conducted in the games community parallels the VR community's efforts, it has the potential to affect a greater audience. Given these trends, VR researchers who want their work to remain relevant must realign to focus on game research and development. Leveraging technology from the visual simulation and virtual reality communities, serious games provide a delivery system for organizational video game instruction and training.\",\n",
       " \"This paper presents an immersive Virtual Reality (VR) therapy system for gait rehabilitation after neurological impairments, e.g., caused by accidents or strokes: The system targets increase of patients' motivation to perform the repeated exercise by providing stimulating virtual exercise environments with the final goal to increase therapy efficiency and effectiveness. Instead of simply working out on immobile stationary devices, the system allows them to walk through and explore a stimulating virtual world. Patients are immersed in the virtual environments using a Head-Mounted Display (HMD). Walking patterns are captured by motion sensors attached to the patients' feet to synchronize locomotion speed between the real and the virtual world. A user-centered design process evaluated usability, user experience, and feasibility to confirm the overall goals of the system before any sensitive clinical trials with impaired patients can start. Overall, the results demonstrated an encouraging user experience and acceptance while it did not induce any unwanted side-effects, e.g., nausea or cyber-sickness.\",\n",
       " \"A significant benefit of virtual reality (VR) simulation is the ability to provide self-direct learning for trainees. This study aims to determine whether there are any differences in performance of cadaver temporal bone dissections between novices who received traditional teaching methods and those who received unsupervised self-directed learning in a VR temporal bone simulator.Randomized blinded control trial.Royal Victorian Eye and Ear Hospital.Twenty novice trainees.After receiving an hour lecture, participants were randomized into 2 groups to receive an additional 2 hours of training via traditional teaching methods or self-directed learning using a VR simulator with automated guidance. The simulation environment presented participants with structured training tasks, which were accompanied by real-time computer-generated feedback as well as real operative videos and photos. After the training, trainees were asked to perform a cortical mastoidectomy on a cadaveric temporal bone. The dissection was videotaped and assessed by 3 otologists blinded to participants' teaching group.The overall performance scores of the simulator-based training group were significantly higher than those of the traditional training group (67% vs 29%; P < .001), with an intraclass correlation coefficient of 0.93, indicating excellent interrater reliability. Using other assessments of performance, such as injury size, the VR simulator-based training group also performed better than the traditional group.This study indicates that self-directed learning on VR simulators can be used to improve performance on cadaver dissection in novice trainees compared with traditional teaching methods alone.\",\n",
       " \"The objective of this study is to determine the feasibility of computerized evaluation of resident performance using hand motion analysis on a virtual reality temporal bone (VR TB) simulator. We hypothesized that both computerized analysis and expert ratings would discriminate the performance of novices from experienced trainees. We also hypothesized that performance on the virtual reality temporal bone simulator (VR TB) would differentiate based on previous drilling experience.The authors conducted a randomized, blind assessment study.Nineteen volunteers from the Otolaryngology-Head and Neck Surgery training program at the University of Toronto drilled both a cadaveric TB and a simulated VR TB. Expert reviewers were asked to assess operative readiness of the trainee based on a blind video review of their performance. Computerized hand motion analysis of each participant's performance was conducted.Expert raters were able to discriminate novices from experienced trainees (P < .05) on cadaveric temporal bones, and there was a trend toward discrimination on VR TB performance. Hand motion analysis showed that experienced trainees had better movement economy than novices (P < .05) on the VR TB.Performance, as measured by hand motion analysis on the VR TB simulator, reflects trainees' previous drilling experience. This study suggests that otolaryngology trainees could accomplish initial temporal bone training on a VR TB simulator, which can provide feedback to the trainee, and may reduce the need for constant faculty supervision and evaluation.\",\n",
       " \"Temporal reprojection is a popular method for mitigating sampling artifacts from a variety of sources. This work investigates it's impact on the subjective quality of specular reflections in Virtual Reality(VR). Our results show that temporal reprojection is highly effective at improving the visual comfort of specular materials, especially at low sample counts. A slightly diminished effect could also be observed in improving the subjective accuracy of the resulting reflection.\",\n",
       " \"Temporal reprojection is a popular method for mitigating sampling artifacts from a variety of sources. This work investigates it's impact on the subjective quality of specular reflections in Virtual Reality(VR). Our results show that temporal reprojection is highly effective at improving the visual comfort of specular materials, especially at low sample counts. A slightly diminished effect could also be observed in improving the subjective accuracy of the resulting reflection.\",\n",
       " 'Social Virtual Reality (social VR or SVR) provides digital spaces for diverse human activities, social interactions, and embodied face-to-face encounters. While our digital bodies in SVR can in general be of almost any conceivable appearance, individualized or even personalized avatars bearing users’ likeness recently became an interesting research topic. Such digital bodies show a great potential to enhance the authenticity of social VR citizens and increase the trustworthiness of interpersonal interaction. However, using such digital bodies might expose users to privacy and identity issues such as identity theft: For instance, how do we know whether the avatars we encounter in the virtual world are who they claim to be? Safeguarding users’ identities and privacy, and preventing harm from identity infringement, are crucial to the future of social VR. This article provides a systematic review on the protection of users’ identity and privacy in social VR, with a specific focus on digital bodies. Based on 814 sources, we identified and analyzed 49 papers that either: 1) discuss or raise concerns about the addressed issues, 2) provide technologies and potential solutions for protecting digital bodies, or 3) examine the relationship between the digital bodies and users of social VR citizens. We notice a severe lack of research and attention on the addressed topic and identify several research gaps that need to be filled. While some legal and ethical concerns about the potential identity issues of the digital bodies have been raised, and despite some progress in specific areas such as user authentication has been made, little research has proposed practical solutions. Finally, we suggest potential future research directions for digital body protection and include relevant research that might provide insights. We hope this work could provide a good overview of the existing discussion, potential solutions, and future directions for researchers with similar concerns. We also wish to draw attention to identity and privacy issues in social VR and call for interdisciplinary collaboration.',\n",
       " 'Social Virtual Reality (social VR or SVR) provides digital spaces for diverse human activities, social interactions, and embodied face-to-face encounters. While our digital bodies in SVR can in general be of almost any conceivable appearance, individualized or even personalized avatars bearing users’ likeness recently became an interesting research topic. Such digital bodies show a great potential to enhance the authenticity of social VR citizens and increase the trustworthiness of interpersonal interaction. However, using such digital bodies might expose users to privacy and identity issues such as identity theft: For instance, how do we know whether the avatars we encounter in the virtual world are who they claim to be? Safeguarding users’ identities and privacy, and preventing harm from identity infringement, are crucial to the future of social VR. This article provides a systematic review on the protection of users’ identity and privacy in social VR, with a specific focus on digital bodies. Based on 814 sources, we identified and analyzed 49 papers that either: 1) discuss or raise concerns about the addressed issues, 2) provide technologies and potential solutions for protecting digital bodies, or 3) examine the relationship between the digital bodies and users of social VR citizens. We notice a severe lack of research and attention on the addressed topic and identify several research gaps that need to be filled. While some legal and ethical concerns about the potential identity issues of the digital bodies have been raised, and despite some progress in specific areas such as user authentication has been made, little research has proposed practical solutions. Finally, we suggest potential future research directions for digital body protection and include relevant research that might provide insights. We hope this work could provide a good overview of the existing discussion, potential solutions, and future directions for researchers with similar concerns. We also wish to draw attention to identity and privacy issues in social VR and call for interdisciplinary collaboration.',\n",
       " 'Measurements of physiological parameters provide an objective, often non-intrusive, and (at least semi-)automatic evaluation and utilization of user behavior. In addition, specific hardware devices of Virtual Reality (VR) often ship with built-in sensors, i.e. eye-tracking and movements sensors. Hence, the combination of physiological measurements and VR applications seems promising. Several approaches have investigated the applicability and benefits of this combination for various fields of applications. However, the range of possible application fields, coupled with potentially useful and beneficial physiological parameters, types of sensor, target variables and factors, and analysis approaches and techniques is manifold. This article provides a systematic overview and an extensive state-of-the-art review of the usage of physiological measurements in VR. We identified 1,119 works that make use of physiological measurements in VR. Within these, we identified 32 approaches that focus on the classification of characteristics of experience, common in VR applications. The first part of this review categorizes the 1,119 works by field of application, i.e. therapy, training, entertainment, and communication and interaction, as well as by the specific target factors and variables measured by the physiological parameters. An additional category summarizes general VR approaches applicable to all specific fields of application since they target typical VR qualities. In the second part of this review, we analyze the target factors and variables regarding the respective methods used for an automatic analysis and, potentially, classification. For example, we highlight which measurement setups have been proven to be sensitive enough to distinguish different levels of arousal, valence, anxiety, stress, or cognitive workload in the virtual realm. This work may prove useful for all researchers wanting to use physiological data in VR and who want to have a good overview of prior approaches taken, their benefits and potential drawbacks.',\n",
       " \"Despite the increasing use of virtual reality, the impact on cerebral representation of topographical knowledge of learning by virtual reality rather than by actual locomotion has never been investigated. To tackle this challenging issue, we conducted an experiment wherein participants learned an immersive virtual environment using a joystick. The following day, participants' brain activity was monitored by functional magnetic resonance imaging while they mentally estimated distances in this environment. Results were compared with that of participants performing the same task but having learned the real version of the environment by actual walking. We detected a large set of areas shared by both groups including the parieto-frontal areas and the parahippocampal gyrus. More importantly, although participants of both groups performed the same mental task and exhibited similar behavioral performances, they differed at the brain activity level. Unlike real learners, virtual learners activated a left-lateralized network associated with tool manipulation and action semantics. This demonstrated that a neural fingerprint distinguishing virtual from real learning persists when subjects use a mental representation of the learnt environment with equivalent performances.\",\n",
       " \"Obesity is a serious disease that can affect both physical and psychological well-being. Due to weight stigmatization, many affected individuals suffer from body image disturbances whereby they perceive their body in a distorted way, evaluate it negatively, or neglect it. Beyond established interventions such as mirror exposure, recent advancements aim to complement body image treatments by the embodiment of visually altered virtual bodies in virtual reality (VR). We present a high-fidelity prototype of an advanced VR system that allows users to embody a rapidly generated personalized, photorealistic avatar and to realistically modulate its body weight in real-time within a carefully designed virtual environment. In a formative multi-method approach, a total of 12 participants rated the general user experience (UX) of our system during body scan and VR experience using semi-structured qualitative interviews and multiple quantitative UX measures. Using body weight modification tasks, we further compared three different interaction methods for real-time body weight modification and measured our system's impact on the body image relevant measures body awareness and body weight perception. From the feedback received, demonstrating an already solid UX of our overall system and providing constructive input for further improvement, we derived a set of design guidelines to guide future development and evaluation processes of systems supporting body image interventions.\",\n",
       " 'Obesity is a serious disease that can affect both physical and psychological well-being. Due to weight stigmatization, many affected individuals suffer from body image disturbances whereby they perceive their body in a distorted way, evaluate it negatively, or neglect it. Beyond established interventions such as mirror exposure, recent advancements aim to complement body image treatments by the embodiment of visually altered virtual bodies in virtual reality (VR). We present a high-fidelity prototype of an advanced VR system that allows users to embody a rapidly generated personalized, photorealistic avatar and to realistically modulate its body weight in real-time within a carefully designed virtual environment. In a formative multi-method approach, a total of 12 participants rated the general user experience (UX) of our system during body scan and VR experience using semi-structured qualitative interviews and multiple quantitative UX measures. Using body weight modification tasks, we further compared three different interaction methods for real-time body weight modification and measured our system’s impact on the body image relevant measures body awareness and body weight perception. From the feedback received, demonstrating an already solid UX of our overall system and providing constructive input for further improvement, we derived a set of design guidelines to guide future development and evaluation processes of systems supporting body image interventions.',\n",
       " \"The purpose of this paper was to examine the effects of virtual play intervention on the level of playfulness of children with cerebral palsy. Thirteen children aged 8--13 years comprised the study group. Children attended eight one-hour virtual reality play sessions in which they were immersed and interacted with virtual reality. The Test of Playfulness (TOP) was used as the measure to assess playfulness. Participants were videotaped while they played during 12 different environments over the course of their intervention time. Three randomly selected virtual reality play sessions were chosen to score three different virtual reality environments within each session yielding a total of nine trials (environments) for each participant. The types of virtual environments varied across participants. Overall, the different virtual reality play environments produced varying levels of playfulness according to the TOP's four different subscale scores. Motivation ranged from 1.50 to 2.25, internal control ranged from 1.00 to 1.88, suspension of reality ranged from 0 to 0.26, and framing ranged from 1.33 to 1.78. The three environments producing the highest playfulness ratings were called Paint, Trip and Island Sounds. These environments allowed creativity, persistence with the task, pleasure, and a certain degree of control. Two environments did not appear to foster playfulness. A possible reason was that these environments were too unpredictable and frustrating for participants. These results will be useful for creating new virtual reality software applications that will encourage playfulness in children with disabilities\",\n",
       " \"Obesity is a serious disease that can affect both physical and psychological well-being. Due to weight stigmatization, many affected individuals suffer from body image disturbances whereby they perceive their body in a distorted way, evaluate it negatively, or neglect it. Beyond established interventions such as mirror exposure, recent advancements aim to complement body image treatments by the embodiment of visually altered virtual bodies in virtual reality (VR). We present a high-fidelity prototype of an advanced VR system that allows users to embody a rapidly generated personalized, photorealistic avatar and to realistically modulate its body weight in real-time within a carefully designed virtual environment. In a formative multi-method approach, a total of 12 participants rated the general user experience (UX) of our system during body scan and VR experience using semi-structured qualitative interviews and multiple quantitative UX measures. Using body weight modification tasks, we further compared three different interaction methods for real-time body weight modification and measured our system's impact on the body image relevant measures body awareness and body weight perception. From the feedback received, demonstrating an already solid UX of our overall system and providing constructive input for further improvement, we derived a set of design guidelines to guide future development and evaluation processes of systems supporting body image interventions.\",\n",
       " 'SEARIS provides a forum for researchers and practitioners working on the design, development, and support of realtime interactive systems (RIS). These systems span from Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR) environments to novel Human-Computer Interaction systems (such as multimodal or multitouch architectures) and entertainment applications in general. Their common principle is a strong user centric orientation which requires real-time processing of simulation aspects as well as input/output events according to perceptual constraints. Therefore, we encourage researchers and developers of real-time human computer interaction systems of all flavors to share their experiences and learn from each other during this workshop.',\n",
       " 'This paper discusses how a projected virtual reality system that uses video gesture recognition technology can influence the person-environment process. The psychosocial concepts of embodiment, environmental centralization, and environmental personalization are illustrated with examples of virtual reality applications with children with disabilities. Through these, the advantages of using virtual reality to influence the person-environment relationship are discussed. Disadvantages of implementing this virtual reality approach are also presented as well as recommendations for future work in this area.',\n",
       " 'This paper presents an immersive Virtual Reality (VR) therapy system for gait rehabilitation after neurological impairments, e.g., caused by accidents or strokes: The system targets increase of patients\\\\u0027 motivation to perform the repeated exercise by providing stimulating virtual exercise environments with the final goal to increase therapy efficiency and effectiveness. Instead of simply working out on immobile stationary devices, the system allows them to walk through and explore a stimulating virtual world. Patients are immersed in the virtual environments using a Head-Mounted Display (HMD). Walking patterns are captured by motion sensors attached to the patients\\\\u0027 feet to synchronize locomotion speed between the real and the virtual world. A user-centered design process evaluated usability, user experience, and feasibility to confirm the overall goals of the system before any sensitive clinical trials with impaired patients can start. Overall, the results demonstrated an encouraging user experience and acceptance while it did not induce any unwanted side-effects, e.g., nausea or cyber-sickness.',\n",
       " 'SEARIS provides a forum for researchers and practitioners working on the design, development, and support of realtime interactive systems (RIS). These systems span from Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR) environments to novel Human-Computer Interaction systems (such as multimodal or multitouch architectures) and entertainment applications in general. Their common principle is a strong user centric orientation which requires real-time processing of simulation aspects as well as input/output events according to perceptual constraints. Therefore, we encourage researchers and developers of real-time human computer interaction systems of all flavors to share their experiences and learn from each other during this workshop.',\n",
       " 'SEARIS provides a forum for researchers and practitioners working on the design, development, and support of realtime interactive systems (RIS). These systems span from Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR) environments to novel Human-Computer Interaction systems (such as multimodal or multitouch architectures) and entertainment applications in general. Their common principle is a strong user centric orientation which requires real-time processing of simulation aspects as well as input/output events according to perceptual constraints. Therefore, we encourage researchers and developers of real-time human computer interaction systems of all flavors to share their experiences and learn from each other during this workshop.',\n",
       " 'In recent years, the applications and accessibility of Virtual Reality (VR) for the healthcare sector have continued to grow. However, so far, most VR applications are only relevant in research settings. Information about what healthcare professionals would need to independently integrate VR applications into their daily working routines is missing. The actual needs and concerns of the people who work in the healthcare sector are often disregarded in the development of VR applications, even though they are the ones who are supposed to use them in practice. By means of this study, we systematically involve health professionals in the development process of VR applications. In particular, we conducted an online survey with 102 healthcare professionals based on a video prototype which demonstrates a software platform that allows them to create and utilise VR experiences on their own. For this study, we adapted and extended the Technology Acceptance Model (TAM). The survey focused on the perceived usefulness and the ease of use of such a platform, as well as the attitude and ethical concerns the users might have. The results show a generally positive attitude toward such a software platform. The users can imagine various use cases in different health domains. However, the perceived usefulness is tied to the actual ease of use of the platform and sufficient support for learning and working with the platform. In the discussion, we explain how these results can be generalized to facilitate the integration of VR in healthcare practice.',\n",
       " 'In recent years, the applications and accessibility of Virtual Reality (VR) for the healthcare sector have continued to grow. However, so far, most VR applications are only relevant in research settings. Information about what healthcare professionals would need to independently integrate VR applications into their daily working routines is missing. The actual needs and concerns of the people who work in the healthcare sector are often disregarded in the development of VR applications, even though they are the ones who are supposed to use them in practice. By means of this study, we systematically involve health professionals in the development process of VR applications. In particular, we conducted an online survey with 102 healthcare professionals based on a video prototype which demonstrates a software platform that allows them to create and utilise VR experiences on their own. For this study, we adapted and extended the Technology Acceptance Model (TAM). The survey focused on the perceived usefulness and the ease of use of such a platform, as well as the attitude and ethical concerns the users might have. The results show a generally positive attitude toward such a software platform. The users can imagine various use cases in different health domains. However, the perceived usefulness is tied to the actual ease of use of the platform and sufficient support for learning and working with the platform. In the discussion, we explain how these results can be generalized to facilitate the integration of VR in healthcare practice.',\n",
       " 'Virtual Reality (VR) is gaining in popularity and its added value for learning is being recognized. However, its richness in representation and manipulation possibilities may also become one of its weaknesses, as some learners may be overwhelmed and be easily lost in a virtual world. Therefore, being able to dynamically adapt the virtual world to the personal preferences, prior knowledge, skills and competences, learning goals and the personal or social context in which the learning takes place becomes important. In this paper, we describe how an adaptive Web-based learning environment can be extended to support VR in the form of adaptive virtual worlds.',\n",
       " \"The goal of this work in progress project is to provide an improvement to the currently used procedure of physician-patient interaction during the awake craniotomy using Virtual Reality (VR) technology. The proposed procedure will evaluate the patient's response to the visual and auditory stimuli, provided via a VR device. We will highlight the functionality of the setup, its advantages and shortcomings, and discuss possible risks of the used technology. Our approach provides a complete virtual environment, including situations and combinations of stimuli that can test complex reactions of the patient, and, on the other hand, is completely under control of the neuropsychologist. We conclude by showing that the benefits of the approach significantly outweigh the downsides, so that the presented technology is not only a new opportunity, but also the future.\",\n",
       " \"Virtual reality is a human-designed system with the help of computer and electronic devices such as cameras and sensors to interact with 3-D environments. Many challenges that educators face in online teaching can be addressed by using VR. VR can provide learners with an immersed environment where otherwise students cannot access. VR is an exciting way for turning ordinary classrooms into places of wonder, inquiry, and adventure. VR increases student's engagement and interest in learning. It makes many hard topics easy by 360-degree realistic view. This paper provides an introduction to virtual reality.\",\n",
       " 'Technological advances in eye tracking methodology have made it possible to unobtrusively measure consumer visual attention during the shopping process. Mobile eye tracking in field settings however has several limitations, including a highly cumbersome data coding process. In addition, field settings allow only limited control of important interfering variables. The present paper argues that virtual reality can provide an alternative setting that combines the benefits of mobile eye tracking with the flexibility and control provided by lab experiments. The paper first reviews key advantages of different eye tracking technologies as available for desktop, natural and virtual environments. It then explains how combining virtual reality settings with eye tracking provides a unique opportunity for shopper research in particular regarding the use of augmented reality to provide shopper assistance.',\n",
       " \"Virtual reality applications employing avatar embodiment typically use virtual mirrors to allow users to perceive their digital selves not only from a first-person perspective but also from a holistic third-person view. However, due to distance-related biases such as the distance compression effect or a reduced relative rendering resolution, the self-observation distance (SOD) between the user and the virtual mirror might influence how users perceive their embodied avatar. Our article systematically investigates the effects of a short (1 meter), middle (2.5 meter), and far (4 meter) SOD between user and mirror on the perception of personalized and self-embodied avatars. The avatars were photorealistic reconstructed using state-of-the-art photogrammetric methods. Thirty participants were repeatedly exposed to their real-time animated self-embodied avatars in each of the three SOD conditions. In each condition, the personalized avatars were repeatedly altered in their body weight, and participants were asked to judge the (1) sense of embodiment, (2) body weight perception, and (3) affective appraisal towards their avatar. We found that the different SODs are unlikely to influence any of our measures except for the perceived body weight estimation difficulty. Here, the participants judged the difficulty significantly higher for the farthest SOD. We further found that the participants' self-esteem significantly impacted their ability to modify their avatar's body weight to their current body weight and that it positively correlated with the perceived attractiveness of the avatar. Additionally, the participants' concerns about their body shape affected how eerie they perceived their avatars. Both measures influenced the perceived body weight estimation difficulty. For practical application, we conclude that the virtual mirror in embodiment scenarios can be freely placed and varied at a distance of one to four meters from the user without expecting major effects on the perception of the avatar.\",\n",
       " \"Several experiments have provided evidence that ego-centric distances are perceived as compressed in immersive virtual environments relative to the real world. The principal factors responsible for this phenomenon have remained largely unknown. However, recent experiments suggest that when the virtual environment (VE) is an exact replica of a user's real physical surroundings, the person's distance perception improves. Furthermore, it has been shown that when users start their virtual reality (VR) experience in such a virtual replica and then gradually transition to a different VE, their sense of presence in the actual virtual world increases significantly. In this case the virtual replica serves as a transitional environment between the real and virtual world. In this paper we examine whether a person's distance estimation skills can be transferred from a transitional environment to a different VE. We have conducted blind walking experiments to analyze if starting the VR experience in a transitional environment can improve a person's ability to estimate distances in an immersive VR system. We found that users significantly improve their distance estimation skills when they enter the virtual world via a transitional environment.\",\n",
       " 'Research in virtual reality (VR) is a relatively young field, which has shown considerable growth in recent years, as the development of new interactive technologies has inevitably impacted the more traditional sciences and arts. This is more evident in the case of novel interactive technologies that fascinate the broad public, as has always been the case with virtual reality. The increasing development of VR technologies has matured enough to expand research from the military and scientific visualization realm into more multidisciplinary ar-eas, such as education, art and entertainment. Consequently, virtual reality interfaces interaction techniques and devices have improved greatly in order to provide more natural and obvious modes of interaction and motivational elements. In spite of various concerns and objections regarding the appropriateness and educational efficacy of virtual reality, there remain compelling reasons for believing that virtual environments warrant serious investigation and can provide strong tools for learning. This paper analyses the direction taken regarding the development of user friendly interfaces and natural modes of interaction for users of varied technical competencies in virtual environments.',\n",
       " 'Virtual reality is increasingly used for tasks such as work and education. Thus, rendering scenarios that do not interfere with such goals and deplete user experience is becoming progressively more relevant. We present a physiologically-adaptive system that optimizes the virtual environment based on physiological arousal, i.e., electrodermal activity. We investigated the usability of the adaptive system in a simulated social virtual reality scenario. Participants completed an n-back task (primary) and a visual detection (secondary) task. Here, we adapted the visual complexity of the secondary task in the form of the number of not-playable characters of the secondary task to accomplish the primary task. We show that an adaptive virtual reality can improve users’ comfort by adapting to physiological arousal the task complexity. Our findings suggest that physiologically-adaptive virtual reality systems can improve users’ experience in a wide range of scenarios.',\n",
       " 'Touch-sensitive screens enable natural interaction without any instrumentation and support tangible feedback on the touch surface. In particular multi-touch interaction has proven its usability for 2D tasks, but the challenges to exploit these technologies in virtual reality (VR) setups have rarely been studied. In this paper we address the challenge to allow users to interact with stereoscopically displayed virtual environments when the input is constrained to a 2D touch surface. During interaction with a large-scale touch display a user changes between three different states: (1) beyond the arm-reach distance from the surface, (2) at arm-reach distance and (3) interaction. We have analyzed the user’s ability to discriminate stereoscopic display parallaxes while she moves through these states, i. e., if objects can be imperceptibly shifted onto the interactive surface and become accessible for natural touch interaction. Our results show that the detection thresholds for such manipulations are related to both user motion and stereoscopic parallax, and that users have problems to discriminate whether they touched an object or not, when tangible feedback is expected.',\n",
       " \"Communication is the most useful tool to impart knowledge, understand ideas, clarify thoughts and expressions, organize plan and manage every single day-to-day activity. Although there are different modes of communication, physical barrier always affects the clarity of the message due to the absence of body language and facial expressions. These barriers are overcome by video calling, which is technically the most advance mode of communication at present. The proposed work concentrates around the concept of video calling in a more natural and seamless way using Augmented Reality (AR). AR can be helpful in giving the users an experience of physical presence in each other's environment. Our work provides an entirely new platform for video calling, wherein the users can enjoy the privilege of their own virtual space to interact with the individual's environment. Moreover, there is no limitation of sharing the same screen space. Any number of participants can be accommodated over a single conference without having to compromise the screen size.\",\n",
       " 'In this paper we introduce new user interface concepts for fish tank virtual reality (VR) systems based on autostereoscopic (AS) display technologies. Such AS displays allow to view stereoscopic content without requiring special glasses. Unfortunately, until now simultaneous monoscopic and stereoscopic display was not possible. Hence prior work on fish tank VR systems focussed either on 2D or 3D interactions. In this paper we introduce so called interscopic interaction concepts providing an improved working experience, which enable great potentials in terms of the interaction between 2D elements, which may be displayed either in monoscopic or stereoscopic, e.g., GUI items, and the 3D virtual environment usually displayed stereoscopically. We present a framework which is based on a software layer between the operating system and its graphical user interface supporting the display of both mono- as well as stereoscopic content in arbitrary regions of an autostereoscopic display. The proposed concepts open up new vistas for the interaction in environments where essential parts of the GUI are displayed monoscopically and other parts are rendered stereoscopically. We address some essential issues of such fish tank VR systems and introduce intuitive interaction concepts which we have realized.',\n",
       " 'Effective data visualization is a key part of the discovery process in the era of “big data”. It is the bridge between the quantitative content of the data and human intuition, and thus an essential component of the scientific path from data into knowledge and understanding. Visualization is also essential in the data mining process, directing the choice of the applicable algorithms, and in helping to identify and remove bad data from the analysis. However, a high complexity or a high dimensionality of modern data sets represents a critical obstacle. How do we visualize interesting structures and patterns that may exist in hyper-dimensional data spaces? A better understanding of how we can perceive and interact with multidimensional information poses some deep questions in the field of cognition technology and human-computer interaction. To this effect, we are exploring the use of immersive virtual reality platforms for scientific data visualization, both as software and inexpensive commodity hardware. These potentially powerful and innovative tools for multi-dimensional data visualization can also provide an easy and natural path to a collaborative data visualization and exploration, where scientists can interact with their data and their colleagues in the same visual space. Immersion provides benefits beyond the traditional “desktop” visualization tools: it leads to a demonstrably better perception of a datascape geometry, more intuitive data understanding, and a better retention of the perceived relationships in the data.',\n",
       " 'Leveraging digital technologies, museums now have the opportunity to embrace innovative approaches such as knowledge graphs, virtual reality, and virtual assistants to enhance the preservation and interactive presentation of cultural information. However, despite these advancements, personalizing the museum experience remains a significant challenge. Thus, this paper aims to investigate the necessary elements for offering personalized access to cultural heritage within a VR exhibition. To accomplish this, a user study was conducted to identify user preferences for tailored content descriptions, track user viewing behavior to gauge their interest in a VR exhibition, and determine preferred methods of information gathering. The study involved 31 participants, and the findings are expected to provide valuable insights for designing effective and engaging VR exhibitions that cater to diverse visitor interests.',\n",
       " 'learning theories evolved with time, beginning with instructivism, constructivism, to social constructivism. These theories no doubt were applied in education and they had their effects on learners. Technology advanced, created a paradigm shift by creating new ways of teaching and learning as found in virtual reality (VR). VR provided creative ways in which students learn, provides opportunity to achieve learning goals by presenting artificial environments. We developed and simulated a virtual reality system on a desktop by deploying Visual Basic.NET, Java and Macromedia Flash. This simulated environment enhanced students’ understanding by providing a degree of reality unattainable in a traditional two-dimensional interface, creating a sensory-rich interactive learning environment.',\n",
       " 'Based on an extensive, international research project on the practical applications of virtual reality, this book demonstrates that there is a growing range of virtual reality applications in an expanding domain of fields -- from power production and civil defense to medicine and banking.',\n",
       " \"Understanding how professional handball goalkeepers acquire skills to combine decision-making and complex motor tasks is a multidisciplinary challenge. In order to improve a goalkeeper's training by allowing insights into their complex perception, learning and action processes, virtual reality (VR) technologies provide a way to standardize experimental sport situations. In this poster we describe a VR-based handball system, which supports the evaluation of perceptual-motor skills of handball goalkeepers during shots. In order to allow reliable analyses it is essential that goalkeepers can move naturally like they would do in a real game situation, which is often inhibited by wires or markers that are usually used in VR systems. To address this challenge, we developed a camera-based goalkeeper analysis system, which allows to detect and measure motions of goalkeepers in real-time.\",\n",
       " \"Embodiment and body perception have become important research topics in the field of virtual reality (VR). VR is considered a particularly promising tool to support research and therapy in regard to distorted body weight perception. However, the influence of embodiment on body weight perception has yet to be clarified. To address this gap, we compared body weight perception of 56 female participants of normal weight using a VR application. They either (a) self-embodied a photorealistic, non-personalized virtual human and performed body movements in front of a virtual mirror or (b) only observed the virtual human as other's avatar (or agent) performing the same movements in front of them. Afterward, participants had to estimate the virtual human's body weight. Additionally, we considered the influence of the participants' body mass index (BMI) on the estimations and captured the participants' feelings of presence and embodiment. Participants estimated the body weight of the virtual human as their embodied self-avatars significantly lower compared to participants rating the virtual human as other's avatar. Furthermore, the estimations of body weight were significantly predicted by the participant's BMI with embodiment, but not without. Our results clearly highlight embodiment as an important factor influencing the perception of virtual humans' body weights in VR.\",\n",
       " \"Embodiment and body perception have become important research topics in the field of virtual reality (VR). VR is considered a particularly promising tool to support research and therapy in regard to distorted body weight perception. However, the influence of embodiment on body weight perception has yet to be clarified. To address this gap, we compared body weight perception of 56 female participants of normal weight using a VR application. They either (a) self-embodied a photorealistic, non-personalized virtual human and performed body movements in front of a virtual mirror or (b) only observed the virtual human as other's avatar (or agent) performing the same movements in front of them. Afterward, participants had to estimate the virtual human's body weight. Additionally, we considered the influence of the participants' body mass index (BMI) on the estimations and captured the participants' feelings of presence and embodiment. Participants estimated the body weight of the virtual human as their embodied self-avatars significantly lower compared to participants rating the virtual human as other's avatar. Furthermore, the estimations of body weight were significantly predicted by the participant's BMI with embodiment, but not without. Our results clearly highlight embodiment as an important factor influencing the perception of virtual humans' body weights in VR.\",\n",
       " \"Embodiment and body perception have become important research topics in the field of virtual reality (VR). VR is considered a particularly promising tool to support research and therapy in regard to distorted body weight perception. However, the influence of embodiment on body weight perception has yet to be clarified. To address this gap, we compared body weight perception of 56 female participants of normal weight using a VR application. They either (a) self-embodied a photorealistic, non-personalized virtual human and performed body movements in front of a virtual mirror or (b) only observed the virtual human as other's avatar (or agent) performing the same movements in front of them. Afterward, participants had to estimate the virtual human's body weight. Additionally, we considered the influence of the participants' body mass index (BMI) on the estimations and captured the participants' feelings of presence and embodiment. Participants estimated the body weight of the virtual human as their embodied self-avatars significantly lower compared to participants rating the virtual human as other's avatar. Furthermore, the estimations of body weight were significantly predicted by the participant's BMI with embodiment, but not without. Our results clearly highlight embodiment as an important factor influencing the perception of virtual humans' body weights in VR.\",\n",
       " \"Embodiment and body perception have become important research topics in the field of virtual reality (VR). VR is considered a particularly promising tool to support research and therapy in regard to distorted body weight perception. However, the influence of embodiment on body weight perception has yet to be clarified. To address this gap, we compared body weight perception of 56 female participants of normal weight using a VR application. They either (a) self-embodied a photorealistic, non-personalized virtual human and performed body movements in front of a virtual mirror or (b) only observed the virtual human as other's avatar (or agent) performing the same movements in front of them. Afterward, participants had to estimate the virtual human's body weight. Additionally, we considered the influence of the participants' body mass index (BMI) on the estimations and captured the participants' feelings of presence and embodiment. Participants estimated the body weight of the virtual human as their embodied self-avatars significantly lower compared to participants rating the virtual human as other's avatar. Furthermore, the estimations of body weight were significantly predicted by the participant's BMI with embodiment, but not without. Our results clearly highlight embodiment as an important factor influencing the perception of virtual humans' body weights in VR.\",\n",
       " \"Embodiment and body perception have become important research topics in the field of virtual reality (VR). VR is considered a particularly promising tool to support research and therapy in regard to distorted body weight perception. However, the influence of embodiment on body weight perception has yet to be clarified. To address this gap, we compared body weight perception of 56 female participants of normal weight using a VR application. They either (a) self-embodied a photorealistic, non-personalized virtual human and performed body movements in front of a virtual mirror or (b) only observed the virtual human as other's avatar (or agent) performing the same movements in front of them. Afterward, participants had to estimate the virtual human's body weight. Additionally, we considered the influence of the participants' body mass index (BMI) on the estimations and captured the participants' feelings of presence and embodiment. Participants estimated the body weight of the virtual human as their embodied self-avatars significantly lower compared to participants rating the virtual human as other's avatar. Furthermore, the estimations of body weight were significantly predicted by the participant's BMI with embodiment, but not without. Our results clearly highlight embodiment as an important factor influencing the perception of virtual humans' body weights in VR.\",\n",
       " 'We introduce “femtoPro,” an interactive simulator of an ultrafast laser laboratory in virtual reality (VR). Gaussian beam propagation as well as linear and nonlinear optical phenomena are calculated in real time on consumer-grade VR devices.',\n",
       " 'We introduce “femtoPro,” an interactive simulator of an ultrafast laser laboratory in virtual reality (VR). Gaussian beam propagation as well as linear and nonlinear optical phenomena are calculated in real time on consumer-grade VR devices.',\n",
       " 'We introduce “femtoPro,” an interactive simulator of an ultrafast laser laboratory in virtual reality (VR). Gaussian beam propagation as well as linear and nonlinear optical phenomena are calculated in real time on consumer-grade VR devices.',\n",
       " 'Technologies for Virtual, Mixed, and Augmented Reality (VR, MR, and AR) allow to artificially augment social interactions and thus to go beyond what is possible in real life. Motivations for the use of social augmentations are manifold, for example, to synthesize behavior when sensory input is missing, to provide additional affordances in shared environments, or to support inclusion and training of individuals with social communication disorders. We review and categorize augmentation approaches and propose a software architecture based on four data layers. Three components further handle the status analysis, the modification, and the blending of behaviors. We present a prototype (injectX) that supports behavior tracking (body motion, eye gaze, and facial expressions from the lower face), status analysis, decision-making, augmentation, and behavior blending in immersive interactions. Along with a critical reflection, we consider further technical and ethical aspects.',\n",
       " 'This paper presents an alternative to existing methods for remotely accessing Virtual Reality (VR) systems. Common solutions are based on specialised software and/or hardware capable of rendering 3D content, which not only restricts accessibility to specific platforms but also increases the barrier for non expert users. Our approach addresses new audiences by making existing Virtual Environments (VEs) ubiquitously accessible. Its appeal is that a large variety of clients, like desktop PCs and handhelds, are ready to connect to VEs out of the box. We achieve this combining established videoconferencing protocol standards with a server based interaction handling. Currently interaction is based on natural speech, typed textual input and visual feedback, but extensions to support natural gestures are possible and planned. This paper presents the conceptual framework enabling videoconferencing with collaborative VEs as well as an example application for a virtual prototyping system.',\n",
       " 'In this paper we describe a model, which gives a virtual environment to a group of people who uses it. The model is integrated with an Immersible Virtual Reality (IVR) design with an Artificial Neural Network (ANN) interface which runs on internet. A user who wants to participate in the virtual environment should have the hybrid IVR and ANN model with internet connection. IVR is the advanced technology used in the model to give an experience to the people to feel a virtual environment as a real one and ANN used to give a shape for the characters in the virtual environment (VE). This model actually gives an illusion to the user that as if they are in the real communication environment.',\n",
       " 'In visual perception, change blindness describes the phenomenon that persons viewing a visual scene may apparently fail to detect significant changes in that scene. These phenomena have been observed in both computer-generated imagery and real-world scenes. Several studies have demonstrated that change blindness effects occur primarily during visual disruptions such as blinks or saccadic eye movements. However, until now the influence of stereoscopic vision on change blindness has not been studied thoroughly in the context of visual perception research. In this paper, we introduce change blindness techniques for stereoscopic virtual reality (VR) systems, providing the ability to substantially modify a virtual scene in a manner that is difficult for observers to perceive. We evaluate techniques for semiimmersive VR systems, i.e., a passive and active stereoscopic projection system as well as an immersive VR system, i.e., a head-mounted display, and compare the results to those of monoscopic viewing conditions. For stereoscopic viewing conditions, we found that change blindness phenomena occur with the same magnitude as in monoscopic viewing conditions. Furthermore, we have evaluated the potential of the presented techniques for allowing abrupt, and yet significant, changes of a stereoscopically displayed virtual reality environment.',\n",
       " 'This book constitutes the refereed proceedings of the 14th International Conference on Virtual Reality and Augmented Reality, EuroVR 2017, held in Laval, France, in December 2017. The 10 full papers and 2 short papers presented were carefully reviewed and selected from 36 submissions. The papers are organized in four topical sections: interaction models and user studies, visual and haptic real-time rendering, perception and cognition, and rehabilitation and safety.',\n",
       " 'Technologies for Virtual, Mixed, and Augmented Reality (VR, MR, and AR) allow to artificially augment social interactions and thus to go beyond what is possible in real life. Motivations for the use of social augmentations are manifold, for example, to synthesize behavior when sensory input is missing, to provide additional affordances in shared environments, or to support inclusion and training of individuals with social communication disorders. We review and categorize augmentation approaches and propose a software architecture based on four data layers. Three components further handle the status analysis, the modification, and the blending of behaviors. We present a prototype (injectX) that supports behavior tracking (body motion, eye gaze, and facial expressions from the lower face), status analysis, decision-making, augmentation, and behavior blending in immersive interactions. Along with a critical reflection, we consider further technical and ethical aspects.',\n",
       " 'Interacting with the 3D content present in games and virtual environments generally involves some form of 3D interaction. As such, the design and development of 3D spatial interaction devices are important in creating more realistic and immersive user experiences in 3D virtual environment applications through natural and intuitive human expression. In general, current commercially available 3D input devices for virtual reality applications like data gloves, multiple DOF sensors and trackers, etc. typically come with a heavy price tag. The objective of this research is to investigate an approach to setting up an inexpensive 6-DOF optical tracking system using Wii Remotes, which is adequate for 3D interaction in an interactive Head-Mounted Display (HMD) virtual reality system. For the purpose of HMD virtual reality, a user should ideally be able to use a 3D interaction device in a space surrounding the user. This cannot be achieved when using this game controller in the conventional manner. Also, normal usage of the controller only allows for relative positioning and cannot reliably track 6-DOF. This paper outlines a method of using Wii Remotes for 3D spatial interaction in an area surrounding the user. This paper also presents experimental results conducted in order to benchmark the accuracy of the system, by comparing the system’s position and orientation estimates with the readings obtained from a commercial 6-DOF magnetic tracker.',\n",
       " 'Technologies for Virtual, Mixed, and Augmented Reality (VR, MR, and AR) allow to artificially augment social interactions and thus to go beyond what is possible in real life. Motivations for the use of social augmentations are manifold, for example, to synthesize behavior when sensory input is missing, to provide additional affordances in shared environments, or to support inclusion and training of individuals with social communication disorders. We review and categorize augmentation approaches and propose a software architecture based on four data layers. Three components further handle the status analysis, the modification, and the blending of behaviors. We present a prototype (injectX) that supports behavior tracking (body motion, eye gaze, and facial expressions from the lower face), status analysis, decision-making, augmentation, and behavior blending in immersive interactions. Along with a critical reflection, we consider further technical and ethical aspects.',\n",
       " 'Technologies for Virtual, Mixed, and Augmented Reality (VR, MR, and AR) allow to artificially augment social interactions and thus to go beyond what is possible in real life. Motivations for the use of social augmentations are manifold, for example, to synthesize behavior when sensory input is missing, to provide additional affordances in shared environments, or to support inclusion and training of individuals with social communication disorders. We review and categorize augmentation approaches and propose a software architecture based on four data layers. Three components further handle the status analysis, the modification, and the blending of behaviors. We present a prototype (injectX) that supports behavior tracking (body motion, eye gaze, and facial expressions from the lower face), status analysis, decision-making, augmentation, and behavior blending in immersive interactions. Along with a critical reflection, we consider further technical and ethical aspects.',\n",
       " 'Welcome to the first Workshop on Software Engineering and Architectures for Realtime Interactive Systems SEARIS. We are delighted to be part of the program of IEEE VR 2008, in Reno, Nevada. These proceedings contain the 15 accepted contributions, which we believe are thought provoking and representative of the current state of the art in designing Virtual and Augmented Reality systems. We are expecting SEARIS to become the premier vent to publish and discuss these systemsâ€\\x90related issues, as there is currently no other veenue for these topics. In previous years, several researchers have organized related workshops and loosely arranged themselves as an unofficial interest group during past events. Their goal was to create a forum where researchers from all directions in the broad field of Virtual and Augmented Reality can contribute and debate their respective technical approaches. This workâ€\\x90 mshop provides a faceâ€\\x90toâ€\\x90face opportunity to further support this emerging project. Several approaches have been developed and utilized in the field of Realtime Interactive Systems RIS. Virtual, Augmented, Virtualized, in general Mixed Realities, as well as realâ€\\x90 time simulation and computer games led to manifold inspiring solutions for RIS developments in research and production. However, it is an ongoing challenge to identify and sepaâ€\\x90 rate both novel results and well known solutions in any new system. The goal of this workshop is to analyze and structure the current stateâ€\\x90ofâ€\\x90theâ€\\x90art in RIS software engineering and architectures. We want to identify common as well as novel paradigms, concepts, methods, and techniques that support technical developments required in this field. A unified presentation of systems will allow us to support research and development in a more efficient way, and will provide a valuable source of information for future developments. This workshop is ur first integrated attempt to address the complex issue of RIS development and to sumâ€\\x90 omarize the work our community is doing. Arranging the contributions into sections of similar key aspects was a difficult process. Many contributions could be grouped according to several aspects. In fact, it is one of the workshop\\\\u0027s goals to identify such key aspects and many authors are shedding light onto several ey issues. We apologize for any ambiguity here. In the end, we needed a good structure and ence we gr kh ouped the papers into 4 main sections: * Systems. Six development systems are presented InTml, Lightning, FlowVR, OpenMASK VISTA, and MORGAN * Abstraction Is â€\\x90 sues. Two papers address the issues of reusable VE platforms and alterna tives to scene graphs. * Special Issues. In this section we collect papers related to the implementation of RIS in particular platforms, such as mobile systems, multiâ€\\x90rate systems, and Mixed Reality. * Semantic and Dynamic Modeling. The four papers in this section show models for expliâ€\\x90 citly describing semantic (i.e. ä chair is on the floor\") and dynamic information. * Semantic and Dynamic Modeling. The four papers in this section show models for expliâ€\\x90 citly describing semantic i.e. ä chair is on the floor\" and dynamic information.',\n",
       " 'This article investigates the effects of different XR displays on the perception and plausibility of personalized virtual humans. We compared immersive virtual reality (VR), video see-through augmented reality (VST AR), and optical see-through AR (OST AR). The personalized virtual alter egos were generated by state-of-the-art photogrammetry methods. 42 participants were repeatedly exposed to animated versions of their 3D-reconstructed virtual alter egos in each of the three XR display conditions. The reconstructed virtual alter egos were additionally modified in body weight for each repetition. We show that the display types lead to different degrees of incongruence between the renderings of the virtual humans and the presentation of the respective environmental backgrounds, leading to significant effects of perceived mismatches as part of a plausibility measurement. The device-related effects were further partly confirmed by subjective misestimations of the modified body weight and the measured spatial presence. Here, the exceedingly incongruent OST AR condition leads to the significantly highest weight misestimations as well as to the lowest perceived spatial presence. However, similar effects could not be confirmed for the affective appraisal (i.e., humanness, eeriness, or attractiveness) of the virtual humans, giving rise to the assumption that these factors might be unrelated to each other.',\n",
       " 'We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible. We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to',\n",
       " 'This paper presents an image-based rendering approach to accelerate rendering time of virtual scenes containing a large number of complex high poly count objects. Our approach replaces complex objects by impostors, light-weight image-based representations leveraging geometry and shading related processing costs. In contrast to their classical implementation, our impostors are specifically designed to work in Virtual-, Augmented- and Mixed Reality scenarios (XR for short), as they support stereoscopic rendering to provide correct depth perception. Motion parallax of typical head movements is compensated by using a ray marched parallax correction step. Our approach provides a dynamic run-time recreation of impostors as necessary for larger changes in view position. The dynamic run-time recreation is decoupled from the actual rendering process. Hence, its associated processing cost is therefore distributed over multiple frames. This avoids any unwanted frame drops or latency spikes even for impostors of objects with complex geometry and many polygons. In addition to the significant performance benefit, our impostors compare favorably against the original mesh representation, as geometric and textural temporal aliasing artifacts are heavily suppressed.',\n",
       " 'This paper presents an image-based rendering approach to accelerate rendering time of virtual scenes containing a large number of complex high poly count objects. Our approach replaces complex objects by impostors, light-weight image-based representations leveraging geometry and shading related processing costs. In contrast to their classical implementation, our impostors are specifically designed to work in Virtual-, Augmented- and Mixed Reality scenarios (XR for short), as they support stereoscopic rendering to provide correct depth perception. Motion parallax of typical head movements is compensated by using a ray marched parallax correction step. Our approach provides a dynamic run-time recreation of impostors as necessary for larger changes in view position. The dynamic run-time recreation is decoupled from the actual rendering process. Hence, its associated processing cost is therefore distributed over multiple frames. This avoids any unwanted frame drops or latency spikes even for impostors of objects with complex geometry and many polygons. In addition to the significant performance benefit, our impostors compare favorably against the original mesh representation, as geometric and textural temporal aliasing artifacts are heavily suppressed.',\n",
       " 'Modern computer-aided design (CAD) systems and software tools have played a significant role in improving the efficiency of the overall product design process, ensuring geometric accuracy and the exchange of product model data.However, the impact of these technologies is largely restricted to the detailed modeling and engineering analysis that occur during the embodiment design phase. Conceptual design has not benefited from these sophisticated and highly precise software tools to the same degree because the creative activities associated with developing and communicating potential solutions with minimal details is far less formulaic in its implementation. At the early stages of product design the specifications and constraints have not been fully established. The industrial designers and engineers need the freedom to change and modify the product configuration and mechanical behavior to investigate a wide range of alternative solutions. Any CAD system that seeks to support and enhance conceptual design must, therefore, enable natural and haptic modes of human-computer interaction. Recent advancements in high- speed, multi-core computer hardware and virtual reality (VR) technology provide opportunities to link the more fluid processes of creative conceptual design with the rigidly defined tasks of product detailing and engineering analysis. This paper discusses the role that virtual reality can play for concept design module.',\n",
       " 'This paper describes the methodological aspects of the application of various established image-based graphics techniques in virtual reality applications, in order to visually enrich and extend the common capabilities of virtual environment visualization platforms. The paper describes these techniques and goes to the extent of explaining various practical implementation issues. Examples and application case studies are provided to demonstrate the enhancements.',\n",
       " 'This work in progress report demonstrates a novel approach for be- havioral augmentations in Virtual Reality (VR). Using a large scale tracking system, groups of five users explored a virtual museum. We investigated how augmenting social interactions impacts this experience, by designing behavioral transformations for behavio- ral phenomena in social interactions. Preliminary data indicate a reduction of perceived isolation, and a more thought-provoking ex- perience with active behavioral augmentation',\n",
       " 'This work in progress report demonstrates a novel approach for be- havioral augmentations in Virtual Reality (VR). Using a large scale tracking system, groups of five users explored a virtual museum. We investigated how augmenting social interactions impacts this experience, by designing behavioral transformations for behavio- ral phenomena in social interactions. Preliminary data indicate a reduction of perceived isolation, and a more thought-provoking ex- perience with active behavioral augmentation',\n",
       " 'The drag-and-drop metaphor is one of the most common direct interaction metaphors used in desktop-based environments. This direct interaction paradigm enables an intuitive method to apply actions by associating iconic representation of objects to each other. Since dragging of these iconic representations is the most time-consuming subtask of the drag-and-drop metaphor, many extensions of this approach have been proposed to enhance this process. However, a transfer of these concepts to virtual reality (VR) systems has not been realized. In this paper we propose the grab-and-throw metaphor which is a VR-based analogon to the drag-and-drop metaphor. The proposed concepts enable users to select a virtual object by grabbing it and to throw the object within a virtual environment (VE) in the direction of another object. As soon as the object hits another object, an associated action is performed. The trajectory of the thrown object is based on physical motions adapted from the real-world. In order to ease hitting a desired target object, snapping strategies are used such that the aimed object attracts the thrown object. 5A We give a technical description of the grab-and-throw metaphor and discuss example application scenarios which benefit from the usage of the described metaphor.',\n",
       " \"This study aims to determine whether there are improved performances in cadaver temporal bone dissection after training using a VR simulator as a teaching aid compared with traditional training methodsRandomized control trial.Twenty participants with minimal temporal bone experience were recruited for this randomized control trial. After receiving the same didactic teaching they were randomized into two groups. The traditional group were to receive addition teaching via traditional teaching methods such as small group tutorials, videos, and models. The VR group received supervised teaching on the VR simulator. At the end of their teaching they were asked to perform a cadaveric temporal bone dissection and had their performance videoed and assessed by blinded assessors. The assessors judged the videos on four domains of assessments looking at the end product, injury size, overall performance, and technique. These assessments were based on the Welling's scale and OSATS.The VR group performed significantly better in the end product of the dissection (VR 80% vs. traditional 45%, P-value <.001) and caused smaller injuries to anatomic structures (VR 19% vs. traditional 36%, P-value = .01). They also did better in the overall performance score (VR 55% vs. traditional 35%, P-value = .04) There were no differences in the technique score. There was a fair to moderate degree of interrater reliability between the assessors (kappa = 0.33-0.47; Intraclass correlation coefficient = 0.34-0.76).Supervised teaching using a VR simulator seems to improve cadaveric temporal bone dissection performance compared with traditional teaching methods.\",\n",
       " 'OBJECTIVE: The authors evaluate an Internet virtual reality technology as an education tool about the hallucinations of psychosis. METHOD: This is a pilot project using Second Life, an Internet-based virtual reality system, in which a virtual reality environment was constructed to simulate the auditory and visual hallucinations of two patients with schizophrenia. Eight hundred sixty-three self-referred users took a self-guided tour. RESULTS: Five hundred seventy-nine (69%) of the users who toured the environment completed a survey. Of the survey responders, 440 (76%) thought the environment improved their understanding of auditory hallucinations, 69% thought it improved their understanding of visual hallucinations, and 82% said they would recommend the environment to a friend. CONCLUSIONS: Computer simulations of the perceptual phenomena of psychiatric illness are feasible with existing personal computer technology. Integration of the evaluation survey into the environment itself was possible. The use of Internet-connected graphics environments holds promise for public education about mental illness.',\n",
       " 'Numerous research studies and controlled trials have unveiled the potential of serious games in various health-related areas 1. Their range of application can be even further extended by the use of virtual reality (VR) technology, which allows the realistic representation of interactive contents. Virtual Reality Exposure Therapy (VRET) is a very promising novel use case for the development of serious games. Held in a virtual environment (VE) adaptive to the needs of the patient, this form of therapy can outperform traditional realworld measures 2, 3. One of its major success factors is the engagement of the patient, which can be increased by an immersive gaming experience. We show a demonstrator of VRET application for a fire-related post-traumatic stress disorder (PTSD). In this demonstrator, features to support actively guided VR experiences are improved on, focusing on the interactive adaptivity of the VE.',\n",
       " 'Numerous research studies and controlled trials have unveiled the potential of serious games in various health-related areas 1. Their range of application can be even further extended by the use of virtual reality (VR) technology, which allows the realistic representation of interactive contents. Virtual Reality Exposure Therapy (VRET) is a very promising novel use case for the development of serious games. Held in a virtual environment (VE) adaptive to the needs of the patient, this form of therapy can outperform traditional realworld measures 2, 3. One of its major success factors is the engagement of the patient, which can be increased by an immersive gaming experience. We show a demonstrator of VRET application for a fire-related post-traumatic stress disorder (PTSD). In this demonstrator, features to support actively guided VR experiences are improved on, focusing on the interactive adaptivity of the VE.',\n",
       " 'Numerous research studies and controlled trials have unveiled the potential of serious games in various health-related areas 1. Their range of application can be even further extended by the use of virtual reality (VR) technology, which allows the realistic representation of interactive contents. Virtual Reality Exposure Therapy (VRET) is a very promising novel use case for the development of serious games. Held in a virtual environment (VE) adaptive to the needs of the patient, this form of therapy can outperform traditional realworld measures 2, 3. One of its major success factors is the engagement of the patient, which can be increased by an immersive gaming experience. We show a demonstrator of VRET application for a fire-related post-traumatic stress disorder (PTSD). In this demonstrator, features to support actively guided VR experiences are improved on, focusing on the interactive adaptivity of the VE.',\n",
       " 'Numerous Studies and new applications like Google Expeditions or Anatomy 4D point to a great potential of Augmented and Virtual Reality for its use for educational purposes. However, related research concerning technology integration in the classroom has shown that a valuable medium alone does not automatically lead to its successful use. It is therefore the aim of the paper to present an approach by which competencies of pre-service teachers for a successful and appropriate integration of Augmented and Virtual Reality-applications in the classroom are fostered - the own design of Augmented and Virtual Reality applications. We will begin with a short discussion of prominent findings and related work in regard to teaching and learning with and about Augmented and Virtual Reality in the first part before introducing the main goals and aspects of the presented pedagogical concept and the seminar in the second part of the paper. In the third part, two applications created in the seminar and qualitative data from an explorative study based on focus group interviews and participant observation will be presented. We close with a discussion of our findings and an outlook to future work.',\n",
       " 'We present Redgraph, the first generic virtual reality visualization program for Semantic Web data. Redgraph is capable of handling large data-sets, as we demonstrate on social network data from the U.S. Patent Trade Office. We develop a Semantic Web vocabulary of virtual reality terms compatible with GraphXML to map graph visualization into the Semantic Web itself. Our approach to visualizing Semantic Web data takes advantage of user-interaction in an immersive environment to bypass a number of difficult issues in 3-dimensional graph visualization layout by relying on users themselves to interactively extrude the nodes and links of a 2-dimensional graph into the third dimension. When users touch nodes in the virtual reality environment, they retrieve data formatted according to the data’s schema or ontology. We applied Redgraph to social network data constructed from patents, inventors, and institutions from the United States Patent and Trademark Office in order to explore networks of innovation in computing. Using this data-set, results of a user study comparing extrusion (3-D) vs. no-extrusion (2-D) are presented. The study showed the use of a 3-D interface by subjects led to significant improvement on answering of fine-grained questions about the data-set, but no significant difference was found for broad questions about the overall structure of the data. Furthermore, inference can be used to improve the visualization, as demonstrated with a data-set of biotechnology patents and researchers.',\n",
       " 'This study examined how desktop virtual reality (VR) enhances learning and not merely does desktop VR influence learning. Various relevant constructs and their measurement factors were identified to examine how desktop VR enhances learning and the fit of the hypothesized model was analyzed using structural equation modeling. The results supported the indirect effect of VR features to the learning outcomes, which was mediated by the interaction experience and the learning experience. Learning experience which was individually measured by the psychological factors, that is, presence, motivation, cognitive benefits, control and active learning, and reflective thinking took central stage in affecting the learning outcomes in the desktop VR-based learning environment. The moderating effect of student characteristics such as spatial ability and learning style was also examined. The results show instructional designers and VR software developers how to improve the learning effectiveness and further strengthen their desktop VR-based learning implementation. Through this research, an initial theoretical model of the determinants of learning effectiveness in a desktop VR-based learning environment is contributed.',\n",
       " 'This paper presents the virtual reality systems, interaction devices and software used at the Foundation of the Hellenic World (FHW). The applications that FHW has produced, associated with the Olympic Games in ancient Greece, are then detailed. The separate virtual reality shows are presented in terms of interactivity and educational value. Technical aspects of the productions are explained, with an emphasis on surround screen projection environments. These techniques were mostly utilized in the recent production regarding the ancient Olympic Games, where much effort has been made to recreate the feeling of the games and help the user/spectator be an interacting part of the edutainment activity.',\n",
       " 'The illusion of virtual body ownership (VBO) plays a critical role in virtual reality (VR). VR applications provide a broad design space which includes contextual aspects of the virtual surroundings as well as user-driven deliberate choices of their appearance in VR potentially influencing VBO and other well-known effects of VR. We propose a protocol for an experiment to investigate the influence of deliberateness and context-match on VBO and presence. In a first study, we found significant interactions with the environment. Based on our results we derive recommendations for future experiments.',\n",
       " 'Das umfassende Lehrbuch bietet Studierenden eine anschauliche Begleit- und Nachschlaglektüre zu Lehrveranstaltungen, die Virtual Reality / Augmented Reality (VR/AR) thematisieren, z.B. im Bereich Informatik, Medien oder Natur- und Ingenieurwissenschaften. Der modulare Aufbau des Buches gestattet es, sowohl die Reihenfolge der Themen den Anforderungen der jeweiligen Unterrichtseinheit anzupassen als auch eine spezifische Auswahl für ein individuelles Selbststudium zu treffen. Die Leser erhalten die Grundlagen, um selbst VR/AR-Systeme zu realisieren oder zu erweitern, User Interfaces und Anwendungen mit Methoden der VR/AR zu verbessern sowie ein vertieftes Verständnis für die Nutzung von VR/AR zu entwickeln. Neben einem theoretischen Fundament vermittelt das Lehrbuch praxisnahe Inhalte. So erhalten auch potenzielle Anwender in Forschung und Industrie einen wertvollen und hinreichend tiefen Einblick in die faszinierenden Welten von VR/AR sowie ihre Möglichkeiten und Grenzen.',\n",
       " 'Genetic programming techniques have been applied to a variety of different problems. In this paper, the authors discuss the use of these techniques in a virtual environment. The use of genetic programming allows the authors a quick method of searching shape and sound spaces. The basic design of the system, problems encountered, and future plans are all discussed.',\n",
       " 'This article presents a modular approach to incorporate multimodal gesture and speech driven interaction into virtual reality systems. Based on existing techniques for modelling VR-applications, the overall task is separated into different problem categories: from sensor synchronisation to a high-level description of crossmodal temporal and semantic coherences, a set of solution concepts is presented that seamlessly fit into both the static (scenegraph-based) representation and into the dynamic (renderloop and immersion) aspects of a realtime application. The developed framework establishes a connecting layer between raw sensor data and a general functional description of multimodal and scenecontext related evaluation procedures for VR-setups. As an example for the concepts, their implementation in a system for virtual construction is described.',\n",
       " 'This article presents a modular approach to incorporate multimodal gesture and speech driven interaction into virtual reality systems. Based on existing techniques for modelling VR-applications, the overall task is separated into different problem categories: from sensor synchronisation to a high-level description of crossmodal temporal and semantic coherences, a set of solution concepts is presented that seamlessly fit into both the static (scenegraph-based) representation and into the dynamic (renderloop and immersion) aspects of a realtime application. The developed framework establishes a connecting layer between raw sensor data and a general functional description of multimodal and scenecontext related evaluation procedures for VR-setups. As an example for the concepts, their implementation in a system for virtual construction is described.',\n",
       " 'Recent advances in VR technology allow users to consume immersive content within their own living room. Substitutional Reality (SR) promises to enhance this experience by integrating the physical environment into the simulation. We propose a novel approach, called Smart Substitutional Reality (SSR), that extends the passive haptics of SR with the interactive functionality of a smart home environment. SSR is meant to serve as a foundation for serious games. In this paper, we describe the concept of SSR alongside the implementation of a prototype in our smart lab. We designed multiple virtual environments with a varying degree of mismatch compared to the real world, and added selected objects to induce additional haptic and thermal stimuli to increase immersion. In two user studies we investigate their impact on spatial presence and intrinsic motivation which are especially important for serious games. Results suggest that spatial presence is maintained with higher mismatch, and increased by inducing additional stimuli. Intrinsic motivation is increased in both cases.',\n",
       " 'We present an application based on a general peripheral view calculation model which extends previous work on attention-based user interfaces that use eye gaze. An intuitive, two dimensional visibility measure based on the concept of solid angle is developed. We determine to which extent an object of interest, observed by a user, intersects with each region of the underlying visual field model. The results are weighted (thereby considering the visual acuity in each visual field) to determine the total visibility of the object. As a proof of concept, we exemplify the proposed model in a virtual reality application which incorporates a head-mounted display with integrated eye tracking functionality. In this context, we implement several proactive system behaviors including contextual information presentation with an adaptive level of detail and attention guidance; the latter is implemented by detecting visual acuity limitations or attention drifts.',\n",
       " 'Recent advances in VR technology allow users to consume immersive content within their own living room. Substitutional Reality (SR) promises to enhance this experience by integrating the physical environment into the simulation. We propose a novel approach, called Smart Substitutional Reality (SSR), that extends the passive haptics of SR with the interactive functionality of a smart home environment. SSR is meant to serve as a foundation for serious games. In this paper, we describe the concept of SSR alongside the implementation of a prototype in our smart lab. We designed multiple virtual environments with a varying degree of mismatch compared to the real world, and added selected objects to induce additional haptic and thermal stimuli to increase immersion. In two user studies we investigate their impact on spatial presence and intrinsic motivation which are especially important for serious games. Results suggest that spatial presence is maintained with higher mismatch, and increased by inducing additional stimuli. Intrinsic motivation is increased in both cases.',\n",
       " 'Virtual reality based geographic information systems (VRGIS) have been successfully employed for urban planning and architectural design in recent years. Tracking technologies and stereoscopic visualization of three-dimensional structures allow a better insight into complex datasets. Unfortunately, these systems often lack intuitive interaction concepts and therefore reduce VRGIS to advanced visualization environments, since manipulation of the content is not or only rudimentarily possible. In this paper, we present a geographic information system for urban planning tasks in semi-immersive virtual reality (VR) systems. The objective of this approach is to provide professional city planners with an enhanced VR interface, which enables comfortable interaction concepts similar to the interactions of the real-world planning task. To assure the usability and relevance of the developed system, urban planners have cooperated closely in the development process. In this paper both the hard- and software architecture of the entire system as well as VR related interaction metaphors and their evaluation are discussed.',\n",
       " 'In this article, we present a selection of recent studies from our research group that investigated the relationship between time perception and virtual reality (VR). We focus on the influence of avatar embodiment, visual fidelity, motion perception, and body representation. We summarize findings on the impact of these factors on time perception, discuss lessons learned, and implications for future applications. In a waiting room experiment, the passage of time in VR with an avatar was perceived significantly faster than without an avatar. The passage of time in the real waiting room was not perceived as significantly different from the waiting room in VR with or without an avatar. In an interactive scenario, the absence of a virtual avatar resulted in a significantly slower perceived passage of time compared to the partial and full-body avatar conditions. High and medium embodiment conditions are assumed to be more plausible and to less different from a real experience. A virtual tunnel that induced the illusion of self-motion (vection) appeared to contribute to the perceived passage of time and experience of time. This effect was shown to increase with tunnel speed and the number of tunnel segments. A framework was proposed for the use of virtual zeitgebers along three dimensions (speed, density, synchronicity) to systematically control the experience of time. The body itself, as well as external objects, seem to be addressed by this theory of virtual zeitgebers. Finally, the standardization of the methodology and future research considerations are discussed.',\n",
       " 'This study explored the degree of motivation children exhibit during virtual reality (VR) play sessions. The overall volitional scores of children with cerebral palsy in the current study indicate that VR play is a motivating activity and thus has potential as a successful intervention tool.',\n",
       " 'Recent advances in VR technology allow users to consume immersive content within their own living room. Substitutional Reality (SR) promises to enhance this experience by integrating the physical environment into the simulation. We propose a novel approach, called Smart Substitutional Reality (SSR), that extends the passive haptics of SR with the interactive functionality of a smart home environment. SSR is meant to serve as a foundation for serious games. In this paper, we describe the concept of SSR alongside the implementation of a prototype in our smart lab. We designed multiple virtual environments with a varying degree of mismatch compared to the real world, and added selected objects to induce additional haptic and thermal stimuli to increase immersion. In two user studies we investigate their impact on spatial presence and intrinsic motivation which are especially important for serious games. Results suggest that spatial presence is maintained with higher mismatch, and increased by inducing additional stimuli. Intrinsic motivation is increased in both cases.',\n",
       " \"The purpose of this paper was to examine the effects of virtual play intervention on the level of playfulness of children with cerebral palsy. Thirteen children aged 8-13 years comprised the study group. Children attended eight one-hour virtual reality play sessions in which they were immersed and interacted with virtual reality. The Test of Playfulness (TOP) was used as the measure to assess playfulness. Participants were videotaped while they played during 12 different environments over the course of their intervention time. Three randomly selected virtual reality play sessions were chosen to score three different virtual reality environments within each session yielding a total of nine trials (environments) for each participant. The types of virtual environments varied across participants. Overall, the different virtual reality play environments produced varying levels of playfulness according to the TOP's four different subscale scores. Motivation ranged from 1.50 to 2.25, internal control ranged from 1.00 to 1.88, suspension of reality ranged from 0 to 0.26, and framing ranged from 1.33 to 1.78. The three environments producing the highest playfulness ratings were called Paint, Trip and Island Sounds. These environments allowed creativity, persistence with the task, pleasure, and a certain degree of control. Two environments did not appear to foster playfulness. A possible reason was that these environments were too unpredictable and frustrating for participants. These results will be useful for creating new virtual reality software applications that will encourage playfulness in children with disabilities.\",\n",
       " 'This article presents a gesture detection and analysis framework for modelling multimodal interactions. It is particulary designed for its use in Virtual Reality (VR) applications and contains an abstraction layer for different sensor hardware. Using the framework, gestures are described by their characteristic spatio-temporal features which are on the lowest level calculated by simple predefined detector modules or nodes. These nodes can be connected by a data routing mechanism to perform more elaborate evaluation functions, therewith establishing complex detector nets. Typical problems that arise from the time-dependent invalidation of multimodal utterances under immersive conditions lead to the development of pre-evaluation concepts that as well support their integration into scene graph based systems to support traversal-type access. Examples of realized interactions illustrate applications which make use of the described concepts.',\n",
       " 'This article presents a gesture detection and analysis framework for modelling multimodal interactions. It is particulary designed for its use in Virtual Reality (VR) applications and contains an abstraction layer for different sensor hardware. Using the framework, gestures are described by their characteristic spatio-temporal features which are on the lowest level calculated by simple predefined detector modules or nodes. These nodes can be connected by a data routing mechanism to perform more elaborate evaluation functions, therewith establishing complex detector nets. Typical problems that arise from the time-dependent invalidation of multimodal utterances under immersive conditions lead to the development of pre-evaluation concepts that as well support their integration into scene graph based systems to support traversal-type access. Examples of realized interactions illustrate applications which make use of the described concepts.',\n",
       " 'This poster presents a maintainable method to manage lexical information required for multimodal interfaces. It is tailored for the application in real-time interactive systems, specifically for Virtual Reality, and solves three problems commonly encountered in this context: (1) The lexical information is defined on and grounded in a common knowledge representation layer (KRL) based on OWL. The KRL describes application objects and possible system functions in one place and avoids error-prone redundant data management. (2) The KRL is tightly integrated into the simulator platform using a semantically enriched object model that is auto-generated from the KRL and thus fosters high performance access. (3) A well-defined interface provides application wide access to semantic application state information in general and the lexical information in specific, which greatly contributes to decoupling, maintainability, and reusability.',\n",
       " 'This poster presents a maintainable method to manage lexical information required for multimodal interfaces. It is tailored for the application in real-time interactive systems, specifically for Virtual Reality, and solves three problems commonly encountered in this context: (1) The lexical information is defined on and grounded in a common knowledge representation layer (KRL) based on OWL. The KRL describes application objects and possible system functions in one place and avoids error-prone redundant data management. (2) The KRL is tightly integrated into the simulator platform using a semantically enriched object model that is auto-generated from the KRL and thus fosters high performance access. (3) A well-defined interface provides application wide access to semantic application state information in general and the lexical information in specific, which greatly contributes to decoupling, maintainability, and reusability.',\n",
       " 'Recently, virtual reality (VR) becomes more and more popular and provides users an immersive experience with a head-mounted display (HMD). However, in some applications, users have to interact with physical objects while immersed in VR. With a non-see-through HMD, it is difficult to perceive visual information from the real world. Users must recall the spatial layout of the real surroundings and grope around to find the physical objects. After locating the objects, it is still inconvenient to use them without any visual feedback, which would detract the immersive experience.',\n",
       " \"Racial bias, implicit or explicit, is still a widespread phenomenon in our modern society, negatively affecting the way we interact with foreigners. These biases can also lead to the general tendency to avoid encounters with foreigners, which has a critical impact on society as a whole. This paper presents an approach to reduce implicit and explicit racial bias using two Intelligent Virtual Agents (IVAs) in Virtual Reality (VR). Based on previous research from social psychology and the field of enculturated IVAs, a sympathetic East African-German mixed-cultural IVA, and an antipathetic German mono-cultural IVA were implemented to interact with the participants in a virtual pub quiz. Pre- and post-intervention measures of participants' implicit and explicit bias showed a significant decrease in both scores. This work demonstrates the capability of enculturated IVAs to reduce real-world racial biases and sets the base for cultural interventions with IVAs.\",\n",
       " 'SHAPE, “Situating Hybrid Assemblies in Public Environments”, is an EU Future and Emerging Technologies project of the Disappearing Computer initiative, concerned with designing and developing novel technology to enhance interpersonal interaction in public locales: exploratoria, galleries, and museums, for example. This paper outlines a use of hybrid reality technology to enhance users’ social experience and learning about antique artefacts and their related history. We describe early SHAPE technical work where we explore whether there are benefits: educational and social, to visitors of extending virtual archaeology or augmented reality archaeology into the public setting of the museum.',\n",
       " \"Augmented Reality (AR) and Virtual Reality (VR) are two developing technologies with enormous promise to change the way education is provided. The purpose of this research is to investigate the fundamental distinctions between AR and VR in the context of course instruction. Educators can acquire insights into how these technologies might be effectively integrated into the classroom by evaluating their specific traits, affordances, and limits. The research begins by defining AR and VR and then compares their key ideas. AR superimposes digital information on the real-world environment to improve the user's impression of reality, whereas VR immerses users in a simulated environment to create a sensation of presence and immersion. These contrasts lay the groundwork for comprehending their disparate uses in education. The study then goes into the educational benefits and difficulties of AR and VR. AR allows students to interact with digital information in real time, encouraging engagement, collaboration, and contextualized learning. It could bridge the gap between abstract ideas and real-world applications. VR, on the other hand, provides an immersive and regulated environment that allows students to explore complicated scenarios, imitate real-world circumstances, and acquire important skills in a safe and cost-effective manner. The research looks at the technology needs, accessibility concerns, and implementation techniques for AR and VR in educational settings. It emphasizes the significance of pedagogical design, content creation, and teacher training to maximize these technologies' educational impact. This review paper investigates the differences between Augmented Reality (AR) and Virtual Reality (VR) in the context of course education. AR and VR have emerged as significant tools for boosting the learning experience as technology continues to alter education. Understanding the distinct characteristics and benefits of each technology is critical for educators to make educated judgments about how to include them in their teaching practices. This paper examines AR and VR in depth, concentrating on their distinctions, uses, and possible influence in educational contexts.\",\n",
       " \"The design and evaluation of assisting technologies to support behavior change processes have become an essential topic within the field of human-computer interaction research in general and the field of immersive intervention technologies in particular. The mechanisms and success of behavior change techniques and interventions are broadly investigated in the field of psychology. However, it is not always easy to adapt these psychological findings to the context of immersive technologies. The lack of theoretical foundation also leads to a lack of explanation as to why and how immersive interventions support behavior change processes. The Behavioral Framework for immersive Technologies (BehaveFIT) addresses this lack by 1) presenting an intelligible categorization and condensation of psychological barriers and immersive features, by 2) suggesting a mapping that shows why and how immersive technologies can help to overcome barriers and finally by 3) proposing a generic prediction path that enables a structured, theory-based approach to the development and evaluation of immersive interventions. These three steps explain how BehaveFIT can be used, and include guiding questions for each step. Further, two use cases illustrate the usage of BehaveFIT. Thus, the present paper contributes to guidance for immersive intervention design and evaluation, showing that immersive interventions support behavior change processes and explain and predict 'why' and 'how' immersive interventions can bridge the intention-behavior-gap.\",\n",
       " 'Navigation through immersive virtual environments is a key concept for virtual reality as it allows users to explore those environments. Therefore, it is important to understand virtual reality navigation interfaces and their impact on the users’ experience. This paper presents an objective performance evaluation of two types of navigation: natural (real walking and walk-in-place) vs. unnatural (gamepad). Steering Law was the objective performance metric chosen since it captures the relationship between the time to travel a path and the difficulty of that path. In addition to performance, subjective metrics were also considered, namely the feeling of presence, cybersickness and user satisfaction. The experiments consisted of having participants complete a series of paths with different indexes of difficulty and the time that a participant took to walk each path was measured. Overall results show that the navigation through real walking yielded better results when it comes to performance, cybersickness, and user satisfaction than the walk-in-place and gamepad navigation interfaces.',\n",
       " 'This paper provides a multi-disciplinary overview of the research issues and achievements in the field of Big Data and its visualization techniques and tools. The main aim is to summarize challenges in visualization methods for existing Big Data, as well as to offer novel solutions for issues related to the current state of Big Data Visualization. This paper provides a classification of existing data types, analytical methods, visualization techniques and tools, with a particular emphasis placed on surveying the evolution of visualization methodology over the past years. Based on the results, we reveal disadvantages of existing visualization methods. Despite the technological development of the modern world, human involvement (interaction), judgment and logical thinking are necessary while working with Big Data. Therefore, the role of human perceptional limitations involving large amounts of information is evaluated. Based on the results, a non-traditional approach is proposed: we discuss how the capabilities of Augmented Reality and Virtual Reality could be applied to the field of Big Data Visualization. We discuss the promising utility of Mixed Reality technology integration with applications in Big Data Visualization. Placing the most essential data in the central area of the human visual field in Mixed Reality would allow one to obtain the presented information in a short period of time without significant data losses due to human perceptual issues. Furthermore, we discuss the impacts of new technologies, such as Virtual Reality displays and Augmented Reality helmets on the Big Data visualization as well as to the classification of the main challenges of integrating the technology.',\n",
       " 'In this paper we present ideas that can help to close the gap between virtual reality and embedded interaction, which are usually assumed to be the two extremes of a dimension that describes mixed reality interaction. We present our preliminary ideas on surface interaction with stereoscopic data as well as our work on mobile camera-projector units. While the first line of research tries to increase the degree of the interaction experience in virtual worlds, the second uses the physical properties of the real world to enhance the augmented reality experience. The first idea uses the physical world to constraint the interaction in the virtual world. The second idea embeds virtual information into the physical world by respecting its physical properties.',\n",
       " 'n the recent decades, obesity has become one of the major public health issues and is associated with severe other diseases. Although current multidisciplinary therapy approaches already include behavioral therapy techniques, the oftentimes remaining lack of psychotherapeutic support after surgery leads to relapses and renewed weight gain. This paper presents an overview of the project ViTraS-Virtual Reality Therapy by Stimulation of Modulated BodyImage - that addresses these challenges by (i) Developing an integrative model predicting the influential paths of immersive media for an effective behavioral change; (ii) Developing an augmented reality (AR)-mirror system enabling an effective therapy on body self-perception of patients, and (iii)Developing a multi-user virtual reality (VR)-system supply-ing social support from therapists and other patients. The three components of the ViTraS projects are briefly introduced, as well as a first VR-based prototype of the mirror system.',\n",
       " 'n the recent decades, obesity has become one of the major public health issues and is associated with severe other diseases. Although current multidisciplinary therapy approaches already include behavioral therapy techniques, the oftentimes remaining lack of psychotherapeutic support after surgery leads to relapses and renewed weight gain. This paper presents an overview of the project ViTraS-Virtual Reality Therapy by Stimulation of Modulated BodyImage - that addresses these challenges by (i) Developing an integrative model predicting the influential paths of immersive media for an effective behavioral change; (ii) Developing an augmented reality (AR)-mirror system enabling an effective therapy on body self-perception of patients, and (iii)Developing a multi-user virtual reality (VR)-system supply-ing social support from therapists and other patients. The three components of the ViTraS projects are briefly introduced, as well as a first VR-based prototype of the mirror system.',\n",
       " 'Schmerzbehandlung zählt zu den täglichen Routinen klinischer Anästhesisten. Im Rahmen eines wohlüberlegten Einsatzes von Schmerzmedikamenten sind Alternativen zur medikamentösen Schmerztherapie notwendig. Virtual Reality (VR) konnte sich in den letzten Jahren durch immer kostengünstigere und bessere Technologien als realistische Ergänzung etablieren. Möglichkeiten der VR sowie Indikationen und Kontraindikationen werden aufgezeigt.',\n",
       " 'The number of stroke patients in Hong Kong continues to increase and the ages of the stroke patients are getting younger. The benefits of introducing virtual reality (VR) technology to enhance the rehabilitation services of post stroke patients are studied. A VR walking game for patients recovering from strokes was developed. Twenty physiotherapists, occupational therapists, and prosthetic-orthotic therapists were invited to evaluate the benefits of the games through questionnaires and interviews. Eighty percents of the therapists agreed or strongly agreed that the introduction of the VR walking game could help them to care for more post-stroke patients. The first prototype used a head-mounted display (HMD) to present the VR simulation and 12 therapists reported that the quality of HMD would cause potential disorientations in elderly patients. A second prototype with three panel-mounted displays was developed and results of further evaluation confirmed that the potential benefits of VR games in clinical management of patients recovering from strokes. Specific technical recommendations concerning an ideal VR game for rehabilitation are reported.',\n",
       " 'Immersive Virtual Reality (VR) is increasingly being explored as an alternative medium for gambling games to attract players. Typically, gambling games try to impair a player’s decision making, usually for the disadvantage of the players’ financial outcome. An impaired decision making results in the inability to differentiate between advantageous and disadvantageous options. We investigated if and how immersion impacts decision making using a VR-based realization of the Iowa Gambling Task (IGT) to pinpoint potential risks and effects of gambling in VR. During the IGT, subjects are challenged to draw cards from four different decks of which two are advantageous. The selections made serve as a measure of a participant’s decision making during the task. In a novel user study, we compared the effects of immersion on decision making between a low-immersive desktop-3D-based IGT realization and a high immersive VR version. Our results revealed significantly more disadvantageous decisions when playing the immersive VR version. This indicates an impair- ing effect of immersion on simulated real life decision making and provides empirical evidence for a high risk potential of gambling games targeting immersive VR.',\n",
       " 'Immersive Virtual Reality (VR) is increasingly being explored as an alternative medium for gambling games to attract players. Typically, gambling games try to impair a player’s decision making, usually for the disadvantage of the players’ financial outcome. An impaired decision making results in the inability to differentiate between advantageous and disadvantageous options. We investigated if and how immersion impacts decision making using a VR-based realization of the Iowa Gambling Task (IGT) to pinpoint potential risks and effects of gambling in VR. During the IGT, subjects are challenged to draw cards from four different decks of which two are advantageous. The selections made serve as a measure of a participant’s decision making during the task. In a novel user study, we compared the effects of immersion on decision making between a low-immersive desktop-3D-based IGT realization and a high immersive VR version. Our results revealed significantly more disadvantageous decisions when playing the immersive VR version. This indicates an impairing effect of immersion on simulated real life decision making and provides empirical evidence for a high risk potential of gambling games targeting immersive VR.',\n",
       " 'Immersive Virtual Reality (VR) is increasingly being explored as an alternative medium for gambling games to attract players. Typically, gambling games try to impair a player’s decision making, usually for the disadvantage of the players’ financial outcome. An impaired decision making results in the inability to differentiate between ad- vantageous and disadvantageous options. We investigated if and how immersion impacts decision making using a VR-based realization of the Iowa Gambling Task (IGT) to pinpoint potential risks and ef- fects of gambling in VR. During the IGT, subjects are challenged to draw cards from four different decks of which two are advantageous. The selections made serve as a measure of a participant’s decision making during the task. In a novel user study, we compared the effects of immersion on decision making between a low-immersive desktop-3D-based IGT realization and a high immersive VR version. Our results revealed significantly more disadvantageous decisions when playing the immersive VR version. This indicates an impair- ing effect of immersion on simulated real life decision making and provides empirical evidence for a high risk potential of gambling games targeting immersive VR.',\n",
       " 'Everyone knows about the awareness of the 5 basic senses they are seeing, feeling, smelling, tasting and hearing. These senses have progressed through billions of years. When we try to encounter a new object/article/experience our inborn senses tries to analysis and investigate that Knowledge involvement, experience and the information that is gained which is used to alter our communication with the environment. In this new phase of technologies the most vital information that helps one to make correct evaluation is something that cannot be ostensible and examined by our inborn natural senses where it is possible only with the help of virtual reality and simulation. That info is the facts in the digital form, and it is accessible to every person through sources like cyberspace. The sixth sense technology idea is a power to unite this information from the digital world into the real world.',\n",
       " 'Interaction in conversational interfaces strongly relies on the sys- tem\\\\u0027s capability to interpret the user\\\\u0027s references to objects via de- ictic expressions. Deictic gestures, especially pointing gestures, provide a powerful way of referring to objects and places, e.g., when communicating with an Embodied Conversational Agent in a Virtual Reality Environment. We highlight results drawn from a study on pointing and draw conclusions for the implementation of pointing-based conversational interactions in partly immersive Vir- tual Reality.',\n",
       " \"Interaction in conversational interfaces strongly relies on the sys- tem's capability to interpret the user's references to objects via de- ictic expressions. Deictic gestures, especially pointing gestures, provide a powerful way of referring to objects and places, e.g., when communicating with an Embodied Conversational Agent in a Virtual Reality Environment. We highlight results drawn from a study on pointing and draw conclusions for the implementation of pointing-based conversational interactions in partly immersive Vir- tual Reality.\",\n",
       " 'Of all senses, it is visual perception that is predominantly deluded in Virtual Realities. Yet, the eyes of the observer, despite the fact that they are the fastest perceivable moving body part, have gotten relatively little attention as an interaction modality. A solid integration of gaze, however, provides great opportunities for implicit and explicit human-computer interaction. We present our work on integrating a lightweight head-mounted eye tracking system in a CAVE-like Virtual Reality Set-Up and provide promising data from a user study on the achieved accuracy and latency.',\n",
       " \"Objective To assess the effect of virtual reality training on an actual laparoscopic operation. Design Prospective randomised controlled and blinded trial. Setting Seven gynaecological departments in the Zeeland region of Denmark. Participants 24 first and second year registrars specialising in gynaecology and obstetrics. Interventions Proficiency based virtual reality simulator training in laparoscopic salpingectomy and standard clinical education (controls). Main outcome measure The main outcome measure was technical performance assessed by two independent observers blinded to trainee and training status using a previously validated general and task specific rating scale. The secondary outcome measure was operation time in minutes. Results The simulator trained group (n=11) reached a median total score of 33 points (interquartile range 32-36 points), equivalent to the experience gained after 20-50 laparoscopic procedures, whereas the control group (n=10) reached a median total score of 23 (22-27) points, equivalent to the experience gained from fewer than five procedures (P<0.001). The median total operation time in the simulator trained group was 12 minutes (interquartile range 10-14 minutes) and in the control group was 24 (20-29) minutes (P<0.001). The observers' inter-rater agreement was 0.79. Conclusion Skills in laparoscopic surgery can be increased in a clinically relevant manner using proficiency based virtual reality simulator training. The performance level of novices was increased to that of intermediately experienced laparoscopists and operation time was halved. Simulator training should be considered before trainees carry out laparoscopic procedures. Trial registration ClinicalTrials.gov NCT00311792 . rtmp://highwirepr.fcod.llnwd.net/a1969/o21vr\\\\_trainingsmall/larc577908.f2\\\\_default.gif This video follows Sofie Leisby, a trainee surgeon, through laparoscopic surgeryfrom practising in VR to a real life procedure. 10.1136/bmj.b1802\",\n",
       " \"PURPOSE: This study explored the degree of motivation children exhibit during virtual reality (VR) play sessions. METHOD: Sixteen children with cerebral palsy aged 8 to 12 years participated. They were observed during a variety of VR environments that were video recorded. The Pediatric Volitional Questionnaire (PVQ) was used to measure children's motivation. The PVQ provides insights into children's inner motives as well as how the virtual environment enhances or attenuates children's motives. Nine VR environments were randomly selected to score with the PVQ. RESULTS: Data were analyzed and descriptive statistics were calculated for modes and medians of total volition scores for each VR environment. Different environments produced varying levels of volitional behaviour. The features of environments that produced higher levels of volition included challenge, variability and competition. PRACTICE IMPLICATIONS: The overall volitional scores of children with cerebral palsy in the current study indicate that VR play is a motivating activity and thus has potential as a successful intervention tool.\",\n",
       " 'Virtual Reality (VR) technology offers promising opportunities to improve traditional treadmill-based rehabilitation programs. We present an immersive VR rehabilitation system that includes a head-mounted display and motion sensors. The application is designed to promote the experience of relatedness, autonomy, and competence. The application uses procedural content generation to generate diverse landscapes. We evaluated the effect of the immersive rehabilitation system on motivation and affect. We conducted a repeated measures study with 36 healthy participants to compare the immersive program to a traditional rehabilitation program. Participants reported significant greater enjoyment, felt more competent and experienced higher decision freedom and meaningfulness in the immersive VR gait training compared to the traditional training. They experienced significantly lower physical demand, simulator sickness, and state anxiety, and felt less pressured while still perceiving a higher personal performance. We derive three design implications for future applications in gait rehabilitation: Immersive VR provides a promising augmentation for gait rehabilitation. Gamification features provide a design guideline for content creation in gait rehabilitation. Relatedness and autonomy provide critical content features in gait rehabilitation.',\n",
       " 'Recent improvements in virtual reality (VR) technology promise the opportunity to redesign established game genres, such as real-time strategy (RTS) games. In this work we have a look at a taxonomy of RTS games and apply it to RTS titles for VR. Hereby, we identify possible difficulties such as the need for novel means of navigation in VR. We discuss conceivable solutions and illustrate them by referring to relevant work by others and by means of AStar0ID, an exploratory prototype VR RTS science fiction game. Our main contribution is the systematic inspection and discussion of foundational RTS aspects in the context of VR and, thus, to provide a substantial basis to further rethink and evolve the RTS genre in this new light.',\n",
       " \"The appearance of avatars can potentially alter changes in their users' perception and behavior. Based on this finding, approaches to support the therapy of body perception disturbances in eating or body weight disorders by mixed reality (MR) systems gain in importance. However, the methodological heterogeneity of previous research has made it difficult to assess the suitability of different MR systems for therapeutic use in these areas. The effects of MR system properties and related psychometric factors on body-related perceptions have so far remained unclear. We developed an interactive virtual mirror embodiment application to investigate the differences between an augmented reality see-through head-mounted-display (HMD) and a virtual reality HMD on the before-mentioned factors. Additionally, we considered the influence of the participant's body-mass-index (BMI) and the BMI difference between participants and their avatars on the estimations. The 54 normal-weight female participants significantly underestimated the weight of their photorealistic, generic avatar in both conditions. Body weight estimations were significantly predicted by the participants' BMI and the BMI difference. We also observed partially significant differences in presence and tendencies for differences in virtual body ownership between the systems. Our results offer new insights into the relationships of body weight perception in different MR environments and provide new perspectives for the development of therapeutic applications.\",\n",
       " \"The appearance of avatars can potentially alter changes in their users' perception and behavior. Based on this finding, approaches to support the therapy of body perception disturbances in eating or body weight disorders by mixed reality (MR) systems gain in importance. However, the methodological heterogeneity of previous research has made it difficult to assess the suitability of different MR systems for therapeutic use in these areas. The effects of MR system properties and related psychometric factors on body-related perceptions have so far remained unclear. We developed an interactive virtual mirror embodiment application to investigate the differences between an augmented reality see-through head-mounted-display (HMD) and a virtual reality HMD on the before-mentioned factors. Additionally, we considered the influence of the participant's body-mass-index (BMI) and the BMI difference between participants and their avatars on the estimations. The 54 normal-weight female participants significantly underestimated the weight of their photorealistic, generic avatar in both conditions. Body weight estimations were significantly predicted by the participants' BMI and the BMI difference. We also observed partially significant differences in presence and tendencies for differences in virtual body ownership between the systems. Our results offer new insights into the relationships of body weight perception in different MR environments and provide new perspectives for the development of therapeutic applications.\",\n",
       " \"The Chief Medical Officer's 2008 annual report highlighted the importance of simulation in medical training. Simulator development has focused on increasing authenticity and fidelity. Development has not necessarily been guided by evidence for educational improvement. On reviewing 34 years of literature, Issenberg et al identified ten features of high-fidelity medical simulators that facilitate learning. This study compares cadaveric temporal bone (CTB) simulation with the Voxel-Man TempoSurg (VT) virtual reality simulator in addressing these features.A questionnaire was designed comparing the VT with CTB. Fourteen trainees and six consultants completed the questionnaire after using the simulator.The VT is better at allowing repetitive practice, ease of control of difficulty, and capturing clinical and pathological variation. The VT is as good as CTB in curriculum integration, allowing multiple learning strategies, providing a controlled environment, individualising learning and defining benchmarks. It appears worse with regards to face validity and feedback.Virtual reality simulation and CTB have features that allow effective learning. Some of these are common to both, in some CTB is better and in others virtual reality is better. Virtual reality could be a significant mode of learning supplementary to CTB and experience in the operating theatre.\",\n",
       " \"This article reviews the rationale, current status and future directions for the development and implementation of virtual reality surgical simulators as training tools.The complexity of modern surgical techniques, which utilize advanced technology, presents a dilemma for surgical training. Hands-on patient experience - the traditional apprenticeship method for teaching operations - may not apply because of the learning curve for skill acquisition and patient safety expectation. The paranasal sinuses and temporal bone have intricate anatomy with a significant amount of vital structures either within the surgical field or in close proximity. The current standard of surgical care in these areas involves the use of endoscopes, cameras and microscopes, requiring additional hand-eye coordination, an accurate command of fine motor skills, and a thorough knowledge of the anatomy under magnified vision. A surgeon's disorientation or loss of perspective can lead to complications, often catastrophic and occasionally lethal. These considerations define the ideal environment for surgical simulation; not surprisingly, significant research and validation of simulators in these areas have occurred.Virtual reality simulators are demonstrating validity as training and skills assessment tools. Future prototypes will find application for routine use in teaching, surgical planning and the development of new instruments and computer-assisted devices.\",\n",
       " \"The design and evaluation of assisting technologies to support behavior change processes have become an essential topic within the field of human-computer interaction research in general and the field of immersive intervention technologies in particular. The mechanisms and success of behavior change techniques and interventions are broadly investigated in the field of psychology. However, it is not always easy to adapt these psychological findings to the context of immersive technologies. The lack of theoretical foundation also leads to a lack of explanation as to why and how immersive interventions support behavior change processes. The Behavioral Framework for immersive Technologies (BehaveFIT) addresses this lack by 1) presenting an intelligible categorization and condensation of psychological barriers and immersive features, by 2) suggesting a mapping that shows why and how immersive technologies can help to overcome barriers and finally by 3) proposing a generic prediction path that enables a structured, theory-based approach to the development and evaluation of immersive interventions. These three steps explain how BehaveFIT can be used, and include guiding questions for each step. Further, two use cases illustrate the usage of BehaveFIT. Thus, the present paper contributes to guidance for immersive intervention design and evaluation, showing that immersive interventions support behavior change processes and explain and predict 'why' and 'how' immersive interventions can bridge the intention-behavior-gap.\",\n",
       " 'Mind-body therapies aim to improve health by combining physical and mental exercises. Recent developments tend to incorporate virtual reality (VR) into their design and execution, but there is a lack of research concerning the inclusion of virtual bodies and their effect on body awareness in these designs. In this study, 24 participants performed in-VR body awareness movement tasks in front of a virtual mirror while embodying a photorealistic, personalized avatar. Subsequently, they performed a heartbeat counting task and rated their perceived body awareness and sense of embodiment towards the avatar. We found a significant relationship between sense of embodiment and self-reported body awareness but not between sense of embodiment and heartbeat counting. Future work can build on these findings and further explore the relationship between avatar embodiment and body awareness.',\n",
       " \"The design and evaluation of assisting technologies to support behavior change processes have become an essential topic within the field of human-computer interaction research in general and the field of immersive intervention technologies in particular. The mechanisms and success of behavior change techniques and interventions are broadly investigated in the field of psychology. However, it is not always easy to adapt these psychological findings to the context of immersive technologies. The lack of theoretical foundation also leads to a lack of explanation as to why and how immersive interventions support behavior change processes. The Behavioral Framework for immersive Technologies (BehaveFIT) addresses this lack by 1) presenting an intelligible categorization and condensation of psychological barriers and immersive features, by 2) suggesting a mapping that shows why and how immersive technologies can help to overcome barriers and finally by 3) proposing a generic prediction path that enables a structured, theory-based approach to the development and evaluation of immersive interventions. These three steps explain how BehaveFIT can be used, and include guiding questions for each step. Further, two use cases illustrate the usage of BehaveFIT. Thus, the present paper contributes to guidance for immersive intervention design and evaluation, showing that immersive interventions support behavior change processes and explain and predict 'why' and 'how' immersive interventions can bridge the intention-behavior-gap.\",\n",
       " 'Mind-body therapies aim to improve health by combining physical and mental exercises. Recent developments tend to incorporate virtual reality (VR) into their design and execution, but there is a lack of research concerning the inclusion of virtual bodies and their effect on body awareness in these designs. In this study, 24 participants performed in-VR body awareness movement tasks in front of a virtual mirror while embodying a photorealistic, personalized avatar. Subsequently, they performed a heartbeat counting task and rated their perceived body awareness and sense of embodiment towards the avatar. We found a significant relationship between sense of embodiment and self-reported body awareness but not between sense of embodiment and heartbeat counting. Future work can build on these findings and further explore the relationship between avatar embodiment and body awareness.',\n",
       " 'Mind-body therapies aim to improve health by combining physical and mental exercises. Recent developments tend to incorporate virtual reality (VR) into their design and execution, but there is a lack of research concerning the inclusion of virtual bodies and their effect on body awareness in these designs. In this study, 24 participants performed in-VR body awareness movement tasks in front of a virtual mirror while embodying a photorealistic, personalized avatar. Subsequently, they performed a heartbeat counting task and rated their perceived body awareness and sense of embodiment towards the avatar. We found a significant relationship between sense of embodiment and self-reported body awareness but not between sense of embodiment and heartbeat counting. Future work can build on these findings and further explore the relationship between avatar embodiment and body awareness.',\n",
       " 'Mind-body therapies aim to improve health by combining physical and mental exercises. Recent developments tend to incorporate virtual reality (VR) into their design and execution, but there is a lack of research concerning the inclusion of virtual bodies and their effect on body awareness in these designs. In this study, 24 participants performed in-VR body awareness movement tasks in front of a virtual mirror while embodying a photorealistic, personalized avatar. Subsequently, they performed a heartbeat counting task and rated their perceived body awareness and sense of embodiment towards the avatar. We found a significant relationship between sense of embodiment and self-reported body awareness but not between sense of embodiment and heartbeat counting. Future work can build on these findings and further explore the relationship between avatar embodiment and body awareness.',\n",
       " 'Mind-body therapies aim to improve health by combining physical and mental exercises. Recent developments tend to incorporate virtual reality (VR) into their design and execution, but there is a lack of research concerning the inclusion of virtual bodies and their effect on body awareness in these designs. In this study, 24 participants performed in-VR body awareness movement tasks in front of a virtual mirror while embodying a photorealistic, personalized avatar. Subsequently, they performed a heartbeat counting task and rated their perceived body awareness and sense of embodiment towards the avatar. We found a significant relationship between sense of embodiment and self-reported body awareness but not between sense of embodiment and heartbeat counting. Future work can build on these findings and further explore the relationship between avatar embodiment and body awareness.',\n",
       " 'In this paper we propose omnidirectional galvanic vestibular stimulation (GVS) to mitigate cybersickness in virtual reality applications. One of the most accepted theories indicates that Cybersickness is caused by the visually induced impression of ego motion while physically remaining at rest. As a result of this sensory mismatch, people associate negative symptoms with VR and sometimes avoid the technology altogether. To reconcile the two contradicting sensory perceptions, we investigate GVS to stimulate the vestibular canals behind our ears with low-current electrical signals that are specifically attuned to the visually displayed camera motion. We describe how to calibrate and generate the appropriate GVS signals in real-time for pre-recorded omnidirectional videos exhibiting ego-motion in all three spatial directions. For validation, we conduct an experiment presenting real-world 360° videos shot from a moving first-person perspective in a VR head-mounted display. Our findings indicate that GVS is able to significantly reduce discomfort for cybersickness-susceptible VR users, creating a deeper and more enjoyable immersive experience for many people.',\n",
       " 'In this paper, we propose a method to realize a virtual reality MMOG (Massively Multiplayer Online Video Game) with ultra-low latency. The basic idea of the proposed method is to introduce a layer consisting of several fog nodes between clients and cloud server to offload a part of the rendering task which is conducted by the cloud server in conventional cloud games. We examine three techniques to reduce the latency in such a fog-assisted cloud game: 1) To maintain the consistency of the virtual game space, collision detection of virtual objects is conducted by the cloud server in a centralized manner; 2) To reflect subtle changes of the line of sight to the 3D game view, each client is assigned to a fog node and the head motion of the player acquired through HMD (Head-Mounted Display) is directly sent to the corresponding fog node; and 3) To offload a part of the rendering task, we separate the rendering of the background view from that of the foreground view, and migrate the former to other nodes including the cloud server. The performance of the proposed method is evaluated by experiments with an AWS-based prototype system. It is confirmed that the proposed techniques achieve the latency of 32.3 ms, which is 66 % faster than the conventional systems.',\n",
       " 'This paper presents the results of a pilot study of a virtual reality play based intervention. The results of four single case studies are presented. The virtual reality intervention used the Mandala Gesture Xtreme technology. It was applied to four school aged children with cerebral palsy. A pretest-posttest design was used. The relevant outcome of interest was upper extremity control as measured with the Quality of Upper Extremity Skills Test (QUEST), item #6 of Subtest #5 of the Bruininks-Oseretsky Test of Motor Proficiency (BOTMP), and a measure of percent accuracy . The total scores on the QUEST indicated clinically significant changes in quality of upper extremity for two of the children. Changes were noted in other measures of upper extremity control for each child.Qualitative comments from the participants revealed a high degree of motivation, interest, pleasure, and opportunity for engagement in play activities not previously engaged in. Overall, this pilot study suggests the viability of a virtual reality play based intervention as part of the rehabilitation process for children with cerebral palsy. These results will form the basis of a larger scale randomized clinical trial.',\n",
       " 'Numerous experimental studies on the design of user interfaces of common desktop monitors exist, whereas empirical studies concerning the design of an Augmented Reality user interface are unknown. Therefore, recommendations relating to the optimal representation size of virtual information for an head mounted display with see through mode, which is used for Augmented Reality, cannot be given at the moment. In order to apply this new technology successfully to an industrial area, such information is urgently necessary. For this reason, three different kinds of displays were tested in this study regarding human information perception. Thus, the smallest target size necessary to provide successful information processing could be determined for different tasks. This study gives exemplary results obtained through tests with the prototype of a \"Virtual Retinal Display\".',\n",
       " 'Biometric measures such as the electroencephalogram (EEG) promise to become viable alternatives to subjective questionnaire ratings for the evaluation of psychophysical effects associated with Virtual Reality (VR) systems, as they provide objective and continuous measurements without breaking the exposure. The extent to which the EEG signal can be disturbed by the presence of VR sys- tems, however, has been barely investigated. This study outlines how to evaluate the compatibility of a given EEG-VR setup on the example of two commercial head-mounted displays (HMDs), the Oculus Rift and the HTC Vive Pro. We use a novel experimental protocol to compare the spectral composition between conditions with and without an HMD present during an eyes-open vs. eyes-closed task. We found general artifacts at the line hum of 50 Hz, and additional HMD refresh rate artifacts (90 Hz) for the Oculus rift exclusively. Frequency components typically most interesting to non-invasive EEG research and applications (<50 Hz), however, remained largely unaffected. We observed similar topographies of visually-induced modulation of alpha band power for both HMD conditions in all subjects. Hence, the study introduces a necessary validation test for HMDs in combination with EEG and further promotes EEG as a potential biometric measurement method for psychophysical effects in VR systems.',\n",
       " 'Many studies show the significance of the Proteus effect for serious virtual reality applications. The present study extends the existing knowledge by considering the relationship (congruence) between the self-embodiment (avatar) and the virtual environment. We investigated the impact of avatar and environment types and their congruence on avatar plausibility, sense of embodiment, spatial presence, and the Proteus effect. In a 2 × 2 between-subjects design, participants embodied either an avatar in sports- or business wear in a semantic congruent or incongruent environment while performing lightweight exercises in virtual reality. The avatar-environment congruence significantly affected the avatar’s plausibility but not the sense of embodiment or spatial presence. However, a significant Proteus effect emerged only for participants who reported a high feeling of (virtual) body ownership, indicating that a strong sense of having and owning a virtual body is key to facilitating the Proteus effect. We discuss the results assuming current theories of bottom-up and top-down determinants of the Proteus effect and thus contribute to understanding its underlying mechanisms and determinants.',\n",
       " 'Visual stimuli are frequently used to improve memory, language learning or perception, and understanding of metacognitive processes. However, in virtual reality (VR), there are few systematically and empirically derived databases. This paper proposes the first collection of virtual objects based on empirical evaluation for inter-and transcultural encounters between English- and German-speaking learners. We used explicit and implicit measurement methods to identify cultural associations and the degree of stereotypical perception for each virtual stimuli (n = 293) through two online studies, including native German and English-speaking participants. The analysis resulted in a final well-describable database of 128 objects (called InteractionSuitcase). In future applications, the objects can be used as a great interaction or conversation asset and behavioral measurement tool in social VR applications, especially in the field of foreign language education. For example, encounters can use the objects to describe their culture, or teachers can intuitively assess stereotyped attitudes of the encounters.',\n",
       " 'Many studies show the significance of the Proteus effect for serious virtual reality applications. The present study extends the existing knowledge by considering the relationship (congruence) between the self-embodiment (avatar) and the virtual environment. We investigated the impact of avatar and environment types and their congruence on avatar plausibility, sense of embodiment, spatial presence, and the Proteus effect. In a 2 × 2 between-subjects design, participants embodied either an avatar in sports- or business wear in a semantic congruent or incongruent environment while performing lightweight exercises in virtual reality. The avatar-environment congruence significantly affected the avatar’s plausibility but not the sense of embodiment or spatial presence. However, a significant Proteus effect emerged only for participants who reported a high feeling of (virtual) body ownership, indicating that a strong sense of having and owning a virtual body is key to facilitating the Proteus effect. We discuss the results assuming current theories of bottom-up and top-down determinants of the Proteus effect and thus contribute to understanding its underlying mechanisms and determinants.',\n",
       " 'Many studies show the significance of the Proteus effect for serious virtual reality applications. The present study extends the existing knowledge by considering the relationship (congruence) between the self-embodiment (avatar) and the virtual environment. We investigated the impact of avatar and environment types and their congruence on avatar plausibility, sense of embodiment, spatial presence, and the Proteus effect. In a 2 × 2 between-subjects design, participants embodied either an avatar in sports- or business wear in a semantic congruent or incongruent environment while performing lightweight exercises in virtual reality. The avatar-environment congruence significantly affected the avatar’s plausibility but not the sense of embodiment or spatial presence. However, a significant Proteus effect emerged only for participants who reported a high feeling of (virtual) body ownership, indicating that a strong sense of having and owning a virtual body is key to facilitating the Proteus effect. We discuss the results assuming current theories of bottom-up and top-down determinants of the Proteus effect and thus contribute to understanding its underlying mechanisms and determinants.',\n",
       " 'Many studies show the significance of the Proteus effect for serious virtual reality applications. The present study extends the existing knowledge by considering the relationship (congruence) between the self-embodiment (avatar) and the virtual environment. We investigated the impact of avatar and environment types and their congruence on avatar plausibility, sense of embodiment, spatial presence, and the Proteus effect. In a 2 × 2 between-subjects design, participants embodied either an avatar in sports- or business wear in a semantic congruent or incongruent environment while performing lightweight exercises in virtual reality. The avatar-environment congruence significantly affected the avatar’s plausibility but not the sense of embodiment or spatial presence. However, a significant Proteus effect emerged only for participants who reported a high feeling of (virtual) body ownership, indicating that a strong sense of having and owning a virtual body is key to facilitating the Proteus effect. We discuss the results assuming current theories of bottom-up and top-down determinants of the Proteus effect and thus contribute to understanding its underlying mechanisms and determinants.',\n",
       " 'Visual stimuli are frequently used to improve memory, language learning or perception, and understanding of metacognitive processes. However, in virtual reality (VR), there are few systematically and empirically derived databases. This paper proposes the first collection of virtual objects based on empirical evaluation for inter-and transcultural encounters between English- and German-speaking learners. We used explicit and implicit measurement methods to identify cultural associations and the degree of stereotypical perception for each virtual stimuli (n = 293) through two online studies, including native German and English-speaking participants. The analysis resulted in a final well-describable database of 128 objects (called InteractionSuitcase). In future applications, the objects can be used as a great interaction or conversation asset and behavioral measurement tool in social VR applications, especially in the field of foreign language education. For example, encounters can use the objects to describe their culture, or teachers can intuitively assess stereotyped attitudes of the encounters.',\n",
       " 'Visual stimuli are frequently used to improve memory, language learning or perception, and understanding of metacognitive processes. However, in virtual reality (VR), there are few systematically and empirically derived databases. This paper proposes the first collection of virtual objects based on empirical evaluation for inter-and transcultural encounters between English- and German-speaking learners. We used explicit and implicit measurement methods to identify cultural associations and the degree of stereotypical perception for each virtual stimuli (n = 293) through two online studies, including native German and English-speaking participants. The analysis resulted in a final well-describable database of 128 objects (called InteractionSuitcase). In future applications, the objects can be used as a great interaction or conversation asset and behavioral measurement tool in social VR applications, especially in the field of foreign language education. For example, encounters can use the objects to describe their culture, or teachers can intuitively assess stereotyped attitudes of the encounters.',\n",
       " 'This article introduces the concept of Semantic Traversers (STs) and exemplarily illustrates its utilization inside a Virtual Reality (VR) platform for multimodal construction. The development of reusable and parameterizable routines which are based on the concept of Semantic Reflection is described. These routines work on the Knowledge Representation Layer (KRL) of a simulation framework to realize complex application logic and data flow concepts like field routing. The KRL is implemented by a functionally extended semantic network. The ST development in C++ preserves real-time capabilities while the abstract description of data structures and application logic realizes a persistent and platform-independent representation of programs. The advantages of traverser representation through Semantic Reflection are elaborated. Finally, an editing tool is presented that enables developers to visualize and modify the semantic networks which are used to describe the application.',\n",
       " 'This article introduces the concept of Semantic Traversers (STs) and exemplarily illustrates its utilization inside a Virtual Reality (VR) platform for multimodal construction. The development of reusable and parameterizable routines which are based on the concept of Semantic Reflection is described. These routines work on the Knowledge Representation Layer (KRL) of a simulation framework to realize complex application logic and data flow concepts like field routing. The KRL is implemented by a functionally extended semantic network. The ST development in C++ preserves real-time capabilities while the abstract description of data structures and application logic realizes a persistent and platform-independent representation of programs. The advantages of traverser representation through Semantic Reflection are elaborated. Finally, an editing tool is presented that enables developers to visualize and modify the semantic networks which are used to describe the application.',\n",
       " \"Currently, there is an ongoing debate about the influencing factors of one's extended reality (XR) experience. Plausibility, congruence, and their role have recently gained more and more attention. One of the latest models to describe XR experiences, the Congruence and Plausibility model (CaP), puts plausibility and congruence right in the center. However, it is unclear what influence they have on the overall XR experience and what influences our perceived plausibility rating. In this paper, we implemented four different incongruencies within a virtual reality scene using breaks in plausibility as an analogy to breaks in presence. These manipulations were either located on the cognitive or perceptual layer of the CaP model. They were also either connected to the task at hand or not. We tested these manipulations in a virtual bowling environment to see which influence they had. Our results show that manipulations connected to the task caused a lower perceived plausibility. Additionally, cognitive manipulations seem to have a larger influence than perceptual manipulations. We were able to cause a break in plausibility with one of our incongruencies. These results show a first direction on how the influence of plausibility in XR can be systematically investigated in the future.\",\n",
       " 'In this interdisciplinary work we present a brief report on our experience with virtual reality (VR) for rehabilitation research and present exemplar research projects on integrated pain, mind, and movement research. We specifically discuss our interdisciplinary experience with the use of 3D (stereo and non-stereo) computer graphics techniques for applications in VR rehabilitation research and include documentary production. Finally we present data that shows how virtual environments and movement can alter the pain experience and improve both physical and cognitive function.',\n",
       " 'We describe the components and implementation of a cost-effective fish tank virtual reality system. It is based on commodity hardware and provides accurate view tracking combined with high resolution stereoscopic rendering. The system is calibrated very quickly in a semi-automatic step using computer vision. By avoiding the resolution disadvantages of current VR headsets, our prototype is suitable for a wide range of perceptual VR studies.',\n",
       " 'This article presents an analysis of using multiple Microsoft Kinects to track users in a VR system. More specifically, we analyse the capability of Kinects to track infrared points for use in VR applications. Multiple Kinect sensors may serve as a low cost and affordable means to track position information across a large lab space in applications where precise location tracking is not necessary. We present our findings and analysis of the tracking range of a Kinect sensor in situations in which multiple Kinects are present. Overall, the Kinect sensor works well for this application and in lieu of more expensive options, the Kinect sensors may be a viable option for very low-cost tracking in VR applications.',\n",
       " \"In this paper we test the hypothesis that Virtual Reality (VR) negotiation training positively influences negotiation skill and knowledge. We discuss the design of the VR training. Then, we present the results of a between subject experiment (n=42) with three experimental conditions (control, training once, repeated training) investigating learning effects on subjects' negotiation skill and knowledge. In our case negotiation skill consists of negotiation outcome (final bid utility) and conversation skill (exploratory conversational choices in VR scenario), and negotiation knowledge is the subjects' quality of reflection upon filmed behavior of two negotiating actors. Our results confirm the hypothesis. We found significant effects of training on conversation skill and negotiation knowledge. We found a marginally significant effect of training on negotation outcome. As the effect of training on negotiation outcome was marginally significant and only present when controlling for overshadowing effects of the act of reflecting, we postulate that other learning approaches (e.g., instruction) are needed for trainees to use the information gained during the joint exploration phase of a negotiation for the construction of a bid. Our results are particularly important given the sparse availability of experimental studies that show learning effects of VR negotiation training, and gives additional support to those studies that do report possitive effects such as with the BiLAT system.\",\n",
       " 'Impaired decision-making leads to the inability to distinguish between advantageous and disadvantageous choices. The impairment of a person’s decision-making is a common goal of gambling games. Given the recent trend of gambling using immersive Virtual Reality it is crucial to investigate the effects of both immersion and the virtual environment (VE) on decision-making. In a novel user study, we measured decision-making using three virtual versions of the Iowa Gambling Task (IGT). The versions differed with regard to the degree of immersion and design of the virtual environment. While emotions affect decision-making, we further measured the positive and negative affect of participants. A higher visual angle on a stimulus leads to an increased emotional response. Thus, we kept the visual angle on the Iowa Gambling Task the same between our conditions. Our results revealed no significant impact of immersion or the VE on the IGT. We further found no significant difference between the conditions with regard to positive and negative affect. This suggests that neither the medium used nor the design of the VE causes an impairment of decision-making. However, in combination with a recent study, we provide first evidence that a higher visual angle on the IGT leads to an effect of impairment.',\n",
       " 'Presence is often considered the most important quale describing the subjective feeling of being in a computer-generated and/or computer-mediated virtual environment. The identification and separation of orthogonal presence components, i.e., the place illusion and the plausibility illusion, has been an accepted theoretical model describing Virtual Reality (VR) experiences for some time. This perspective article challenges this presence-oriented VR theory. First, we argue that a place illusion cannot be the major construct to describe the much wider scope of virtual, augmented, and mixed reality (VR, AR, MR: or XR for short). Second, we argue that there is no plausibility illusion but merely plausibility, and we derive the place illusion caused by the congruent and plausible generation of spatial cues and similarly for all the current model’s so-defined illusions. Finally, we propose congruence and plausibility to become the central essential conditions in a novel theoretical model describing XR experiences and effects.',\n",
       " 'Impaired decision-making leads to the inability to distinguish between advantageous and disadvantageous choices. The impairment of a person’s decision-making is a common goal of gambling games. Given the recent trend of gambling using immersive Virtual Reality it is crucial to investigate the effects of both immersion and the virtual environment (VE) on decision-making. In a novel user study, we measured decision-making using three virtual versions of the Iowa Gambling Task (IGT). The versions differed with regard to the degree of immersion and design of the virtual environment. While emotions affect decision-making, we further measured the positive and negative affect of participants. A higher visual angle on a stimulus leads to an increased emotional response. Thus, we kept the visual angle on the Iowa Gambling Task the same between our conditions. Our results revealed no significant impact of immersion or the VE on the IGT. We further found no significant difference between the conditions with regard to positive and negative affect. This suggests that neither the medium used nor the design of the VE causes an impairment of decision-making. However, in combination with a recent study, we provide first evidence that a higher visual angle on the IGT leads to an effect of impairment.',\n",
       " 'Impaired decision-making leads to the inability to distinguish between advantageous and disadvantageous choices. The impairment of a person’s decision-making is a common goal of gambling games. Given the recent trend of gambling using immersive Virtual Reality it is crucial to investigate the effects of both immersion and the virtual environment (VE) on decision-making. In a novel user study, we measured decision-making using three virtual versions of the Iowa Gambling Task (IGT). The versions differed with regard to the degree of immersion and design of the virtual environment. While emotions affect decision-making, we further measured the positive and negative affect of participants. A higher visual angle on a stimulus leads to an increased emotional response. Thus, we kept the visual angle on the Iowa Gambling Task the same between our conditions. Our results revealed no significant impact of immersion or the VE on the IGT. We further found no significant difference between the conditions with regard to positive and negative affect. This suggests that neither the medium used nor the design of the VE causes an impairment of decision-making. However, in combination with a recent study, we provide first evidence that a higher visual angle on the IGT leads to an effect of impairment.',\n",
       " 'Increased levels of interactivity and multi-sensory stimulation have been shown to enhance the immersion of Virtual Reality experiences. We present the AirRes mask that enables users to utilize their breathing for precise natural interactions with the virtual environment without suffering from limitations of the sensing equipment such as motion artifacts. Furthermore, the AirRes mask provides breathing resistance as novel output modality that can be adjusted in real-time by the application. In a user study, we demonstrate the mask’s precision measurements for interaction as well as its ability to use breathing resistance to communicate contextual information such as adverse environmental conditions that affect the user’s virtual avatar. Our results show that the AirRes mask enhances virtual experiences and has the potential to create more immersive scenarios for applications by enforcing the perception of danger or improving situational awareness in training simulations, or for psychotherapy by providing additional physical stimuli.',\n",
       " 'The purpose of our study is to identify the potential roles of a virtual reality (VR) temporal bone simulator in the Otolaryngology residency program. Study Design: Cross-sectional educational design Methods: Five residents ranging from PGY-1 to PGY-4 were evaluated at three settings: 1) Virtual reality temporal bone simulator, 2) Temporal bone laboratory 3) Operating room. Using a previously validated checklist for mastoidectomy, faculty evaluated the residents for the following tasks: 1) Identification and definition of tegmen 2) Definition of sigmoid sinus and sino-dural angle. 3) Use of drill and 4) Overall surgical performance. The evaluations on virtual reality temporal bone simulator were then compared with the evaluations conducted in the operating room and the temporal bone laboratory. Results: The mean total score obtained for residents on VR temporal bone simulator was 2.54±0.75, 3.13±0.55 for OR, and 2.18±0.29 for temporal bone laboratory. The mean total score obtained for all settings among Otology 1 was 2.27±0.72, and for Otology 2 was 2.70±0.68. Conclusion: The surgical performance on the VR temporal bone simulator is concordant with the temporal bone laboratory performance, which provides an indirect evidence of transferability of surgical skills to the OR. VR simulator can assist in learning technical skills for mastoidectomy procedure and attain surgical competency at a faster rate.',\n",
       " 'In market research, the adoption of interactive virtualreality-techniques could be expected to contain many advantages: artificial lab environments could be designed in a more realistic manner and the consideration of “time to the market”-factors could be improved. On the other hand, with an increasing degree of presence and the notional attendance in a simulated test environment, the market research task could fall prey to the tensing virtual reality adventure. In the following study a 3D-technique is empirically tested for its usability in market research. It will be shown that the interactive 3D-simulation is not biased by the immersion it generates and provides considerably better test results than 2D-stimuli do.',\n",
       " 'Due to the central role of the human factor in information security, the need for information security awareness (ISA) is constantly increasing. In order to maintain a high level of ISA, trainings have to be carried out frequently to ensure sustainability. Since education via virtual reality (VR) has led to a sustainable learning effect in other ﬁelds, we evaluated the use of VR for ISA trainings. Moreover, we combined our VR training with immersive storytelling. For the evaluation we used two sets of participants. The ﬁrst used a traditional e-Learning method to answer the questionnaire. The second used our VR training. After one week we repeated the questionnaires. The results showed that the VR group could achieve higher scores than the noVR group. Moreover, the VR group achieved even higher scores after one week which might be due to the sustained learning effect from the VR training.',\n",
       " 'Objective: Gait adaptation to environmental challenges is fundamental for independent and safe community ambulation. The possibility of precisely studying gait modulation using standardized protocols of gait analysis closely resembling everyday life scenarios is still an unmet need.Methods: We have developed a fully-immersive virtual reality (VR) environment where subjects have to adjust their walking pattern to avoid collision with a virtual agent (VA) crossing their gait trajectory. We collected kinematic data of 12 healthy young subjects walking in real world (RW) and in the VR environment, both with (VR/A+) and without (VR/A-) the VA perturbation. The VR environment closely resembled the RW scenario of the gait laboratory. To ensure standardization of the obstacle presentation the starting time speed and trajectory of the VA were defined using the kinematics of the participant as detected online during each walking trial.Results: We did not observe kinematic differences between walking in RW and VR/A-, suggesting that our VR environment per se might not induce significant changes in the locomotor pattern. When facing the VA all subjects consistently reduced stride length and velocity while increasing stride duration. Trunk inclination and mediolateral trajectory deviation also facilitated avoidance of the obstacle.Conclusions: This proof-of-concept study shows that our VR/A+ paradigm effectively induced a timely gait modulation in a standardized immersive and realistic scenario. This protocol could be a powerful research tool to study gait modulation and its derangements in relation to aging and clinical conditions.',\n",
       " 'Television and movie images have been altered ever since it was technically possible. Nowadays embedding advertisements, or incorporating text and graphics in TV scenes, are common practice, but they can not be considered as integrated part of the scene. The introduction of new services for interactive augmented television is discussed in this paper. We analyse the main aspects related with the whole chain of augmented reality production. Interactivity is one of the most important added values of the digital television: This paper aims to break the model where all TV viewers receive the same final image. Thus, we introduce and discuss the new concept of interactive augmented television, i. e. real time composition of video and computer graphics - e.g. a real scene and freely selectable images or spatial rendered objects - edited and customized by the end user within the context of the user\\'s set top box and TV receiver. We demonstrate a sample application introducing \"Interactive Augmented Television\" for sport broadcasts additionally with 3D virtual objects in order to enhance or alter the presentation of the match with a new interface. We also introduce a pure virtual world where the user can select the camera position.',\n",
       " 'This paper presents an approach for constructing improved visual representations of high dimensional objective spaces using virtual reality. These spaces arise from the solution of multi-objective optimisation problems with more than 3 objective functions which lead to high dimensional Pareto fronts. The 3-D representations of m-dimensional Pareto fronts, or their approximations, are constructed via similarity structure mappings between the original objective spaces and the 3-D space. Alpha shapes are introduced for the representation and compared with previous approaches based on convex hulls. In addition, the mappings minimising a measure of the amount of dissimilarity loss are obtained via genetic programming. This approach is preliminarily investigated using both theoretically derived high dimensional Pareto fronts for a test problem (DTLZ2) and practically obtained objective spaces for the 4 dimensional knapsack problem via multi-objective evolutionary algorithms like HLGA, NSGA, and VEGA. The improved representation captures more accurately the real nature of the m-dimensional objective spaces and the quality of the mappings obtained with genetic programming is equivalent to those computed with classical optimization algorithms.',\n",
       " 'The use of augmented reality (AR) in formal education could prove a key component in future learning environments that are richly populated with a blend of hardware and software applications. However, relatively little is known about the potential of this technology to support teaching and learning with groups of young children in the classroom. Analysis of teacher–child dialogue in a comparative study between use of an AR virtual mirror interface and more traditional science teaching methods for 10-year-old children, revealed that the children using AR were less engaged than those using traditional resources. We suggest four design requirements that need to be considered if AR is to be successfully adopted into classroom practice. These requirements are: flexible content that teachers can adapt to the needs of their children, guided exploration so learning opportunities can be maximised, in a limited time, and attention to the needs of institutional and curricular requirements.',\n",
       " 'Objective: Gait adaptation to environmental challenges is fundamental for independent and safe community ambulation. The possibility of precisely studying gait modulation using standardized protocols of gait analysis closely resembling everyday life scenarios is still an unmet need.Methods: We have developed a fully-immersive virtual reality (VR) environment where subjects have to adjust their walking pattern to avoid collision with a virtual agent (VA) crossing their gait trajectory. We collected kinematic data of 12 healthy young subjects walking in real world (RW) and in the VR environment, both with (VR/A+) and without (VR/A-) the VA perturbation. The VR environment closely resembled the RW scenario of the gait laboratory. To ensure standardization of the obstacle presentation the starting time speed and trajectory of the VA were defined using the kinematics of the participant as detected online during each walking trial.Results: We did not observe kinematic differences between walking in RW and VR/A-, suggesting that our VR environment per se might not induce significant changes in the locomotor pattern. When facing the VA all subjects consistently reduced stride length and velocity while increasing stride duration. Trunk inclination and mediolateral trajectory deviation also facilitated avoidance of the obstacle.Conclusions: This proof-of-concept study shows that our VR/A+ paradigm effectively induced a timely gait modulation in a standardized immersive and realistic scenario. This protocol could be a powerful research tool to study gait modulation and its derangements in relation to aging and clinical conditions.',\n",
       " 'We perform biological simulations in the virtual reality context. In order to run large simulations, we choose to put together a set of standard computers and create a grid in charge of distributing the biological simulations. We propose to make the distribution on to the ReISCOP generic model, developed in our laboratory, which allows us to easily design biological simulations. This method is also based on the replication of passive elements. It is a spatial distribution in which local simulations are periodically synchronized and the consistency of replicated data is checked. This synchronization is not a strong synchronization but a weak one. The consistency method is built on the transmission between nodes of the grid of the state variations of the data. The software used for distributing simulations is named DIVA, and is an individual based software located on each node of the grid. DIVA confers a peer to peer architecture upon the grid.',\n",
       " 'Cybersickness is a unpleasant phenomenon caused by the visually induced impression of ego-motion while in fact being seated. To reduce its negative impact in VR experiences, we analyze the effectiveness of two techniques - peripheral blurring and field of view reduction - through an experiment in an interactive race game environment displayed with a commercial head-mounted display with integrated eye tracker. To measure the level of discomfort experienced by our participants, we utilize self-report and physiological measurements. Our results indicate that, among both techniques, reducing the displayed field of view up to 10 degrees is most efficient to mitigate cybersickness.',\n",
       " 'A teaching package has been developed centered around a relativistic virtual reality. It introduces concepts of special relativity to students in a gamelike environment where users experience the effects of traveling at near light speeds. From this perspective, space and time are significantly different from that experienced in everyday life. We explore how students worked with this environment and how they used this experience in their study of special relativity. Students found the simulation to be a positive learning experience and described the subject area as being less abstract after its use. Students were more capable of correctly answering concept questions relating to special relativity, and a small but measurable improvement was observed in the final exam.',\n",
       " 'This article investigates performance and user experience in Social Virtual Reality (SVR) targeting distributed, embodied, and immersive, face-to-face encounters. We demonstrate the close relationship between scalability, reproduction accuracy, and the resulting performance characteristics, as well as the impact of these characteristics on users co-located with larger groups of embodied virtual others. System scalability provides a variable number of co-located avatars and AI-controlled agents with a variety of different appearances, including realistic-looking virtual humans generated from photogrammetry scans. The article reports on how to meet the requirements of embodied SVR with today\\\\u0027s technical off-the-shelf solutions and what to expect regarding features, performance, and potential limitations. Special care has been taken to achieve low latencies and sufficient frame rates necessary for reliable communication of embodied social signals. We propose a hybrid evaluation approach which coherently relates results from technical benchmarks to subjective ratings and which confirms required performance characteristics for the target scenario of larger distributed groups. A user-study reveals positive effects of an increasing number of co-located social companions on the quality of experience of virtual worlds, i.e., on presence, possibility of interaction, and co-presence. It also shows that variety in avatar/agent appearance might increase eeriness but might also stimulate an increased interest of participants about the environment.',\n",
       " 'Plenty of theories, models, measures, and investigations target the understanding of virtual presence, i.e., the sense of presence in immersive Virtual Reality (VR). Other varieties of the so-called eXtended Realities (XR), e.g., Augmented and Mixed Reality (AR and MR) incorporate immersive features to a lesser degree and continuously combine spatial cues from the real physical space and the simulated virtual space. This blurred separation questions the applicability of the accumulated knowledge about the similarities of virtual presence and presence occurring in other varieties of XR, and corresponding outcomes. The present work bridges this gap by analyzing the construct of presence in mixed realities (MR). To achieve this, the following presents (1) a short review of definitions, dimensions, and measurements of presence in VR, and (2) the state of the art views on MR. Additionally, we (3) derived a working definition of MR, extending the Milgram continuum. This definition is based on entities reaching from real to virtual manifestations at one time point. Entities possess different degrees of referential power, determining the selection of the frame of reference. Furthermore, we (4) identified three research desiderata, including research questions about the frame of reference, the corresponding dimension of transportation, and the dimension of realism in MR. Mainly the relationship between the main aspects of virtual presence of immersive VR, i.e., the place-illusion, and the plausibility-illusion, and of the referential power of MR entities are discussed regarding the concept, measures, and design of presence in MR. Finally, (5) we suggested an experimental setup to reveal the research heuristic behind experiments investigating presence in MR. The present work contributes to the theories and the meaning of and approaches to simulate and measure presence in MR. We hypothesize that research about essential underlying factors determining user experience (UX) in MR simulations and experiences is still in its infancy and hopes this article provides an encouraging starting point to tackle related questions.',\n",
       " 'This article investigates performance and user experience in Social Virtual Reality (SVR) targeting distributed, embodied, and immersive, face-to-face encounters. We demonstrate the close relationship between scalability, reproduction accuracy, and the resulting performance characteristics, as well as the impact of these characteristics on users co-located with larger groups of embodied virtual others. System scalability provides a variable number of co-located avatars and AI-controlled agents with a variety of different appearances, including realistic-looking virtual humans generated from photogrammetry scans. The article reports on how to meet the requirements of embodied SVR with today\\\\u0027s technical off-the-shelf solutions and what to expect regarding features, performance, and potential limitations. Special care has been taken to achieve low latencies and sufficient frame rates necessary for reliable communication of embodied social signals. We propose a hybrid evaluation approach which coherently relates results from technical benchmarks to subjective ratings and which confirms required performance characteristics for the target scenario of larger distributed groups. A user-study reveals positive effects of an increasing number of co-located social companions on the quality of experience of virtual worlds, i.e., on presence, possibility of interaction, and co-presence. It also shows that variety in avatar/agent appearance might increase eeriness but might also stimulate an increased interest of participants about the environment.',\n",
       " 'In visual perception, change blindness describes the phenomenon that persons viewing a visual scene may apparently fail to detect significant changes in that scene. These phenomena have been observed in both computer generated imagery and real-world scenes. Several studies have demonstrated that change blindness effects occur primarily during visual disruptions such as blinks or saccadic eye movements. However, until now the influence of stereoscopic vision on change blindness has not been studied thoroughly in the context of visual perception research. In this paper we introduce change blindness techniques for stereoscopic projection systems, providing the ability to substantially modify a virtual scene in a manner that is difficult for observers to perceive. We evaluate techniques for passive and active stereoscopic viewing and compare the results to those of monoscopic viewing conditions. For stereoscopic viewing conditions, we find that change blindness phenomena can be applied with a larger magnitude as compared to monoscopic viewing of a scene. We have also evaluated the potential of the presented techniques for allowing abrupt, and yet significant, changes of a stereoscopically displayed virtual reality environment.',\n",
       " 'This article investigates performance and user experience in Social Virtual Reality (SVR) targeting distributed, embodied, and immersive, face-to-face encounters. We demonstrate the close relationship between scalability, reproduction accuracy, and the resulting performance characteristics, as well as the impact of these characteristics on users co-located with larger groups of embodied virtual others. System scalability provides a variable number of co-located avatars and AI-controlled agents with a variety of different appearances, including realistic-looking virtual humans generated from photogrammetry scans. The article reports on how to meet the requirements of embodied SVR with today\\\\u0027s technical off-the-shelf solutions and what to expect regarding features, performance, and potential limitations. Special care has been taken to achieve low latencies and sufficient frame rates necessary for reliable communication of embodied social signals. We propose a hybrid evaluation approach which coherently relates results from technical benchmarks to subjective ratings and which confirms required performance characteristics for the target scenario of larger distributed groups. A user-study reveals positive effects of an increasing number of co-located social companions on the quality of experience of virtual worlds, i.e., on presence, possibility of interaction, and co-presence. It also shows that variety in avatar/agent appearance might increase eeriness but might also stimulate an increased interest of participants about the environment.',\n",
       " 'Virtual reality (VR) has long promised to revolutionize education, but with little follow-through. Part of the reason for this is the prohibitive cost of immersive VR headsets or caves. This has changed with the advent of smartphone-based VR (along the lines of Google cardboard) which allows students to use smartphones and inexpensive plastic or cardboard viewers to enjoy stereoscopic VR simulations. We have completed the largest-ever such study on 627 students enrolled in calculus-based freshman physics at The Ohio State University. This initial study focused on student understanding of electric fields. Students were split into three treatments groups: VR, video, and static 2D images. Students were asked questions before, during, and after treatment. Here we present a preliminary analysis including overall post-pre improvement among the treatment groups, dependence of improvement on gender, and previous video game experience. Results on select questions are discussed. Several electric field visualizations similar to those used in this study are freely available on Google Play <a href=\"http://go.osu.edu/BuckeyeVR\">this http URL</a>',\n",
       " 'Humanoid robots (i.e., robots with a human-like body) are projected to be mass marketed in the future in several fields of application. Today, however, user evaluations of humanoid robots are often based on mediated depictions rather than actual observations or interactions with a robot, which holds true not least for scientific user studies. People can be confronted with robots in various modes of presentation, among them (1) 2D videos, (2) 3D, i.e., stereoscopic videos, (3) immersive Virtual Reality (VR), or (4) live on site. A systematic investigation into how such differential modes of presentation influence user perceptions of a robot is still lacking. Thus, the current study systematically compares the effects of different presentation modes with varying immersive potential on user evaluations of a humanoid service robot. Participants (N = 120) observed an interaction between a humanoid service robot and an actor either on 2D or 3D video, via a virtual reality headset (VR) or live. We found support for the expected effect of the presentation mode on perceived immediacy. Effects regarding the degree of human likeness that was attributed to the robot were mixed. The presentation mode had no influence on evaluations in terms of eeriness, likability, and purchase intentions. Implications for empirical research on humanoid robots and practice are discussed.',\n",
       " \"Post-surgical pain has been consistently reported in pediatrics as being difficult to manage and limiting to surgical outcomes. Pain management of children is not ideal, and some children unable to tolerate traditional pharmacological agents. Virtual reality (VR) is a new and promising form of non-pharmacologic analgesia. This case study explored the use of VR analgesia with a 16-year-old patient with cerebral palsy participating in a twice-daily physiotherapy program following Single Event Multi-Level Surgery. Over 6 days, the patient spent half of his physiotherapy sessions using VR and the other half without (order randomized). Traditional pharmacological pain management was administered throughout the trial. Using a subjective pain scale (five faces denoting levels of pain), the patient's overall pain ratings whilst in the VR (experimental) condition were 41.2\\\\% less than those in the no-VR (control) condition. This case report provides the first evidence that VR may serve as a powerful non-pharmacologic analgesic for children following surgery.\",\n",
       " 'Navigation is considered one of the most fundamental challenges in Virtual Reality (VR) and has been extensively researched 11. The world-in-miniature (WIM) navigation metaphor allows users to travel in large-scale virtual environments (VEs) regardless of available physical space while maintaining a high-level overview of the VE. It relies on a hand-held, scaled-down duplicate of the entire VE, where the user’s current position is displayed, and an interface provided to introduce his/her next movements 17. There are several extensions to deal with challenges of this navigation technique, e.g. scaling and scrolling 23. In this work, a WIM is presented that integrates state-of-the-art research insights and incorporates additional features that became apparent during the integration process. These features are needed to improve user interactions and to provide both look-ahead and post-travel feedback. For instance, a novel occlusion handling feature hides the WIM geometry in a rounded space reaching from the user’s hand to his/her forearm. This allows the user to interact with occluded areas of the WIM such as buildings. Further extensions include different visualizations for occlusion handling, an interactive preview screen, post-travel feedback, automatic WIM customization, a unified diegetic UI design concerning WIM and user representation, and an adaptation of widely established gestures to control scaling and scrolling of the WIM. Overall, the presented WIM design integrates and extends state-of-the-art interaction tasks and visualization concepts to overcome open conceptual gaps and to provide a comprehensive practical solution for traveling in VR.',\n",
       " 'Augmented Reality (AR) und Virtual Reality (VR) finden zunehmend Eingang in die Bildungspraxis. Mit ihrem Einsatz sind sowohl Potentiale als auch mögliche Problemlagen für Lehr- und Lernprozesse verbunden. Daher ist es Aufgabe der Lehrerbildung, (angehenden) Lehrpersonen einen Kompetenzerwerb für die Einbindung von AR und VR in Lehr- und Lernprozesse zu ermöglichen. Vor diesem Hintergrund wurde ein interdisziplinäres Konzept für die Hochschullehre entwickelt und hinsichtlich der Zielerreichung empirisch evaluiert. Im Beitrag werden zunächst bedeutsame Gestaltungsaspekte des Konzepts sowie erste Befunde aus einer Pilotuntersuchung vorgestellt. Im Anschluss werden handlungspraktische Erfahrungen der interdisziplinären Zusammenarbeit reflektiert und diskutiert.',\n",
       " 'Augmented Reality (AR) und Virtual Reality (VR) finden zunehmend Eingang in die Bildungspraxis. Mit ihrem Einsatz sind sowohl Potentiale als auch mögliche Problemlagen für Lehr- und Lernprozesse verbunden. Daher ist es Aufgabe der Lehrerbildung, (angehenden) Lehrpersonen einen Kompetenzerwerb für die Einbindung von AR und VR in Lehr- und Lernprozesse zu ermöglichen. Vor diesem Hintergrund wurde ein interdisziplinäres Konzept für die Hochschullehre entwickelt und hinsichtlich der Zielerreichung empirisch evaluiert. Im Beitrag werden zunächst bedeutsame Gestaltungsaspekte des Konzepts sowie erste Befunde aus einer Pilotuntersuchung vorgestellt. Im Anschluss werden handlungspraktische Erfahrungen der interdisziplinären Zusammenarbeit reflektiert und diskutiert.',\n",
       " 'Navigation is considered one of the most fundamental challenges in Virtual Reality (VR) and has been extensively researched 11. The world-in-miniature (WIM) navigation metaphor allows users to travel in large-scale virtual environments (VEs) regardless of available physical space while maintaining a high-level overview of the VE. It relies on a hand-held, scaled-down duplicate of the entire VE, where the user’s current position is displayed, and an interface provided to introduce his/her next movements 17. There are several extensions to deal with challenges of this navigation technique, e.g. scaling and scrolling 23. In this work, a WIM is presented that integrates state-of-the-art research insights and incorporates additional features that became apparent during the integration process. These features are needed to improve user interactions and to provide both look-ahead and post-travel feedback. For instance, a novel occlusion handling feature hides the WIM geometry in a rounded space reaching from the user’s hand to his/her forearm. This allows the user to interact with occluded areas of the WIM such as buildings. Further extensions include different visualizations for occlusion handling, an interactive preview screen, post-travel feedback, automatic WIM customization, a unified diegetic UI design concerning WIM and user representation, and an adaptation of widely established gestures to control scaling and scrolling of the WIM. Overall, the presented WIM design integrates and extends state-of-the-art interaction tasks and visualization concepts to overcome open conceptual gaps and to provide a comprehensive practical solution for traveling in VR.',\n",
       " 'Augmented Reality (AR) und Virtual Reality (VR) finden zunehmend Eingang in die Bildungspraxis. Mit ihrem Einsatz sind sowohl Potentiale als auch mögliche Problemlagen für Lehr- und Lernprozesse verbunden. Daher ist es Aufgabe der Lehrerbildung, (angehenden) Lehrpersonen einen Kompetenzerwerb für die Einbindung von AR und VR in Lehr- und Lernprozesse zu ermöglichen. Vor diesem Hintergrund wurde ein interdisziplinäres Konzept für die Hochschullehre entwickelt und hinsichtlich der Zielerreichung empirisch evaluiert. Im Beitrag werden zunächst bedeutsame Gestaltungsaspekte des Konzepts sowie erste Befunde aus einer Pilotuntersuchung vorgestellt. Im Anschluss werden handlungspraktische Erfahrungen der interdisziplinären Zusammenarbeit reflektiert und diskutiert.',\n",
       " 'This article presents an immersive Virtual Reality (VR) system for training classroom management skills, with a specific focus on learning to manage disruptive student behaviour in face-to-face, one-to-many teaching scenarios. The core of the system is a real-time 3D virtual simulation of a classroom, populated by twenty-four semi-autonomous virtual students. The system has been designed as a companion tool for classroom management seminars in a syllabus for primary and secondary school teachers. Whereby, it will allow lecturers to link theory with practice, using the medium of VR. The system is therefore designed for two users: a trainee teacher and an instructor supervising the training session. The teacher is immersed in a real-time 3D simulation of a classroom by means of a head-mounted display and headphone. The instructor operates a graphical desktop console which renders a view of the class and the teacher, whose avatar movements are captured by a marker-less tracking system. This console includes a 2D graphics menu with convenient behaviour and feedback control mechanisms to provide human-guided training sessions. The system is built using low-cost consumer hardware and software. Its architecture and technical design are described in detail. A first evaluation confirms its conformance to critical usability requirements (i.e., safety and comfort, believability, simplicity, acceptability, extensibility, affordability and mobility). Our initial results are promising, and constitute the necessary first step toward a possible investigation of the efficiency and effectiveness of such a system in terms of learning outcomes and experience.',\n",
       " 'This article presents an immersive Virtual Reality (VR) system for training classroom management skills, with a specific focus on learning to manage disruptive student behaviour in face-to-face, one-to-many teaching scenarios. The core of the system is a real-time 3D virtual simulation of a classroom, populated by twenty-four semi-autonomous virtual students. The system has been designed as a companion tool for classroom management seminars in a syllabus for primary and secondary school teachers. Whereby, it will allow lecturers to link theory with practice, using the medium of VR. The system is therefore designed for two users: a trainee teacher and an instructor supervising the training session. The teacher is immersed in a real-time 3D simulation of a classroom by means of a head-mounted display and headphone. The instructor operates a graphical desktop console which renders a view of the class and the teacher, whose avatar movements are captured by a marker-less tracking system. This console includes a 2D graphics menu with convenient behaviour and feedback control mechanisms to provide human-guided training sessions. The system is built using low-cost consumer hardware and software. Its architecture and technical design are described in detail. A first evaluation confirms its conformance to critical usability requirements (i.e., safety and comfort, believability, simplicity, acceptability, extensibility, affordability and mobility). Our initial results are promising, and constitute the necessary first step toward a possible investigation of the efficiency and effectiveness of such a system in terms of learning outcomes and experience.',\n",
       " 'A method for the construction of Virtual Reality spaces for visual data mining using multi-objective optimisation with genetic algorithms on non-linear discriminant (NDA) neural networks is presented. Two neural network layers (output and last hidden) are used for the construction of simultaneous solutions for: a supervised classification of data patterns and an unsupervised similarity structure preservation between the original data matrix and its image in the new space. A set of spaces are constructed from selected solutions along the Pareto front. This strategy represents a conceptual improvement over spaces computed by single-objective optimisation. In addition, genetic programming (in particular gene expression programming) is used for finding analytic representations of the complex mappings generating the spaces (a composition of NDA and orthogonal principal components). The presented approach is domain independent and is illustrated via application to the geophysical prospecting of caves.',\n",
       " 'This article presents an immersive Virtual Reality (VR) system for training classroom management skills, with a specific focus on learning to manage disruptive student behaviour in face-to-face, one-to-many teaching scenarios. The core of the system is a real-time 3D virtual simulation of a classroom, populated by twenty-four semi-autonomous virtual students. The system has been designed as a companion tool for classroom management seminars in a syllabus for primary and secondary school teachers. Whereby, it will allow lecturers to link theory with practice, using the medium of VR. The system is therefore designed for two users: a trainee teacher and an instructor supervising the training session. The teacher is immersed in a real-time 3D simulation of a classroom by means of a head-mounted display and headphone. The instructor operates a graphical desktop console which renders a view of the class and the teacher, whose avatar movements are captured by a marker-less tracking system. This console includes a 2D graphics menu with convenient behaviour and feedback control mechanisms to provide human-guided training sessions. The system is built using low-cost consumer hardware and software. Its architecture and technical design are described in detail. A first evaluation confirms its conformance to critical usability requirements (i.e., safety and comfort, believability, simplicity, acceptability, extensibility, affordability and mobility). Our initial results are promising, and constitute the necessary first step toward a possible investigation of the efficiency and effectiveness of such a system in terms of learning outcomes and experience.',\n",
       " 'This article presents an immersive virtual reality (VR) system for training classroom management skills, with a specific focus on learning to manage disruptive student behavior in face-to-face, one-to-many teaching scenarios. The core of the system is a real-time 3D virtual simulation of a classroom populated by twenty-four semi-autonomous virtual students. The system has been designed as a companion tool for classroom management seminars in a syllabus for primary and secondary school teachers. This will allow lecturers to link theory with practice using the medium of VR. The system is therefore designed for two users: a trainee teacher and an instructor supervising the training session. The teacher is immersed in a real-time 3D simulation of a classroom by means of a head-mounted display and headphone. The instructor operates a graphical desktop console, which renders a view of the class and the teacher whose avatar movements are captured by a marker less tracking system. This console includes a 2D graphics menu with convenient behavior and feedback control mechanisms to provide human-guided training sessions. The system is built using low-cost consumer hardware and software. Its architecture and technical design are described in detail. A first evaluation confirms its conformance to critical usability requirements (i.e., safety and comfort, believability, simplicity, acceptability, extensibility, affordability, and mobility). Our initial results are promising and constitute the necessary first step toward a possible investigation of the efficiency and effectiveness of such a system in terms of learning outcomes and experience.',\n",
       " 'Virtual environments (VEs) can evoke and support emotions, as experienced when playing emotionally arousing games. We theoretically approach the design of fear and joy evoking VEs based on a literature review of empirical studies on virtual and real environments as well as video games’ reviews and content analyses. We define the design space and identify central design elements that evoke specific positive and negative emotions. Based on that, we derive and present guidelines for emotion-inducing VE design with respect to design themes, colors and textures, and lighting configurations. To validate our guidelines in two user studies, we 1) expose participants to 360° videos of VEs designed following the individual guidelines and 2) immerse them in a neutral, positive and negative emotion-inducing VEs combining all respective guidelines in Virtual Reality. The results support our theoretically derived guidelines by revealing significant differences in terms of fear and joy induction.',\n",
       " 'Recent work has demonstrated that fall risk can be attributed to cognitive as well as motor deficits. Indeed, everyday walking in complex environments utilizes executive function, dual tasking, planning and scanning, all while walking forward. Pilot studies suggest that a multi-modal intervention that combines treadmill training to target motor function and a virtual reality obstacle course to address the cognitive components of fall risk may be used to successfully address the motor-cognitive interactions that are fundamental for fall risk reduction. The proposed randomized controlled trial will evaluate the effects of treadmill training augmented with virtual reality on fall risk.',\n",
       " 'Driven by large industry investments, developments of Virtual Reality (VR) technologies including unobtrusive sensors, actuators and novel display devices are rapidly progressing. Realism and interactivity have been postulated as crucial aspects of immersive VR since the naissance of the concept. However, today\\'s VR still falls short from creating real life-like experiences in many regards. This holds particularly true when introducing the \"social dimension\" into the virtual worlds. Apparently, creating convincing virtual selves and virtual others and conveying meaningful and appropriate social behavior still is an open challenge for future VR. This challenge implies both, technical aspects, such as the real-time capacities of the systems, but also psychological aspects, such as the dynamics of human communication. Our knowledge of VR systems is still fragmented with regard to social cognition, although the social dimension is crucial when aiming at autonomous agents with a certain social background intelligence. It can be questioned though whether a perfect copy of real life interactions is a realistic or even meaningful goal of social VR development at this stage. Taking into consideration the specific strengths and weaknesses of humans and machines, we propose a conceptual turn in social VR which focuses on what we call \"hybrid avatar-agent systems\". Such systems are required to generate i) avatar mediated interactions between real humans, taking advantage of their social intuitions and flexible communicative skills and ii) an artificial social intelligence (AIS) which monitors, and potentially moderates or transforms manipulations of behavior in intercultural conversations. The current article sketches a respective base architecture and discusses necessary research prospects and challenges as a starting point for future research and development.',\n",
       " 'Driven by large industry investments, developments of Virtual Reality (VR) technologies including unobtrusive sensors, actuators and novel display devices are rapidly progressing. Realism and interactivity have been postulated as crucial aspects of immersive VR since the naissance of the concept. However, today\\\\u0027s VR still falls short from creating real life-like experiences in many regards. This holds particularly true when introducing the \"social dimension\" into the virtual worlds. Apparently, creating convincing virtual selves and virtual others and conveying meaningful and appropriate social behavior still is an open challenge for future VR. This challenge implies both, technical aspects, such as the real-time capacities of the systems, but also psychological aspects, such as the dynamics of human communication. Our knowledge of VR systems is still fragmented with regard to social cognition, although the social dimension is crucial when aiming at autonomous agents with a certain social background intelligence. It can be questioned though whether a perfect copy of real life interactions is a realistic or even meaningful goal of social VR development at this stage. Taking into consideration the specific strengths and weaknesses of humans and machines, we propose a conceptual turn in social VR which focuses on what we call \"hybrid avatar-agent systems\". Such systems are required to generate i) avatar mediated interactions between real humans, taking advantage of their social intuitions and flexible communicative skills and ii) an artificial social intelligence (AIS) which monitors, and potentially moderates or transforms manipulations of behavior in intercultural conversations. The current article sketches a respective base architecture and discusses necessary research prospects and challenges as a starting point for future research and development.',\n",
       " 'Driven by large industry investments, developments of Virtual Reality (VR) technologies including unobtrusive sensors, actuators and novel display devices are rapidly progressing. Realism and interactivity have been postulated as crucial aspects of immersive VR since the naissance of the concept. However, today\\'s VR still falls short from creating real life-like experiences in many regards. This holds particularly true when introducing the \"social dimension\" into the virtual worlds. Apparently, creating convincing virtual selves and virtual others and conveying meaningful and appropriate social behavior still is an open challenge for future VR. This challenge implies both, technical aspects, such as the real-time capacities of the systems, but also psychological aspects, such as the dynamics of human communication. Our knowledge of VR systems is still fragmented with regard to social cognition, although the social dimension is crucial when aiming at autonomous agents with a certain social background intelligence. It can be questioned though whether a perfect copy of real life interactions is a realistic or even meaningful goal of social VR development at this stage. Taking into consideration the specific strengths and weaknesses of humans and machines, we propose a conceptual turn in social VR which focuses on what we call \"hybrid avatar-agent systems\". Such systems are required to generate i) avatar mediated interactions between real humans, taking advantage of their social intuitions and flexible communicative skills and ii) an artificial social intelligence (AIS) which monitors, and potentially moderates or transforms manipulations of behavior in intercultural conversations. The current article sketches a respective base architecture and discusses necessary research prospects and challenges as a starting point for future research and development.',\n",
       " 'This paper investigates the e ect of avatar realism on embodiment and social interactions in Virtual Reality (VR). We compared abstract avatar representations based on a wooden mannequin with high fidelity avatars generated from photogrammetry 3D scan methods. Both avatar representations were alternately applied to participating users and to the virtual counterpart in dyadic social encounters to examine the impact of avatar realism on self-embodiment and social interaction quality. Users were immersed in a virtual room via a head mounted display (HMD). Their full-body movements were tracked and mapped to respective movements of their avatars. Embodiment was induced by presenting the users’ avatars to themselves in a virtual mirror. Afterwards they had to react to a non-verbal behavior of a virtual interaction partner they encountered in the virtual space. Several measures were taken to analyze the effect of the appearance of the users’ avatars as well as the effect of the appearance of the others’ avatars on the users. The realistic avatars were rated significantly more human-like when used as avatars for the others and evoked a stronger acceptance in terms of virtual body ownership (VBO). There also was some indication of a potential uncanny valley. Additionally, there was an indication that the appearance of the others’ avatars impacts the self-perception of the users.',\n",
       " \"Active exploration enables us humans to construct a rich and coherent percept of our environment. By far the most natural way to move through the real world is via locomotion like walking or running. The same should also be true for computer generated three-dimensional environments. Keeping such an active and dynamic ability to navigate through large-scale virtual scenes is of great interest for many 3D applications demanding locomotion, such as tourism, architecture or interactive entertainment. However, today it is still mostly impossible to freely walk through computer generated environments in order to actively explore them. The primary reason for this is the scientific and technological underdevelopment in this sector. While moving in the real world, sensory information such as vestibular, proprioceptive, and efferent copy signals as well as visual information create consistent multi-sensory cues that indicate one's own motion, i.e., acceleration, speed and direction of travel. Computer graphics environments were initially restricted to visual displays, combined with interaction devices, e.g. joystick or mouse, for providing (often unnatural) inputs to generate self-motion. Nowadays, more and more interaction devices, e.g., Nintendo's Wii or Sony's EyeToy, enable intuitive and natural interaction. In this context, many research groups are investigating natural, multimodal methods of generating self-motion in virtual worlds based on these consumer hardware. An obvious approach is to transfer the user's tracked head movements to changes of the camera in the virtual world by means of a one-to-one mapping. Then, a one meter movement in the real world is mapped to a one meter movement of the virtual camera in the corresponding direction in the virtual environment (VE). This technique has the drawback that the user's movements are restricted by a limited range of the tracking sensors, e.g. optical cameras, and a rather small workspace in the real world. The size of the virtual world often differs from the size of the tracked space so that a straightforward implementation of omni-directional and unlimited walking is not possible. Thus, concepts for virtual locomotion methods are needed that enable walking over large distances in the virtual world while remaining within a relatively small space in the real world. In this tutorial we will present an overview about the development of locomotion interfaces for computer generated virtual environments ranging from desktop-based camera manipulations simulating walking, and different walking metaphors for virtual reality (VR)-based environments to state-of-the-art hardware-based solutions that enable omni-directional and unlimited real walking through virtual worlds.\",\n",
       " 'We present in this paper a new method for building and exploring a 3D hypermedia in virtual reality and for a biomedical domain. Starting from the acquisition of stereoscopic images and from the calibration of cameras, our system offers the user the possibility to visualize these images in 3D and to annotate specific areas with texts or voice recording. Then the user may define links between annotations, and each annotation may point to annotations from the same image or from other images. The user thus constructs a 3D hypermedia. He may have a global view of the images database using a graph display. We present a first graph which was built with images of faces.',\n",
       " 'Medical graduates lack procedural skills experience required to manage emergencies. Recent advances in virtual reality (VR)technology enable the creation of highly immersive learning environments representing easy-to-use and affordable solutionsfor training with simulation. However, the feasibility in compulsory teaching, possible side effects of immersion, perceivedstress, and didactic benefits have to be investigated systematically. VR-based training sessions using head-mounted displaysalongside a real-time dynamic physiology system were held by student assistants for small groups followed by debriefing witha tutor. In the pilot study, 36 students rated simulation sickness. In the main study, 97 students completed a virtual scenarioas active participants (AP) and 130 students as observers (OBS) from the first-person perspective on a monitor. Participantscompleted questionnaires for evaluation purposes and exploratory factor analysis was performed on the items. The extent ofsimulation sickness remained low to acceptable among participants of the pilot study. In the main study, students valued therealistic environment and guided practical exercise. AP perceived the degree of immersion as well as the estimated learn-ing success to be greater than OBS and proved to be more motivated post training. With respect to AP, the factor “sense ofcontrol” revealed a typical inverse U-shaped relationship to the scales “didactic value” and “individual learning benefit”.Summing up, curricular implementation of highly immersive VR-based training of emergencies proved feasible and founda high degree of acceptance among medical students. This study also provides insights into how different conceptions of perceived stress distinctively moderate subjective learning success.',\n",
       " 'Medical graduates lack procedural skills experience required to manage emergencies. Recent advances in virtual reality (VR) technology enable the creation of highly immersive learning environments representing easy-to-use and affordable solutions for training with simulation. However, the feasibility in compulsory teaching, possible side effects of immersion, perceived stress, and didactic benefits have to be investigated systematically. VR-based training sessions using head-mounted displays alongside a real-time dynamic physiology system were held by student assistants for small groups followed by debriefing with a tutor. In the pilot study, 36 students rated simulation sickness. In the main study, 97 students completed a virtual scenario as active participants (AP) and 130 students as observers (OBS) from the first-person perspective on a monitor. Participants completed questionnaires for evaluation purposes and exploratory factor analysis was performed on the items. The extent of simulation sickness remained low to acceptable among participants of the pilot study. In the main study, students valued the realistic environment and guided practical exercise. AP perceived the degree of immersion as well as the estimated learning success to be greater than OBS and proved to be more motivated post training. With respect to AP, the factor “sense of control” revealed a typical inverse U-shaped relationship to the scales “didactic value” and “individual learning benefit”. Summing up, curricular implementation of highly immersive VR-based training of emergencies proved feasible and found a high degree of acceptance among medical students. This study also provides insights into how different conceptions of perceived stress distinctively moderate subjective learning success.',\n",
       " 'Medical graduates lack procedural skills experience required to manage emergencies. Recent advances in virtual reality (VR) technology enable the creation of highly immersive learning environments representing easy-to-use and affordable solutions for training with simulation. However, the feasibility in compulsory teaching, possible side effects of immersion, perceived stress, and didactic benefits have to be investigated systematically. VR-based training sessions using head-mounted displays alongside a real-time dynamic physiology system were held by student assistants for small groups followed by debriefing with a tutor. In the pilot study, 36 students rated simulation sickness. In the main study, 97 students completed a virtual scenario as active participants (AP) and 130 students as observers (OBS) from the first-person perspective on a monitor. Participants completed questionnaires for evaluation purposes and exploratory factor analysis was performed on the items. The extent of simulation sickness remained low to acceptable among participants of the pilot study. In the main study, students valued the realistic environment and guided practical exercise. AP perceived the degree of immersion as well as the estimated learning success to be greater than OBS and proved to be more motivated post training. With respect to AP, the factor “sense of control” revealed a typical inverse U-shaped relationship to the scales “didactic value” and “individual learning benefit”. Summing up, curricular implementation of highly immersive VR-based training of emergencies proved feasible and found a high degree of acceptance among medical students. This study also provides insights into how different conceptions of perceived stress distinctively moderate subjective learning success.',\n",
       " 'Speed has become a way of life. We are asymptotically piling data. Speed can be achieved with new design processes, techniques, and Technology. Innovations AR and VR are just some of the many forms of technologies that will play a key role in shaping the Architecture and Planning of tomorrow, making it future-ready and ushering in a new age of innovation. AR and VR in Architecture & Planning were introduced as assisting tools and has helped generate multiple design options, expanded possibilities of visualization, and provided us with more enhanced, detailed, and specific experience in real-time; enabling us to see the resultsof work on hand well before the commencement of the project. These tools are further developed for city development decisions, helping citizens interact with local authorities, access public services, and plan their commute. After reviewing multiple research papers, it had been observed that each one is moving forward with the changes brought by it, without entirely understanding its role. This paper provides a summary of theappliance of AR & VR in architecture and planning.',\n",
       " 'This paper presents a framework to achieve real-time augmented reality applications. We propose a framework based on the visual servoing approach well known in robotics. We consider pose or viewpoint computation as a similar problem to visual servoing. It allows one to take advantage of all the research that has been carried out in this domain in the past. The proposed method features simplicity, accuracy, efficiency, and scalability wrt. to the camera model as well as wrt. the features extracted from the image. We illustrate the efficiency of our approach on augmented reality applications with various real image sequences.',\n",
       " 'This article compares two state-of-the-art text input techniques between non-stationary virtual reality (VR) and video see-through augmented reality (VST AR) use-cases as XR display condition. The developed contact-based mid-air virtual tap and wordgesture (swipe) keyboard provide established support functions for text correction, word suggestions, capitalization, and punctuation. A user evaluation with 64 participants revealed that XR displays and input techniques strongly affect text entry performance, while subjective measures are only influenced by the input techniques. We found significantly higher usability and user experience ratings for tap keyboards compared to swipe keyboards in both VR and VST AR. Task load was also lower for tap keyboards. In terms of performance, both input techniques were significantly faster in VR than in VST AR. Further, the tap keyboard was significantly faster than the swipe keyboard in VR. Participants showed a significant learning effect with only ten sentences typed per condition. Our results are consistent with previous work in VR and optical see-through (OST) AR, but additionally provide novel insights into usability and performance of the selected text input techniques for VST AR. The significant differences in subjective and objective measures emphasize the importance of specific evaluations for each possible combination of input techniques and XR displays to provide reusable, reliable, and high-quality text input solutions. With our work, we form a foundation for future research and XR workspaces. Our reference implementation is publicly available to encourage replicability and reuse in future XR workspaces.',\n",
       " \"As the demand of Virtual Reality (VR) video streaming to mobile devices increases, novel optimization transport techniques need to be designed to cope with these ultra-high-bandwidth video services. One approach currently attracting attention is the application of adaptive tile-based streaming solutions to the VR video arena. The VR videos are encoded at different quality levels, temporally divided into segments and spatially split into tiles. During the streaming, each of these tiles can be transmitted independently according to its location within the frame (i.e., within or outside of the user's field of view). These methods open a new venue for bandwidth and latency optimization for the streaming of VR videos. However, the effect of the different adaptive streaming optimizations on the end-user perception is still an open research topic. In this demo, we present a VR video platform to experience the working principles of adaptive tile-based VR video streaming services. Through different tiling approaches, bandwidth conditions and viewport algorithms, it allows the users to explore the effects that each optimization has on the perception of the service. In addition, the platform provides real-time bandwidth savings and objective Quality of Experience (QoE) measurements to provide a quantitative analysis of the optimizations effects. This demo aims to provide a common playground for researchers to benchmark, and evaluate the performance of their optimization solutions.\",\n",
       " 'Immersive virtual environments and natural 3D user interfaces have shown great potential in the field of architecture, especially for exploration, presentation and review of designs. In this paper we propose an augmented virtual studio environment for architectural exploration based on a mixed-reality head-mounted display environment. The proposed system supports (1) real walking through large virtual building models, (2) visual feedback about the user’s body and (3) display of real-world objects in the virtual view based on color transfer functions. We describe the locomotion user interface for immersive exploration, as well as a mixed-reality 3D user interface for interaction with virtual designs.',\n",
       " 'Augmented Reality uses Head Mounted Displays (HMD) to overlay the real word with additional virtual information. Virtual Retinal Displays (VRD), a new display technology, no longer need Liquid Crystal Displays (LCD). The VRD technology addresses the retina directly with a single laser stream of pixels. Empirical studies concerning the user’s informational strain of this new VRD technology are unknown. Various papers have shown, that the Heart Rate Variability (HRV) is a valid indicator for the user’s informational strain. An empirical test revealed no difference in the user’s HRV between the VRD technology and the LCD technology. Consequently, there seems to be an comparable users informational strain regarding the display types.',\n",
       " \"Augmented Reality uses Head Mounted Displays (HMD) to overlay the real word with additional virtual information. Virtual Retinal Displays (VRD), a new display technology, no longer requires Liquid Crystal Displays (LCD). VRD technology addresses the retina directly with a single laser stream of pixels. There are no studies on the user's informational strain in this new VRD technology. Various papers have shown that Heart Rate Variability (HRV) is a valid indicator for the user's informational strain. An empirical test revealed no difference in the user's HRV between VRD technology and LCD technology. Consequently, there seems to be a comparable user informational strain regarding the display types.\",\n",
       " 'In this paper we propose solutions that allow several co-located city planners to perform domain-specific interaction tasks together in one semi-immersive pro jection-based VR system. After a registration process, e.g., by posing gestures or by using speech commands, this group activity can be performed either in a cooperative mode, i.e., tasks are accomplished consecutively, or in a col laborative interaction mode, i.e., users collaborate simultaneously. Both modes as well as multimodal concepts for the registration process are discussed and evaluated. Further VR-based interaction strategies for the city planning domain are presented, which are adapted to the demands of city planners working together in a shared virtual environment.',\n",
       " 'Goal: This paper presents an immersive Virtual Reality (VR) system to analyze and train Executive Functions (EFs) of soccer players. EFs are important cognitive functions for athletes. They are a relevant quality that distinguishes amateurs from professionals. Method: The system is based on immersive technology, hence, the user interacts naturally and experiences a training session in a virtual world. The proposed system has a modular design supporting the extension of various so-called game modes. Game modes combine selected game mechanics with specific simulation content to target particular training aspects. The system architecture decouples selection/parameterization and analysis of training sessions via a coaching app from an Unity3D-based VR simulation core. Monitoring of user performance and progress is recorded by a database that sends the necessary feedback to the coaching app for analysis. Results: The system is tested for VR-critical performance criteria to reveal the usefulness of a new interaction paradigm in the cognitive training and analysis of EFs. Subjective ratings for overall usability show that the design as VR application enhances the user experience compared to a traditional desktop app; whereas the new, unfamiliar interaction paradigm does not negatively impact the effort for using the application. Conclusion: The system can provide immersive training of EF in a fully virtual environment, eliminating potential distraction. It further provides an easy-to-use analyzes tool to compare user but also an automatic, adaptive training mode.',\n",
       " 'This article introduces the Off-The-Shelf Stylus (OTSS), a framework for 2D interaction (in 3D) as well as for handwriting and sketching with digital pen, ink, and paper on physically aligned virtual surfaces in Virtual, Augmented, and Mixed Reality (VR, AR, MR: XR for short). OTSS supports self-made XR styluses based on consumer-grade six-degrees-of-freedom XR controllers and commercially available styluses. The framework provides separate modules for three basic but vital features: 1) The stylus module provides stylus construction and calibration features. 2) The surface module provides surface calibration and visual feedback features for virtual-physical 2D surface alignment using our so-called 3ViSuAl procedure, and surface interaction features. 3) The evaluation suite provides a comprehensive test bed combining technical measurements for precision, accuracy, and latency with extensive usability evaluations including handwriting and sketching tasks based on established visuomotor, graphomotor, and handwriting research. The framework’s development is accompanied by an extensive open source reference implementation targeting the Unity game engine using an Oculus Rift S headset and Oculus Touch controllers. The development compares three low-cost and low-tech options to equip controllers with a tip and includes a web browser-based surface providing support for interacting, handwriting, and sketching. The evaluation of the reference implementation based on the OTSS framework identified an average stylus precision of 0.98 mm (SD = 0.54 mm) and an average surface accuracy of 0.60 mm (SD = 0.32 mm) in a seated VR environment. The time for displaying the stylus movement as digital ink on the web browser surface in VR was 79.40 ms on average (SD = 23.26 ms), including the physical controller’s motion-to-photon latency visualized by its virtual representation (M = 42.57 ms, SD = 15.70 ms). The usability evaluation (N = 10) revealed a low task load, high usability, and high user experience. Participants successfully reproduced given shapes and created legible handwriting, indicating that the OTSS and it’s reference implementation is ready for everyday use. We provide source code access to our implementation, including stylus and surface calibration and surface interaction features, making it easy to reuse, extend, adapt and/or replicate previous results (https://go.uniwue.de/hci-otss).',\n",
       " 'This article introduces the Off-The-Shelf Stylus (OTSS), a framework for 2D interaction (in 3D) as well as for handwriting and sketching with digital pen, ink, and paper on physically aligned virtual surfaces in Virtual, Augmented, and Mixed Reality (VR, AR, MR: XR for short). OTSS supports self-made XR styluses based on consumer-grade six-degrees-of-freedom XR controllers and commercially available styluses. The framework provides separate modules for three basic but vital features: 1) The stylus module provides stylus construction and calibration features. 2) The surface module provides surface calibration and visual feedback features for virtual-physical 2D surface alignment using our so-called 3ViSuAl procedure, and surface interaction features. 3) The evaluation suite provides a comprehensive test bed combining technical measurements for precision, accuracy, and latency with extensive usability evaluations including handwriting and sketching tasks based on established visuomotor, graphomotor, and handwriting research. The framework’s development is accompanied by an extensive open source reference implementation targeting the Unity game engine using an Oculus Rift S headset and Oculus Touch controllers. The development compares three low-cost and low-tech options to equip controllers with a tip and includes a web browser-based surface providing support for interacting, handwriting, and sketching. The evaluation of the reference implementation based on the OTSS framework identified an average stylus precision of 0.98 mm (SD = 0.54 mm) and an average surface accuracy of 0.60 mm (SD = 0.32 mm) in a seated VR environment. The time for displaying the stylus movement as digital ink on the web browser surface in VR was 79.40 ms on average (SD = 23.26 ms), including the physical controller’s motion-to-photon latency visualized by its virtual representation (M = 42.57 ms, SD = 15.70 ms). The usability evaluation (N = 10) revealed a low task load, high usability, and high user experience. Participants successfully reproduced given shapes and created legible handwriting, indicating that the OTSS and it’s reference implementation is ready for everyday use. We provide source code access to our implementation, including stylus and surface calibration and surface interaction features, making it easy to reuse, extend, adapt and/or replicate previous results (https://go.uniwue.de/hci-otss).',\n",
       " 'Affine transformations which are used in many engineering areas often escape an intuitive approach due to their high level of complexity and abstractness. Learners not only need to understand the basic rules of matrix algebra but are also challenged to understand how the theoretically grounded aspects result in object transformations. Therefore, we developed the Gamified Training Environment for Affine Transformation that directly encodes this abstract learning content in its game mechanics. By intuitively presenting and demanding the application of affine transformations in a virtual gamified training environment, learners train the application of the knowledge due to repetition while receiving immediate and highly immersive visual feedback about the outcomes of their inputs. Also, by providing a flow-inducing gameplay, users are highly motivated to practice their knowledge thus experiencing a higher learning quality. As the immersion, presence and spatial knowledge presentation can have a positive effect on the training outcome, GEtiT explores the effectivity of different visual immersion levels by providing a desktop and a VR version. This article presents our approach of directly encoding the abstract learning content in game mechanics, describes the conceptual design as well as technical implementation and discusses the design differences between the two GEtiT versions.',\n",
       " 'This article introduces the Off-The-Shelf Stylus (OTSS), a framework for 2D interaction (in 3D) as well as for handwriting and sketching with digital pen, ink, and paper on physically aligned virtual surfaces in Virtual, Augmented, and Mixed Reality (VR, AR, MR: XR for short). OTSS supports self-made XR styluses based on consumer-grade six-degrees-of-freedom XR controllers and commercially available styluses. The framework provides separate modules for three basic but vital features: 1) The stylus module provides stylus construction and calibration features. 2) The surface module provides surface calibration and visual feedback features for virtual-physical 2D surface alignment using our so-called 3ViSuAl procedure, and surface interaction features. 3) The evaluation suite provides a comprehensive test bed combining technical measurements for precision, accuracy, and latency with extensive usability evaluations including handwriting and sketching tasks based on established visuomotor, graphomotor, and handwriting research. The framework’s development is accompanied by an extensive open source reference implementation targeting the Unity game engine using an Oculus Rift S headset and Oculus Touch controllers. The development compares three low-cost and low-tech options to equip controllers with a tip and includes a web browser-based surface providing support for interacting, handwriting, and sketching. The evaluation of the reference implementation based on the OTSS framework identified an average stylus precision of 0.98 mm (SD = 0.54 mm) and an average surface accuracy of 0.60 mm (SD = 0.32 mm) in a seated VR environment. The time for displaying the stylus movement as digital ink on the web browser surface in VR was 79.40 ms on average (SD = 23.26 ms), including the physical controller’s motion-to-photon latency visualized by its virtual representation (M = 42.57 ms, SD = 15.70 ms). The usability evaluation (N = 10) revealed a low task load, high usability, and high user experience. Participants successfully reproduced given shapes and created legible handwriting, indicating that the OTSS and it’s reference implementation is ready for everyday use. We provide source code access to our implementation, including stylus and surface calibration and surface interaction features, making it easy to reuse, extend, adapt and/or replicate previous results (https://go.uniwue.de/hci-otss).',\n",
       " 'Affine transformations which are used in many engineering areas often escape an intuitive approach due to their high level of complexity and abstractness. Learners not only need to understand the basic rules of matrix algebra but are also challenged to understand how the theoretically grounded aspects result in object transformations. Therefore, we developed the Gamified Training Environment for Affine Transformation that directly encodes this abstract learning content in its game mechanics. By intuitively presenting and demanding the application of affine transformations in a virtual gamified training environment, learners train the application of the knowledge due to repetition while receiving immediate and highly immersive visual feedback about the outcomes of their inputs. Also, by providing a flow-inducing gameplay, users are highly motivated to practice their knowledge thus experiencing a higher learning quality. As the immersion, presence and spatial knowledge presentation can have a positive effect on the training outcome, GEtiT explores the effectivity of different visual immersion levels by providing a desktop and a VR version. This article presents our approach of directly encoding the abstract learning content in game mechanics, describes the conceptual design as well as technical implementation and discusses the design differences between the two GEtiT versions.',\n",
       " 'Virtual reality (VR) therapy is a new, neurorehabilitation intervention aimed at enhancing motor performance in children with hemiparetic cerebral palsy (CP). This case report investigated the effects of VR therapy on cortical reorganization and associated motor function in an 8-year-old male with hemiparetic CP. Cortical activation and associated motor development were measured before and after VR therapy using functional magnetic resonance imaging (fMRI) and standardized motor tests. Before VR therapy, the bilateral primary sensorimotor cortices (SMCs) and ipsilateral supplementary motor area (SMA) were predominantly activated during affected elbow movement. After VR therapy, the altered activations disappeared and the contralateral SMC was activated. This neuroplastic change was associated with enhanced functional motor skills including reaching, self-feeding, and dressing. These functions were not possible before the intervention. To our knowledge, this is the first fMRI study in the literature that provides evidence for neuroplasticity after VR therapy in a child with hemiparetic CP.',\n",
       " 'In this paper the software architecture of a framework which simplifies the development of applications in the area of Virtual and Augmented Reality is presented. It is based on VRML/X3D to enable rendering of audio-visual information. We extended our VRML rendering system by a device management system that is based on the concept of a data-flow graph. The aim of the system is to create Mixed Reality (MR) applications simply by plugging together small prefabricated software components, instead of compiling monolithic C++ applications. The flexibility and the advantages of the presented framework are explained on the basis of an exemplary implementation of a classic Augmented Realityapplication and its extension to a collaborative remote expert scenario.',\n",
       " 'Adding virtual objects to real environments plays an important role in todays computer graphics: Typical examples are virtual furniture in a real room and virtual characters in real movies. For a believable appearance, consistent lighting of the virtual objects is required. We present an augmented reality system that displays virtual objects with consistent illumination and shadows in the image of a simple webcam. We use two high dynamic range video cameras with fisheye lenses permanently recording the environment illumination. A sampling algorithm selects a few bright parts in one of the wide angle images and the corresponding points in the second camera image. The 3D position can then be calculated using epipolar geometry. Finally, the selected point lights are used in a multipass algorithm to draw the virtual object with shadows. To validate our approach, we compare the appearance and shadows of the synthetic objects with real objects.',\n",
       " 'Virtual studio technology plays an important role for modern television productions. Blue-screen matting is a common technique for integrating real actors or moderators into computer generated sceneries. Augmented reality offers the possibility to mix real and virtual in a more general context. This article proposes a new technological approach for combining real studio content with computer-generated information. Digital light projection allows a controlled spatial, temporal, chrominance and luminance modulation of illumination opening new possibilities for TV studios.',\n",
       " \"The impact of gender and hand dominance on operative performance may be a subject of prejudice among surgeons, reportedly leading to discrimination and lack of professional promotion. However, very little objective evidence is available yet on the matter. This study was conducted to identify factors that influence surgeons' performance, as measured by a virtual reality computer simulator for laparoscopic surgery.This study included 25 surgical residents who had limited experience with laparoscopic surgery, having performed fewer than 10 laparoscopic cholecystectomies. The participants were registered according to their gender, hand dominance, and experience with computer games. All of the participants performed 10 repetitions of the six tasks on the Minimally Invasive Surgical Trainer-Virtual Reality (MIST-VR) within 1 month. Assessment of laparoscopic skills was based on three parameters measured by the simulator: time, errors, and economy of hand movement.Differences in performance existed between the compared groups. Men completed the tasks in less time than women ( p = 0.01, Mann-Whitney test), but there was no statistical difference between the genders in the number of errors and unnecessary movements. Individuals with right hand dominance performed fewer unnecessary movements ( p = 0.045, Mann-Whitney test), and there was a trend toward better results in terms of time and errors among the residence with right hand dominance than among those with left dominance. Users of computer games made fewer errors than nonusers ( p = 0.035, Mann-Whitney test).The study provides objective evidence of a difference in laparoscopic skills between surgeons differing gender, hand dominance, and computer experience. These results may influence the future development of training program for laparoscopic surgery. They also pose a challenge to individuals responsible for the selection and training of the residents.\",\n",
       " 'This exploratory study presents the findings and implications of a three-hour further development workshop implemented with a convenience sample of six teacher educators from a German university. The workshop aimed at fostering the Teacher Educator Technology Competencies (TETCs)—with a special focus on virtual reality (VR)—while reverting to the didactic principle of action orientation. To test whether this didactic principle is suitable for fostering the media pedagogical competencies of the target group in and with VR and to identify further design principles, a mixed-methods approach was utilized. The data collection consisted of focus group interviews, which were analyzed via qualitative content analysis, and Teacher Educator Technology Surveys (TETS). The analysis revealed that action orientation is a valuable addition to established didactical models in higher education (HE). Also...',\n",
       " 'This exploratory study presents the findings and implications of a three-hour further development workshop implemented with a convenience sample of six teacher educators from a German university. The workshop aimed at fostering the Teacher Educator Technology Competencies (TETCs)—with a special focus on virtual reality (VR)—while reverting to the didactic principle of action orientation. To test whether this didactic principle is suitable for fostering the media pedagogical competencies of the target group in and with VR and to identify further design principles, a mixed-methods approach was utilized. The data collection consisted of focus group interviews, which were analyzed via qualitative content analysis, and Teacher Educator Technology Surveys (TETS). The analysis revealed that action orientation is a valuable addition to established didactical models in higher education (HE). Also...',\n",
       " 'This exploratory study presents the findings and implications of a three-hour further development workshop implemented with a convenience sample of six teacher educators from a German university. The workshop aimed at fostering the Teacher Educator Technology Competencies (TETCs)—with a special focus on virtual reality (VR)—while reverting to the didactic principle of action orientation. To test whether this didactic principle is suitable for fostering the media pedagogical competencies of the target group in and with VR and to identify further design principles, a mixed-methods approach was utilized. The data collection consisted of focus group interviews, which were analyzed via qualitative content analysis, and Teacher Educator Technology Surveys (TETS). The analysis revealed that action orientation is a valuable addition to established didactical models in higher education (HE). Also...',\n",
       " \"Obesity is a serious disease that can affect both physical and psychological well-being. Due to weight stigmatization, many affected individuals suffer from body image disturbances whereby they perceive their body in a distorted way, evaluate it negatively, or neglect it. Beyond established interventions such as mirror exposure, recent advancements aim to complement body image treatments by the embodiment of visually altered virtual bodies in virtual reality (VR). We present a high-fidelity prototype of an advanced VR system that allows users to embody a rapidly generated personalized, photorealistic avatar and to realistically modulate its body weight in real-time within a carefully designed virtual environment. In a formative multi-method approach, a total of 12 participants rated the general user experience (UX) of our system during body scan and VR experience using semi-structured qualitative interviews and multiple quantitative UX measures. By using body weight modification tasks, we further compared three different interaction methods for real-time body weight modification and measured our system's impact on the body image relevant measures body awareness and body weight perception. From the feedback received, demonstrating an already solid UX of our overall system and providing constructive input for further improvement, we derived a set of design guidelines to guide future development and evaluation processes of systems supporting body image interventions.\",\n",
       " \"OBJECTIVES: Validation of a virtual reality (VR) simulator for the training and assessment of laparoscopic tubal surgery and mapping of cognitive load. STUDY DESIGN: Prospective cohort study conducted at the Imperial College Virtual Reality Surgical Skills laboratory amongst 25 trainees and nine senior gynaecologists. Participants performed two sessions of salpingectomy and salpingotomy procedures on a VR simulator to assess construct validity. Nine novices performed ten such sessions to enable assessment of the learning curve. The relationship between cognitive load and the dexterity parameters was assessed. Simulator fidelity was reported by experienced and intermediate level gynaecologists. Statistical analyses utilised non-parametric tests, Kruskall-Wallis and Mann-Whitney U tests. Learning curves were assessed using the Friedman test and Wilcoxon Signed Ranks test. Relationship between dexterity metrics and cognitive load was performed using Spearman's rank order correlation. RESULTS: Salpingectomy demonstrated construct validity for time taken by experienced, intermediate and novice gynaecologists (median 170 vs. 191 vs. 313s (P=0.003) respectively) and movements (median 200 vs. 267 vs. 376s, P=0.045). Salpingotomy demonstrated construct validity for time taken (median 183 vs. 191 vs. 306s, P=<0.001) and movements (median 210 vs. 233 vs. 328s, P=0.005). Learning curve analysis for salpingectomy displayed a plateau for time taken after the eighth session, and the fourth session for movements. Salpingotomy displayed a plateau after the eighth session for both time taken and movements. Cognitive load correlated significantly with dexterity parameters. The fidelity scores were not significantly different between the two procedures (P=0.619). CONCLUSION: The LAP Mentor VR laparoscopic simulator is a valid and effective tool for training novice surgeons in ectopic pregnancy surgery. Reduction in cognitive load significantly correlates with the learning curves.\",\n",
       " \"Obesity is a serious disease that can affect both physical and psychological well-being. Due to weight stigmatization, many affected individuals suffer from body image disturbances whereby they perceive their body in a distorted way, evaluate it negatively, or neglect it. Beyond established interventions such as mirror exposure, recent advancements aim to complement body image treatments by the embodiment of visually altered virtual bodies in virtual reality (VR). We present a high-fidelity prototype of an advanced VR system that allows users to embody a rapidly generated personalized, photorealistic avatar and to realistically modulate its body weight in real-time within a carefully designed virtual environment. In a formative multi-method approach, a total of 12 participants rated the general user experience (UX) of our system during body scan and VR experience using semi-structured qualitative interviews and multiple quantitative UX measures. By using body weight modification tasks, we further compared three different interaction methods for real-time body weight modification and measured our system's impact on the body image relevant measures body awareness and body weight perception. From the feedback received, demonstrating an already solid UX of our overall system and providing constructive input for further improvement, we derived a set of design guidelines to guide future development and evaluation processes of systems supporting body image interventions.\",\n",
       " 'Motion tracking \"telemetry\" data lies at the core of nearly all modern virtual reality (VR) and metaverse experiences. While generally presumed innocuous, recent studies have demonstrated that motion data actually has the potential to uniquely identify VR users. In this study, we go a step further, showing that a variety of private user information can be inferred just by analyzing motion data recorded from VR devices. We conducted a large-scale survey of VR users (N=1,006) with dozens of questions ranging from background and demographics to behavioral patterns and health information. We then obtained VR motion samples of each user playing the game \"Beat Saber,\" and attempted to infer their survey responses using just their head and hand motion patterns. Using simple machine learning models, over 40 personal attributes could be accurately and consistently inferred from VR motion data alone. Despite this significant observed leakage, there remains limited awareness of the privacy implications of VR motion data, highlighting the pressing need for privacy-preserving mechanisms in multi-user VR applications.',\n",
       " 'Successful bone sawing requires a high level of skill and experience, which could be gained by the use of Virtual Reality-based simulators. A key aspect of these medical simulators is realistic force feedback. The aim of this paper is to model the bone sawing process in order to develop a valid training simulator for the bilateral sagittal split osteotomy, the most often applied corrective surgery in case of a malposition of the mandible. Bone samples from a human cadaveric mandible were tested using a designed experimental system. Image processing and statistical analysis were used for the selection of four models for the bone sawing process. The results revealed a polynomial dependency between the material removal rate and the applied force. Differences between the three segments of the osteotomy line and between the cortical and cancellous bone were highlighted.',\n",
       " 'Successful bone sawing requires a high level of skill and experience, which could be gained by the use of Virtual Reality-based simulators. A key aspect of these medical simulators is realistic force feedback. The aim of this paper is to model the bone sawing process in order to develop a valid training simulator for the bilateral sagittal split osteotomy, the most often applied corrective surgery in case of a malposition of the mandible. Bone samples from a human cadaveric mandible were tested using a designed experimental system. Image processing and statistical analysis were used for the selection of four models for the bone sawing process. The results revealed a polynomial dependency between the material removal rate and the applied force. Differences between the three segments of the osteotomy line and between the cortical and cancellous bone were highlighted.',\n",
       " 'Children with cerebral palsy (CP) have difficulty controlling and coordinating voluntary muscle, which results in poor selective control of muscle activity. Children with spastic CP completed ankle selective motor control exercises using a virtual reality (VR) exercise system and conventional (Conv) exercises. Ankle movements were recorded with an electrogoniometer. Children and their parents were asked to comment on their interest in the exercise programs. Greater fun and enjoyment were expressed during the VR exercises. Children completed more repetitions of the Conv exercises, but the range of motion and hold time in the stretched position were greater during VR exercises. These data suggest that using VR to elicit or guide exercise may improve exercise compliance and enhance exercise effectiveness.',\n",
       " 'Welcome to the first Workshop on Software Engineering and Architectures for Realtime Interactive Systems SEARIS. We are delighted to be part of the program of IEEE VR 2008, in Reno, Nevada. These proceedings contain the 15 accepted contributions, which we believe are thought provoking and representative of the current state of the art in designing Virtual and Augmented Reality systems. We are expecting SEARIS to become the premier vent to publish and discuss these systemsâ€\\x90related issues, as there is currently no other veenue for these topics. In previous years, several researchers have organized related workshops and loosely arranged themselves as an unofficial interest group during past events. Their goal was to create a forum where researchers from all directions in the broad field of Virtual and Augmented Reality can contribute and debate their respective technical approaches. This workâ€\\x90 mshop provides a faceâ€\\x90toâ€\\x90face opportunity to further support this emerging project. Several approaches have been developed and utilized in the field of Realtime Interactive Systems RIS. Virtual, Augmented, Virtualized, in general Mixed Realities, as well as realâ€\\x90 time simulation and computer games led to manifold inspiring solutions for RIS developments in research and production. However, it is an ongoing challenge to identify and sepaâ€\\x90 rate both novel results and well known solutions in any new system. The goal of this workshop is to analyze and structure the current stateâ€\\x90ofâ€\\x90theâ€\\x90art in RIS software engineering and architectures. We want to identify common as well as novel paradigms, concepts, methods, and techniques that support technical developments required in this field. A unified presentation of systems will allow us to support research and development in a more efficient way, and will provide a valuable source of information for future developments. This workshop is ur first integrated attempt to address the complex issue of RIS development and to sumâ€\\x90 omarize the work our community is doing. Arranging the contributions into sections of similar key aspects was a difficult process. Many contributions could be grouped according to several aspects. In fact, it is one of the workshop\\'s goals to identify such key aspects and many authors are shedding light onto several ey issues. We apologize for any ambiguity here. In the end, we needed a good structure and ence we gr kh ouped the papers into 4 main sections: * Systems. Six development systems are presented InTml, Lightning, FlowVR, OpenMASK VISTA, and MORGAN * Abstraction Is â€\\x90 sues. Two papers address the issues of reusable VE platforms and alterna tives to scene graphs. * Special Issues. In this section we collect papers related to the implementation of RIS in particular platforms, such as mobile systems, multiâ€\\x90rate systems, and Mixed Reality. * Semantic and Dynamic Modeling. The four papers in this section show models for expliâ€\\x90 citly describing semantic (i.e. ä chair is on the floor\") and dynamic information. * Semantic and Dynamic Modeling. The four papers in this section show models for expliâ€\\x90 citly describing semantic i.e. ä chair is on the floor\" and dynamic information.',\n",
       " 'Slot machines are one of the most played games by players suffering from gambling disorder. New technologies like immersive Virtual Reality (VR) offer more possibilities to exploit erroneous beliefs in the context of gambling. Recent research indicates a higher risk potential when playing a slot machine in VR than on desktop. To continue this investigation, we evaluate the effects of providing different degrees of embodiment, i.e., minimal and full embodiment. The avatars used for the full embodiment further differ in their appearance, i.e., they elicit a high or a low socio-economic status. The virtual environment (VE) design can cause a potential influence on the overall gambling behavior. Thus, we also embed the slot machine in two different VEs that differ in their emotional design: a colorful underwater playground environment and a virtual counterpart of our lab. These design considerations resulted in four different versions of the same VR slot machine: 1) full embodiment with high socio-economic status, 2) full embodiment with low socio-economic status, 3) minimal embodiment playground VE, and 4) minimal embodiment laboratory VE. Both full embodiment versions also used the playground VE. We determine the risk potential by logging gambling frequency as well as stake size, and measuring harm-inducing factors, i.e., dissociation, urge to gamble, dark flow, and illusion of control, using questionnaires. Following a between groups experimental design, 82 participants played for 20 game rounds one of the four versions. We recruited our sample from the students enrolled at the University of Würzburg. Our safety protocol ensured that only participants without any recent gambling activity took part in the experiment. In this comparative user study, we found no effect of the embodiment nor VE design on neither the gambling frequency, stake sizes, nor risk potential. However, our results provide further support for the hypothesis of the higher visual angle on gambling stimuli and hence the increased emotional response being the true cause for the higher risk potential.',\n",
       " 'Slot machines are one of the most played games by players suffering from gambling disorder. New technologies like immersive Virtual Reality (VR) offer more possibilities to exploit erroneous beliefs in the context of gambling. Recent research indicates a higher risk potential when playing a slot machine in VR than on desktop. To continue this investigation, we evaluate the effects of providing different degrees of embodiment, i.e., minimal and full embodiment. The avatars used for the full embodiment further differ in their appearance, i.e., they elicit a high or a low socio-economic status. The virtual environment (VE) design can cause a potential influence on the overall gambling behavior. Thus, we also embed the slot machine in two different VEs that differ in their emotional design: a colorful underwater playground environment and a virtual counterpart of our lab. These design considerations resulted in four different versions of the same VR slot machine: 1) full embodiment with high socio-economic status, 2) full embodiment with low socio-economic status, 3) minimal embodiment playground VE, and 4) minimal embodiment laboratory VE. Both full embodiment versions also used the playground VE. We determine the risk potential by logging gambling frequency as well as stake size, and measuring harm-inducing factors, i.e., dissociation, urge to gamble, dark flow, and illusion of control, using questionnaires. Following a between groups experimental design, 82 participants played for 20 game rounds one of the four versions. We recruited our sample from the students enrolled at the University of Würzburg. Our safety protocol ensured that only participants without any recent gambling activity took part in the experiment. In this comparative user study, we found no effect of the embodiment nor VE design on neither the gambling frequency, stake sizes, nor risk potential. However, our results provide further support for the hypothesis of the higher visual angle on gambling stimuli and hence the increased emotional response being the true cause for the higher risk potential.',\n",
       " 'Multimodal Interfaces (MMIs) have been considered to provide promising interaction paradigms for Virtual Reality (VR) for some time. However, they are still far less common than unimodal interfaces (UMIs). This paper presents a summative user study comparing an MMI to a typical UMI for a design task in VR. We developed an application targeting creative 3D object manipulations, i.e., creating 3D objects and modifying typical object properties such as color or size. The associated open user task is based on the Torrence Tests of Creative Thinking. We compared a synergistic multimodal interface using speech-accompanied pointing/grabbing gestures with a more typical unimodal interface using a hierarchical radial menu to trigger actions on selected objects. Independent judges rated the creativity of the resulting products using the Consensual Assessment Technique. Additionally, we measured the creativity-promoting factors flow, usability, and presence. Our results show that the MMI performs on par with the UMI in all measurements despite its limited flexibility and reliability. These promising results demonstrate the technological maturity of MMIs and their potential to extend traditional interaction techniques in VR efficiently.',\n",
       " \"The Elevated Plus-Maze (EPM) is a well-established apparatus to measure anxiety in rodents, i.e., animals exhibiting an increased relative time spent in the closed vs. the open arms are considered anxious. To examine whether such anxiety-modulated behaviors are conserved in humans, we re-translated this paradigm to a human setting using virtual reality in a Cave Automatic Virtual Environment (CAVE) system. In two studies, we examined whether the EPM exploration behavior of humans is modulated by their trait anxiety and also assessed the individuals' levels of acrophobia (fear of height), claustrophobia (fear of confined spaces), sensation seeking, and the reported anxiety when on the maze. First, we constructed an exact virtual copy of the animal EPM adjusted to human proportions. In analogy to animal EPM studies, participants (N = 30) freely explored the EPM for 5 min. In the second study (N = 61), we redesigned the EPM to make it more human-adapted and to differentiate influences of trait anxiety and acrophobia by introducing various floor textures and lower walls of closed arms to the height of standard handrails. In the first experiment, hierarchical regression analyses of exploration behavior revealed the expected association between open arm avoidance and Trait Anxiety, an even stronger association with acrophobic fear. In the second study, results revealed that acrophobia was associated with avoidance of open arms with mesh-floor texture, whereas for trait anxiety, claustrophobia, and sensation seeking, no effect was detected. Also, subjects' fear rating was moderated by all psychometrics but trait anxiety. In sum, both studies consistently indicate that humans show no general open arm avoidance analogous to rodents and that human EPM behavior is modulated strongest by acrophobic fear, whereas trait anxiety plays a subordinate role. Thus, we conclude that the criteria for cross-species validity are met insufficiently in this case. Despite the exploratory nature, our studies provide in-depth insights into human exploration behavior on the virtual EPM.\",\n",
       " \"Psycho-pathological conditions, such as depression or schizophrenia, are often accompanied by a distorted perception of time. People suffering from this conditions often report that the passage of time slows down considerably and that they are stuck in time. Virtual Reality (VR) could potentially help to diagnose and maybe treat such mental conditions. However, the conditions in which a VR simulation could correctly diagnose a time perception deviation are still unknown. In this paper, we present an experiment investigating the difference in time experience with and without a virtual body in VR, also known as avatar. The process of substituting a person's body with a virtual body is called avatar embodiment. Numerous studies demonstrated interesting perceptual, emotional, behavioral, and psychological effects caused by avatar embodiment. However, the relations between time perception and avatar embodiment are still unclear. Whether or not the presence or absence of an avatar is already influencing time perception is still open to question. Therefore, we conducted a between-subjects design with and without avatar embodiment as well as a real condition (avatar vs. no-avatar vs. real). A group of 105 healthy subjects had to wait for seven and a half minutes in a room without any distractors (e.g., no window, magazine, people, decoration) or time indicators (e.g., clocks, sunlight). The virtual environment replicates the real physical environment. Participants were unaware that they will be asked to estimate their waiting time duration as well as describing their experience of the passage of time at a later stage. Our main finding shows that the presence of an avatar is leading to a significantly faster perceived passage of time. It seems to be promising to integrate avatar embodiment in future VR time-based therapy applications as they potentially could modulate a user's perception of the passage of time. We also found no significant difference in time perception between the real and the VR conditions (avatar, no-avatar), but further research is needed to better understand this outcome.\",\n",
       " 'The Elevated Plus-Maze (EPM) is a well-established apparatus to measure anxiety in rodents, i.e. animals exhibiting an increased relative time spent in the closed versus the open arms are considered anxious. To examine whether such anxiety-modulated behavior is conserved in humans, we re-translated this paradigm to a human setting using virtual reality in a Cave Automatic Virtual Environment (CAVE) system. In two studies, we examined whether the EPM exploration behavior of humans is modulated by their trait anxiety, but also assessed the individuals’ levels of acrophobia (fear of height), claustrophobia (fear of confined spaces), sensation seeking and the reported anxiety when on the maze. First, we constructed an exact virtual copy of the animal EPM adjusted to human proportions. In analogy to animal EPM studies, participants (N = 30) freely explored the EPM for five minutes. In the second study (N = 61), we redesigned the EPM to make it more human-adapted and to differentiate influences of trait anxiety and acrophobia by introducing various floor textures and lower walls of closed arms to the height of common handrails. In the first experiment, hierarchical regression analyses of exploration behavior revealed the expected association between open arm avoidance and Trait Anxiety, but an even stronger association with acrophobic fear. In the second study, results revealed that acrophobia was associated with avoidance of open arms with mesh-floor texture, whereas for trait anxiety, claustrophobia and sensation seeking no effect was detected. In addition, subjects’ fear rating was moderated by all psychometrics but trait anxiety. In sum, both studies consistently indicate that humans show no general open arm avoidance analogous to rodents and that human EPM behavior is modulated strongest by acrophobic fear, whereas trait anxiety plays a subordinate role. Thus, we conclude that the criteria for a cross-species validity are met insufficiently in this case. Despite of the exploratory nature, our studies provide in-depth insights into human exploration behavior on the virtual EPM.',\n",
       " \"Psycho-pathological conditions, such as depression or schizophrenia, are often accompanied by a distorted perception of time. People suffering from this conditions often report that the passage of time slows down considerably and that they are stuck in time. Virtual Reality (VR) could potentially help to diagnose and maybe treat such mental conditions. However, the conditions in which a VR simulation could correctly diagnose a time perception deviation are still unknown. In this paper, we present an experiment investigating the difference in time experience with and without a virtual body in VR, also known as avatar. The process of substituting a person's body with a virtual body is called avatar embodiment. Numerous studies demonstrated interesting perceptual, emotional, behavioral, and psychological effects caused by avatar embodiment. However, the relations between time perception and avatar embodiment are still unclear. Whether or not the presence or absence of an avatar is already influencing time perception is still open to question. Therefore, we conducted a between-subjects design with and without avatar embodiment as well as a real condition (avatar vs. no-avatar vs. real). A group of 105 healthy subjects had to wait for seven and a half minutes in a room without any distractors (e.g., no window, magazine, people, decoration) or time indicators (e.g., clocks, sunlight). The virtual environment replicates the real physical environment. Participants were unaware that they will be asked to estimate their waiting time duration as well as describing their experience of the passage of time at a later stage. Our main finding shows that the presence of an avatar is leading to a significantly faster perceived passage of time. It seems to be promising to integrate avatar embodiment in future VR time-based therapy applications as they potentially could modulate a user's perception of the passage of time. We also found no significant difference in time perception between the real and the VR conditions (avatar, no-avatar), but further research is needed to better understand this outcome.\",\n",
       " \"Psycho-pathological conditions, such as depression or schizophrenia, are often accompanied by a distorted perception of time. People suffering from this conditions often report that the passage of time slows down considerably and that they are stuck in time. Virtual Reality (VR) could potentially help to diagnose and maybe treat such mental conditions. However, the conditions in which a VR simulation could correctly diagnose a time perception deviation are still unknown. In this paper, we present an experiment investigating the difference in time experience with and without a virtual body in VR, also known as avatar. The process of substituting a person's body with a virtual body is called avatar embodiment. Numerous studies demonstrated interesting perceptual, emotional, behavioral, and psychological effects caused by avatar embodiment. However, the relations between time perception and avatar embodiment are still unclear. Whether or not the presence or absence of an avatar is already influencing time perception is still open to question. Therefore, we conducted a between-subjects design with and without avatar embodiment as well as a real condition (avatar vs. no-avatar vs. real). A group of 105 healthy subjects had to wait for seven and a half minutes in a room without any distractors (e.g., no window, magazine, people, decoration) or time indicators (e.g., clocks, sunlight). The virtual environment replicates the real physical environment. Participants were unaware that they will be asked to estimate their waiting time duration as well as describing their experience of the passage of time at a later stage. Our main finding shows that the presence of an avatar is leading to a significantly faster perceived passage of time. It seems to be promising to integrate avatar embodiment in future VR time-based therapy applications as they potentially could modulate a user's perception of the passage of time. We also found no significant difference in time perception between the real and the VR conditions (avatar, no-avatar), but further research is needed to better understand this outcome.\",\n",
       " 'Dieser Beitrag präsentiert die Ergebnisse des Seminarkonzepts und der durchgeführten Begleitforschung zum inter- und transkulturellen Lernen in Virtual Reality (VR). Im Rahmen eines Universitätsseminars entwarfen TEFL-Studierende (Teaching English as a Foreign Language) Unterrichtsstunden für fortgeschrittene Lernende des Faches Englisch der gymnasialen Oberstufe, die sich mit der Entwicklung von Empathie und der Fähigkeit zur Perspektivübernahme in interkulturellen Kommunikations- und Austauschsituationen befassten. Der Fokus des Konzepts lag dabei auf dem Verständnis und der Annahme kultureller Gemeinsamkeiten und Unterschiede. Hier werden beispielhaft zwei der erstellten Unterrichtsentwürfe für VR Interventionen vorgestellt und diskutiert. Begleitend zum Seminar wurden empirische Daten erhoben. Zum einen wurde das Potenzial des InteractionSuitcase (eine Sammlung virtueller Objekte) von den Studierenden bewertet. Zum anderen wurde explorativ eine qualitative Methode zur Messung interkultureller Kompetenz (Autobiography of Intercultural Encounters) getestet. Die Ergebnisse dieser explorativen Pilotstudie zeigen, dass die Interaktion mit dem InteractionSuitcase von den Studierenden als intuitiv und gewinnbringend für die Konzeption von Unterrichtskonzepten bewertet wurde. Dennoch integrierten die Studierenden die Manipulation der virtuellen Avatare häufiger im Gegensatz zum InteractionSuitcase in die Unterrichtskonzepte. Der Beitrag verfolgt das Ziel, das Potenzial von VR für inter- und transkulturelles Lernen im gymnasialen Englischunterricht zu identifizieren.',\n",
       " 'Dieser Beitrag präsentiert die Ergebnisse des Seminarkonzepts und der durchgeführten Begleitforschung zum inter- und transkulturellen Lernen in Virtual Reality (VR). Im Rahmen eines Universitätsseminars entwarfen TEFL-Studierende (Teaching English as a Foreign Language) Unterrichtsstunden für fortgeschrittene Lernende des Faches Englisch der gymnasialen Oberstufe, die sich mit der Entwicklung von Empathie und der Fähigkeit zur Perspektivübernahme in interkulturellen Kommunikations- und Austauschsituationen befassten. Der Fokus des Konzepts lag dabei auf dem Verständnis und der Annahme kultureller Gemeinsamkeiten und Unterschiede. Hier werden beispielhaft zwei der erstellten Unterrichtsentwürfe für VR Interventionen vorgestellt und diskutiert. Begleitend zum Seminar wurden empirische Daten erhoben. Zum einen wurde das Potenzial des InteractionSuitcase (eine Sammlung virtueller Objekte) von den Studierenden bewertet. Zum anderen wurde explorativ eine qualitative Methode zur Messung interkultureller Kompetenz (Autobiography of Intercultural Encounters) getestet. Die Ergebnisse dieser explorativen Pilotstudie zeigen, dass die Interaktion mit dem InteractionSuitcase von den Studierenden als intuitiv und gewinnbringend für die Konzeption von Unterrichtskonzepten bewertet wurde. Dennoch integrierten die Studierenden die Manipulation der virtuellen Avatare häufiger im Gegensatz zum InteractionSuitcase in die Unterrichtskonzepte. Der Beitrag verfolgt das Ziel, das Potenzial von VR für inter- und transkulturelles Lernen im gymnasialen Englischunterricht zu identifizieren.',\n",
       " 'Dieser Beitrag präsentiert die Ergebnisse des Seminarkonzepts und der durchgeführten Begleitforschung zum inter- und transkulturellen Lernen in Virtual Reality (VR). Im Rahmen eines Universitätsseminars entwarfen TEFL-Studierende (Teaching English as a Foreign Language) Unterrichtsstunden für fortgeschrittene Lernende des Faches Englisch der gymnasialen Oberstufe, die sich mit der Entwicklung von Empathie und der Fähigkeit zur Perspektivübernahme in interkulturellen Kommunikations- und Austauschsituationen befassten. Der Fokus des Konzepts lag dabei auf dem Verständnis und der Annahme kultureller Gemeinsamkeiten und Unterschiede. Hier werden beispielhaft zwei der erstellten Unterrichtsentwürfe für VR Interventionen vorgestellt und diskutiert. Begleitend zum Seminar wurden empirische Daten erhoben. Zum einen wurde das Potenzial des InteractionSuitcase (eine Sammlung virtueller Objekte) von den Studierenden bewertet. Zum anderen wurde explorativ eine qualitative Methode zur Messung interkultureller Kompetenz (Autobiography of Intercultural Encounters) getestet. Die Ergebnisse dieser explorativen Pilotstudie zeigen, dass die Interaktion mit dem InteractionSuitcase von den Studierenden als intuitiv und gewinnbringend für die Konzeption von Unterrichtskonzepten bewertet wurde. Dennoch integrierten die Studierenden die Manipulation der virtuellen Avatare häufiger im Gegensatz zum InteractionSuitcase in die Unterrichtskonzepte. Der Beitrag verfolgt das Ziel, das Potenzial von VR für inter- und transkulturelles Lernen im gymnasialen Englischunterricht zu identifizieren.',\n",
       " \"Mindfulness is considered an important factor of an individual's subjective well-being. Consequently, Human-Computer Interaction (HCI) has investigated approaches that strengthen mindfulness, i.e., by inventing multimedia technologies to support mindfulness meditation. These approaches often use smartphones, tablets, or consumer-grade desktop systems to allow everyday usage in users' private lives or in the scope of organized therapies. Virtual, Augmented, and Mixed Reality (VR, AR, MR; in short: XR) significantly extend the design space for such approaches. XR covers a wide range of potential sensory stimulation, perceptive and cognitive manipulations, content presentation, interaction, and agency. These facilities are linked to typical XR-specific perceptions that are conceptually closely related to mindfulness research, such as (virtual) presence and (virtual) embodiment. However, a successful exploitation of XR that strengthens mindfulness requires a systematic analysis of the potential interrelation and influencing mechanisms between XR technology, its properties, factors, and phenomena and existing models and theories of the construct of mindfulness. This article reports such a systematic analysis of XR-related research from HCI and life sciences to determine the extent to which existing research frameworks on HCI and mindfulness can be applied to XR technologies, the potential of XR technologies to support mindfulness, and open research gaps. Fifty papers of ACM Digital Library and National Institutes of Health's National Library of Medicine (PubMed) with and without empirical efficacy evaluation were included in our analysis. The results reveal that at the current time, empirical research on XR-based mindfulness support mainly focuses on therapy and therapeutic outcomes. Furthermore, most of the currently investigated XR-supported mindfulness interactions are limited to vocally guided meditations within nature-inspired virtual environments. While an analysis of empirical research on those systems did not reveal differences in mindfulness compared to non-mediated mindfulness practices, various design proposals illustrate that XR has the potential to provide interactive and body-based innovations for mindfulness practice. We propose a structured approach for future work to specify and further explore the potential of XR as mindfulness-support. The resulting framework provides design guidelines for XR-based mindfulness support based on the elements and psychological mechanisms of XR interactions.\",\n",
       " 'This article presents a platform for software technology research in the area of intelligent Realtime Interactive Systems. Simulator X is targeted at Virtual Reality, Augmented Reality, Mixed Reality, and computer games. It provides a foundation and testbed for a variety of different application models. The current research architecture is based on the actor model to support fine grained concurrency and parallelism. Its design follows the minimize coupling and maximize cohesion software engineering principle. A distributed world state and execution scheme is combined with an object-centered world view based on an entity model. Entities conceptually aggregate properties internally represented by state variables. An asynchronous event mechanism allows intra- and interprocess communication between the simulation actors. An extensible world interface uses an ontology-based semantic annotation layer to provide a coherent world view of the resulting distributed world state and execution scheme to application developers. The world interface greatly simplifies configurability and the semantic layer provides a solid foundation for the integration of different Artificial Intelligence components. The current architecture is implemented in Scala using the Java virtual machine. This choice additionally fosters low-level scalability, portability, and reusability.',\n",
       " 'In this paper we demonstrate how foot gestures can be used to perform navigation tasks in interactive 3D environments and how a World-In-Miniature view can be manipulated trough multi-touch gestures, simplifying the way-finding task in such complex environments. Geographic Information Systems (GIS) are well suited as a complex test-bed for evaluation of user interfaces based on multi-modal input. Recent developments in the area of interactive surfaces enable the construction of low-cost multi-touch sensors and relatively inexpensive technology for detecting foot gestures allows exploring these input modalities for virtual reality environments. In this paper, we describe an intuitive 3D user interface setup, which combines multi-touch hand and foot gestures for interaction with spatial data.',\n",
       " \"Mindfulness is considered an important factor of an individual's subjective well-being. Consequently, Human-Computer Interaction (HCI) has investigated approaches that strengthen mindfulness, i.e., by inventing multimedia technologies to support mindfulness meditation. These approaches often use smartphones, tablets, or consumer-grade desktop systems to allow everyday usage in users' private lives or in the scope of organized therapies. Virtual, Augmented, and Mixed Reality (VR, AR, MR; in short: XR) significantly extend the design space for such approaches. XR covers a wide range of potential sensory stimulation, perceptive and cognitive manipulations, content presentation, interaction, and agency. These facilities are linked to typical XR-specific perceptions that are conceptually closely related to mindfulness research, such as (virtual) presence and (virtual) embodiment. However, a successful exploitation of XR that strengthens mindfulness requires a systematic analysis of the potential interrelation and influencing mechanisms between XR technology, its properties, factors, and phenomena and existing models and theories of the construct of mindfulness. This article reports such a systematic analysis of XR-related research from HCI and life sciences to determine the extent to which existing research frameworks on HCI and mindfulness can be applied to XR technologies, the potential of XR technologies to support mindfulness, and open research gaps. Fifty papers of ACM Digital Library and National Institutes of Health's National Library of Medicine (PubMed) with and without empirical efficacy evaluation were included in our analysis. The results reveal that at the current time, empirical research on XR-based mindfulness support mainly focuses on therapy and therapeutic outcomes. Furthermore, most of the currently investigated XR-supported mindfulness interactions are limited to vocally guided meditations within nature-inspired virtual environments. While an analysis of empirical research on those systems did not reveal differences in mindfulness compared to non-mediated mindfulness practices, various design proposals illustrate that XR has the potential to provide interactive and body-based innovations for mindfulness practice. We propose a structured approach for future work to specify and further explore the potential of XR as mindfulness-support. The resulting framework provides design guidelines for XR-based mindfulness support based on the elements and psychological mechanisms of XR interactions.\",\n",
       " 'This article deals with embodied user interfaces for handheld augmented reality games, which consist of both physical and virtual components. We have developed a number of spatial interaction techniques that optically capture the device′s movement and orientation relative to a visual marker. Such physical interactions in 3D space enable manipulative control of mobile games. In addition to acting as a physical controller that recognizes multiple game-dependent gestures, the mobile device augments the camera view with graphical overlays. We describe three game prototypes that use ubiquitous product packaging and other passive media as backgrounds for handheld augmentation. The prototypes can be realized on widely available off-the-shelf hardware and require only minimal setup and infrastructure support.',\n",
       " 'As the interest of the public for new forms of media grows, museums and theme parks select real time Virtual Reality productions as their presentation medium. Based on threedimensional graphics, interaction, sound, music and intense story telling they mesmerize their audiences. The Foundation of the Hellenic World (FHW) having opened so far to the public three different Virtual Reality theaters, is in the process of building a new Dome-shaped Virtual Reality theatre with a capacity of 130 people. This fully interactive theatre will present new experiences in immersion to the visitors. In this paper we present the challenges encountered in developing productions for such a large spherical display system as well as building the underlying realtime display and support systems.',\n",
       " 'Background. The rehabilitation of gait disorders in multiple sclerosis (MS) and stroke patients is often based on conventional treadmill training. Virtual reality (VR)-based treadmill training can increase motivation and improve therapy outcomes. Objective. The present study aimed at (1) demonstrating the feasibility and acceptance of an immersive virtual reality application (presented via head-mounted display, HMD) for gait rehabilitation with patients, and (2) compare its effects to a semi-immersive presentation (via a monitor) and a conventional treadmill training without VR. Methods and results. 36 healthy participants and 14 persons with MS or stroke participated in each of the three experimental conditions. For both groups, the walking speed in the HMD condition was higher than in treadmill training without VR. Healthy participants reported a higher motivation after the HMD condition as compared with the other conditions. Importantly, no side effects in the sense of simulator sickness occurred and usability ratings were high. Most of the healthy study participants (89 %) and patients (71 %) preferred the HMD-based training among the three conditions and most patients could imagine using it more frequently. Conclusion. The study demonstrated the feasibility of combining a treadmill training with immersive VR. Due to its high usability and low side effects, the immersive system could serve as a valid alternative to conventional treadmill training in gait rehabilitation. It might be particularly suited for patients to improve training motivation and training outcome e. g. the walking speed compared with treadmill training using no or only semi-immersive VR.',\n",
       " 'Joint applications of virtual reality (VR) systems and electroencephalography (EEG) offer numerous new possibilities ranging from behavioral science to therapy. VR systems allow for highly controlled experimental environments, while EEG offers a non-invasive window to brain activity with a millisecond-ranged temporal resolution. However, EEG measurements are highly susceptible to electromagnetic (EM) noise and the influence of EM noise of head-mounted-displays (HMDs) on EEG signal quality has not been conclusively investigated. In this paper, we propose a structured approach to test HMDs for EM noise potentially harmful to EEG measures. The approach verifies the impact of HMDs on the frequency- and time-domain of the EEG signal recorded in healthy subjects. The verification task includes a comparison of conditions with and without an HMD during (i) an eyes-open vs. eyes-closed task, and (ii) with respect to the sensory- evoked brain activity. The approach is developed and tested to derive potential effects of two commercial HMDs, the Oculus Rift and the HTC Vive Pro, on the quality of 64-channel EEG measurements. The results show that the HMDs consistently introduce artifacts, especially at the line hum of 50 Hz and the HMD refresh rate of 90 Hz, respectively, and their harmonics. The frequency range that is typically most important in non-invasive EEG research and applications (<50 Hz) however, remained largely unaffected. Hence, our findings demonstrate that high-quality EEG recordings, at least in the frequency range up to 50 Hz, can be obtained with the two tested HMDs. However, the number of commercially available HMDs is constantly rising. We strongly suggest to thoroughly test such devices upfront since each HMD will most likely have its own EM footprint and this article provides a structured approach to implement such tests with arbitrary devices.',\n",
       " 'ABSTRACT The development of embodied Virtual Reality (VR) systems involves multiple central design choices. These design choices affect the user perception and therefore require thorough consideration. This article reports on two user studies investigating the influence of common design choices on relevant intermediate factors (sense of embodiment, presence, motivation, activation, and task load) in a VR application for physical exercises. The first study manipulated the avatar fidelity (abstract, partial body vs. anthropomorphic, full-body) and the environment (with vs. without mirror). The second study manipulated the avatar type (healthy vs. injured) and the environment type (beach vs. hospital) and, hence, the avatar-environment congruence. The full-body avatar significantly increased the sense of embodiment and decreased mental demand. Interestingly, the mirror did not influence the dependent variables. The injured avatar significantly increased the temporal demand. The beach environment significantly reduced the tense activation. On the beach, participants felt more present in the incongruent condition embodying the injured avatar.',\n",
       " 'ABSTRACT The development of embodied Virtual Reality (VR) systems involves multiple central design choices. These design choices affect the user perception and therefore require thorough consideration. This article reports on two user studies investigating the influence of common design choices on relevant intermediate factors (sense of embodiment, presence, motivation, activation, and task load) in a VR application for physical exercises. The first study manipulated the avatar fidelity (abstract, partial body vs. anthropomorphic, full-body) and the environment (with vs. without mirror). The second study manipulated the avatar type (healthy vs. injured) and the environment type (beach vs. hospital) and, hence, the avatar-environment congruence. The full-body avatar significantly increased the sense of embodiment and decreased mental demand. Interestingly, the mirror did not influence the dependent variables. The injured avatar significantly increased the temporal demand. The beach environment significantly reduced the tense activation. On the beach, participants felt more present in the incongruent condition embodying the injured avatar.',\n",
       " 'Multiple sclerosis (MS) is a multi-focal progressive disorder of the central nervous system often resulting in diverse clinical manifestations. Imbalance appears in most people with multiple sclerosis (PwMS). A popular balance training tool is virtual reality (VR) with several advantages including increased compliance and user satisfaction. Therefore, the aim of this pilot RCT (Trial registration number, date: ISRCTN14425615, 21/01/2016) was to examine the efficacy of a 6-week VR balance training program using the computer assisted rehabilitation environment (CAREN) system (Motek Medical BV, Amsterdam, Netherlands) on balance measures in PwMS. Results were compared with those of a conventional balance exercise group. Secondary aims included the impact of this program on the fear of falling.',\n",
       " 'Recent technology advances in both Virtual Reality and Augmented Reality are creating an opportunity for a paradigm shift in the design of human-computer interaction systems. Delving into the Reality-Virtuality Continuum, we find Mixed Reality – systems designed to augment the physical world with virtual entities that embody characteristics of real world objects. In the medical field, Mixed Reality systems can overlay real-time and spatially accurate results onto a patient’s body without the need for external screens. The complexity of these systems previously required specialized prototypes, but newly available commercial products like the Microsoft HoloLens make the technology more available. Through a combination of literature review, expert analysis, and prototyping we explore the use of Mixed Reality in this setting. From the experience of prototyping HoloSim and Patiently, two applications for augmenting medical training and education, we outline considerations for the future design and development of virtual interfaces grounded in reality.',\n",
       " 'Dieser Artikel beschreibt die digitale Umsetzung eines rollenspielbasierten Brettspiels zur Exploration neuer Interaktionstechniken. Als gemeinsame Mixed-Reality-Spielumgebung dient ein Multitouch-Tisch mit Objekterkennung für haptisch erfassbare Spielelemente (Spielfiguren, Karten, ...). Das System ergänzt die realen Objekte mit multimedialen Informationen gemäß des aktuellen Spielgeschehens. Die Integration tragbarer Endgeräte über eine HTML5 -Schnittstelle ermöglicht private und individualisierte Interaktionsbereiche. Das System vereint unterschiedliche Interaktionstechniken wie Touch-Eingabe und Interaktion mit greifbaren Objekten, um den Zufriedenheitsgrad bei Interaktionen positiv zu beeinflussen. Eine Pilotstudie mit rollenspielerfahrenen Benutzern prüft die Akzeptanz der neuen Spiel- und Interaktionsmöglichkeiten.',\n",
       " 'Dieser Artikel beschreibt die digitale Umsetzung eines rollenspielbasierten Brettspiels zur Exploration neuer Interaktionstechniken. Als gemeinsame Mixed-Reality-Spielumgebung dient ein Multitouch-Tisch mit Objekterkennung für haptisch erfassbare Spielelemente (Spielfiguren, Karten, ...). Das System ergänzt die realen Objekte mit multimedialen Informationen gemäß des aktuellen Spielgeschehens. Die Integration tragbarer Endgeräte über eine HTML5 -Schnittstelle ermöglicht private und individualisierte Interaktionsbereiche. Das System vereint unterschiedliche Interaktionstechniken wie Touch-Eingabe und Interaktion mit greifbaren Objekten, um den Zufriedenheitsgrad bei Interaktionen positiv zu beeinflussen. Eine Pilotstudie mit rollenspielerfahrenen Benutzern prüft die Akzeptanz der neuen Spiel- und Interaktionsmöglichkeiten.',\n",
       " 'Dieser Artikel beschreibt die digitale Umsetzung eines rollenspielbasierten Brettspiels zur Exploration neuer Interaktionstechniken. Als gemeinsame Mixed-Reality-Spielumgebung dient ein Multitouch-Tisch mit Objekterkennung für haptisch erfassbare Spielelemente (Spielfiguren, Karten, ...). Das System ergänzt die realen Objekte mit multimedialen Informationen gemäß des aktuellen Spielgeschehens. Die Integration tragbarer Endgeräte über eine HTML5 -Schnittstelle ermöglicht private und individualisierte Interaktionsbereiche. Das System vereint unterschiedliche Interaktionstechniken wie Touch-Eingabe und Interaktion mit greifbaren Objekten, um den Zufriedenheitsgrad bei Interaktionen positiv zu beeinflussen. Eine Pilotstudie mit rollenspielerfahrenen Benutzern prüft die Akzeptanz der neuen Spiel- und Interaktionsmöglichkeiten.',\n",
       " 'Recent developments in the field of semi-immersive display technologies pro- vide new possibilities for engaging users in interactive three-dimensional virtual environments (VEs). For instance, combining low-cost tracking systems (such as the Microsoft Kinect) and multi-touch interfaces enables inexpensive and easily maintainable interactive setups. The goal of this work is to bring together virtual as well as real objects on a stereoscopic multi- touch enabled tabletop surface. Therefore, we present a prototypical implementation of such a mixed reality (MR) space for tangible interaction by extending the smARTbox FLBS12. The smARTbox is a responsive touch-enabled stereoscopic out-of-the-box system that is able to track users and objects above as well as on the surface. We describe the prototypical hard- and software setup which extends this setup to a MR space, and highlight design challenges for the several application examples.',\n",
       " 'Recent developments in the field of semi-immersive display technologies pro- vide new possibilities for engaging users in interactive three-dimensional virtual environments (VEs). For instance, combining low-cost tracking systems (such as the Microsoft Kinect) and multi-touch interfaces enables inexpensive and easily maintainable interactive setups. The goal of this work is to bring together virtual as well as real objects on a stereoscopic multi- touch enabled tabletop surface. Therefore, we present a prototypical implementation of such a mixed reality (MR) space for tangible interaction by extending the smARTbox FLBS12. The smARTbox is a responsive touch-enabled stereoscopic out-of-the-box system that is able to track users and objects above as well as on the surface. We describe the prototypical hard- and software setup which extends this setup to a MR space, and highlight design challenges for the several application examples.',\n",
       " 'Mixed Reality (MR) aims to create user interfaces in which interactive virtual objects are overlaid on the physical environment, naturally blending with it in real time. In this paper we present Tiles, a MR authoring interface for easy and effective spatial composition, layout and arrangement of digital objects in MR environments. Based on a tangible MR interface approach, Tiles is a transparent user interface that allows users to seamlessly interact with both virtual and physical objects. It also introduces a consistent MR interface model, providing a set of tools that allows users to dynamically add, remove, copy, duplicate and annotate virtual objects anywhere in the 3D physical workspace. Although our interaction techniques are broadly applicable, we ground them in an application for rapid prototyping and evaluation of aircraft instrument panels. We also present informal user observations and a preliminary framework for further work.',\n",
       " \"Recent technology advances in both Virtual Reality and Augmented Reality are creating an opportunity for a paradigm shift in the design of human-computer interaction systems. Delving into the Reality-Virtuality Continuum, we find Mixed Reality - systems designed to augment the physical world with virtual entities that embody characteristics of real world objects. In the medical field, Mixed Reality systems can overlay real-time and spatially accurate results onto a patient's body without the need for external screens. The complexity of these systems previously required specialized prototypes, but newly available commercial products like the Microsoft HoloLens make the technology more available. Through a combination of literature review, expert analysis, and prototyping we explore the use of Mixed Reality in healthcare. From the experience of prototyping Patiently and HoloSim, two applications for augmenting medical training, we outline considerations for the future design and development of virtual interfaces grounded in reality.\",\n",
       " \"Collaborative technologies increasingly permeate our everyday lives. Mixed reality games use these technologies to entertain, motivate, educate, and inspire. We understand mixed reality games as goal-directed, structured play experiences that are not fully contained by virtual or physical worlds. They transform existing technologies, relationships, and places into platforms for gameplay. While the design of mixed reality games has received increasing attention across multiple disciplines, a focus on the collaborative potential of mixed reality formats, such as augmented and alternate reality games, has been lacking. We believe the CSCW community can play an essential and unique role in examining and designing the next generation of mixed reality games and technologies that support them. To this end, we seek to bring together researchers, designers, and players to advance an integrated mixed reality games' research canon and outline key opportunities and challenges for future research and development.\",\n",
       " 'In this paper the software architecture of a framework which simplifies the development of applications in the area of Virtual and Augmented Reality is presented. It is based on VRML/X3D to enable rendering of audio-visual information. We extended our VRML rendering system by a device management system that is based on the concept of a data-flow graph. The aim of the system is to create Mixed Reality (MR) applications simply by plugging together small prefabricated software components, instead of compiling monolithic C++ applications. The flexibility and the advantages of the presented framework are explained on the basis of an exemplary implementation of a classic Augmented Realityapplication and its extension to a collaborative remote expert scenario.',\n",
       " 'Mixed reality computer systems aim to fuse digital and physical information and features, either as an augmentation of real-world environments or as a means of providing physically-based interaction with computer-based systems. So - for example - museum displays can be augmented with added information about their history and provenance, and this information delivered to a visitor via their mobile telephone as they pass in front of the display. An increasing number of systems are exploiting mixed reality but to date there are no systematic methods, techniques or guidelines for the development of such systems. In bringing together contributions on a broad range of mixed reality development issues this book provides a sound theoretical foundation for a disciplined approach to mixed reality engineering. Divided into three parts, interaction design, software design and implementation, this book brings together state of the art developments in the engineering of mixed reality systems, addressing issues at these three levels. Part 1 covers generic and specific mixed reality design elements and provides an overview of the design method; Part 2 addresses technical solutions for interaction techniques, development tools and a global view of the mixed reality software development process. Finally part 3 contains detailed case studies to highlight the application of mixed reality in a variety of fields including aviation, architecture, emergency management, games, and healthcare. Multimodal and tangible interaction designers, systems engineers working in augmented reality and ubiquitous systems, human factors researchers, and others interested in the use of mixed reality techniques in their work, will find this book a useful addition to their bookshelf.',\n",
       " 'This paper presents a RESTful Web service platform for applications for both Web browsers and mobile clients. Having a common service backend makes creating applications fast, simple, and open to third parties. The paper presents two mixed reality applications that have been built on the platform. It summarizes requirements for a mixed reality platform and defines a mixed reality domain model that the platform and applications share. In addition, it describes how the clients can use the REST interface to perform operations on user-generated content, as well as access real-life commercial geo-content such as street-view panoramas and building models.',\n",
       " 'Quest - XRoads is a multimodal and multimedia mixed reality version of the traditional role-play tabletop game Quest: Zeit der Helden. The original game concept is augmented with virtual content, controllable via auditory, tangible and spatial interfaces to permit a novel gaming experience and to increase the satisfaction while playing. The demonstration consists of a turn-based skirmish, where up to four players have to collaborate to defeat an opposing player. In order to be victorious, players have to control heroes or villains and use their abilities via speech, gesture, touch as well as tangible interactions.',\n",
       " 'Aerial photographs play a major role in current remote sens- ing applications. Traditionally such photographs were ac- quired using airplanes or satellites, and later processed in order to generate a virtual environment (VE). The industrial research project AVIGLE explores novel approaches to multimodal remote sensing by using a swarm of Miniature Un- manned Aerial Vehicles (MUAVs), which are equipped with different sensing and network technologies. The acquired data will be sent in quasi-real time to a ground control sta- tion, where a VE will be generated automatically. Although this approach introduces a new low-cost alternative to the traditional remote sensing processes, it opens up numerous research and engineering challenges. In this position paper we introduce a mixed reality (MR)-based simulation framework, which has been developed in the scope of the AVIGLE project. The main idea for this framework is to create a con- trolled environment in which parts of the system are real, whereas other parts are virtual and can be simulated for testing purposes. This MR simulation framework is intended to use virtual and real sensing technology in combination to provide a flexible solution to simulate and evaluate different hardware setups and algorithms.',\n",
       " \"In previous work, researchers have repeatedly demonstrated that robots' use of deictic gestures enables effective and natural human-robot interaction. However, new technologies such as augmented reality head mounted displays enable environments in which mixed-reality becomes possible, and in such environments, physical gestures become but one category among many different types of mixed reality deictic gestures. In this paper, we present the first experimental exploration of the effectiveness of mixed reality deictic gestures beyond physical gestures. Specifically, we investigate human perception of videos simulating the display of allocentric gestures, in which robots circle their targets in users' fields of view. Our results suggest that this is an effective communication strategy, both in terms of objective accuracy and subjective perception, especially when paired with complex natural language references.\",\n",
       " 'This paper examines the role of adults’ participation in a mixed reality experience in aiding children to discover, reason and reflect about historical places and events. An experience was developed which involved a paper-based ‘history hunt’ around a castle searching for clues about historical events that happened at the site. The clues involved people making drawings or rubbings on paper at a variety of locations. The paper was then electronically tagged and used to interact with a story tent. This consisted of a tarpaulin structure with projections on either side. A Radio Frequency ID tag reader and ultra violet light were positioned inside the tent. When placed on the tag reader, each paper clue revealed an historic 3D environment of the castle from the location at which the clue was found. ‘Secret writing’ revealed the story of a character at this location, while 2D images on the tent displayed corresponding scenes from this historical period. Visitors to the castle over 4 days took part in this experience. Video analysis of children and adults in the Storytent suggest that the quality of adult engagement reflects the extent to which children discover and make connections. The importance of these findings is discussed in relation to future design of mixed reality technologies to support collaborative learning.',\n",
       " 'In diesem Beitrag wird zunächst ein Gestaltungsmodell für mobile Mensch-Maschine-Systeme vorgestellt, das zum einen unterschiedliche Aspekte von Mobilität und zum anderen ein strukturiertes Vorgehen unter expliziter Nutzerbeteiligung integriert. Der Gestaltungsprozess stützt sich dabei auf arbeitswissenschaftliche Methoden und umfasst in Anlehnung an DIN EN ISO 13407 die Phasen der Analyse, Konzeption, Integration und Evaluation. Die exemplarische Nutzung dieses Modells wird im zweiten Teil des Betrags anhand der Entwicklung einer konkreten Mixed-Reality-Anwendung erläutert, mit der Servicefälle bei Werkzeugmaschinen multimodal unterstützt werden sollen, da in diesem Anwendungsfeld herkömmliche Technologien kaum mehr ausreichen, um die komplexen Diagnose- und Reparaturprozeduren zu unterstützten. In der Analysephase wurden die Anforderungen partizipativ und szenarienbasiert in verschiedenen Fokusgruppen erhoben und zu Use Cases zusammengefasst. Bei der Konzeption wurden mögliche Mixed-Reality-unterstützte Arbeitsprozesse gemeinsam mit den potentiellen zukünftigen Benutzern weiter konkretisiert. Die Prototypentwicklung und Integration erfolgten zunächst mit einfachen Mock-Ups. Ein iteratives Vorgehen mit komplexeren prototypischen Umsetzungen bildete dann die Basis für detailliertere weitere Evaluationen und Tests mit den Anwendern.',\n",
       " \"Performing time-critical procedures such as Cardiopulmonary Resuscitation (CPR) usually requires trained individuals on the scene. Even when step by step instructions are available, most bystanders do not attempt resuscitation due to panic or fear of failing, often at the cost of the victim's life. We propose Mixed Reality (MR) as a compelling medium to support time-critical emergencies, and study its use in this context through an iterative user-centered design process. Our research outlines a number of key considerations for the design of time-critical emergency interfaces that led to the creation of HoloCPR, an MR application providing real-time instructions for resuscitation through a combination of visual and spatial cues. HoloCPR's comparative evaluation during a realistic resuscitation scenario indicates how the use of MR can result in decreased reaction time and increased procedural accuracy. With this work, we hope to bootstrap a new wave of MR applications for time-critical emergencies that can be included in first aid kits in the future.\",\n",
       " 'Fahrzeugnavigationssysteme (FNS) sind komplexe Einheiten, die Funktionen zur Ortung, Routenplanung und Zielführung unter Echtzeitanforderungen zur Verfügung stellen. Dabei werden alle Funktionen durch ein benutzerfreundliches Interface zugänglich gemacht. Heutige Fahrzeugnavigationssysteme verwenden meist eine Zielführung mit zweidimensionaler Karte, gesprochenen Anweisungen und der Einblendung von Piktogrammen (z.B. Pfeile). Neuere Entwicklungsprojekte arbeiten daran, 3D Stadtmodelle anstelle von zweidimensionalen Karten zu verwenden, was jedoch nur einen begrenzten praktischen Mehrwert hat. Ganz andere Möglichkeiten bieten sich durch Techniken der Augmented Reality (AR) und Mixed Reality (MR). Diese haben den Vorteil, dass keine detaillierten Modelle der Umgebung berechnet werden müssen. In diesem Beitrag wird eine MR Visualisierungstechnik vorgestellt, die als Grundlage Bilder von Kreuzungen erfasst und in Abhängigkeit von den Fahranweisungen mit virtuellen Piktogrammen überlagert. Dazu entstanden sowohl die notwendigen Erfassungswerkzeuge als auch ein auf einer kommerziellen Navigationssoftware basierender Demonstrator, der unter realen Bedingungen innerhalb des Testgebietes der Hannoverschen Nordstadt validiert wurde.',\n",
       " 'The rapid development of 3D scanning technology combined with state-of-the-art mapping algorithms allows to capture 3D point clouds with high resolution and accuracy. The high amount of data collected with LiDAR, RGB-D cameras or generated through SfM approaches makes the direct use of the recorded data for realistic rendering and simulation problematic. Therefore, these point clouds have to be transformed into representations that fulfill the computational requirements for VR and AR setups. In this tutorial participants will be introduced to state-of-the-art methods in point cloud processing and surface reconstruction with open source software to learn the benefits for AR and VR applications by interleaved presentations, software demonstrations and software trials. The focus lies on 3D point cloud data structures (range images, octrees, k-d trees) and algorithms, and their implementation in C/C++. Surface reconstruction using Marching Cubes and other meshing methods will play another central role. Reference material for subtopics like 3D point cloud registration and SLAM, calibration, filtering, segmentation, meshing, and large scale surface reconstruction will be provided. Participants are invited to bring their Linux, MacOS or Windows laptops to gain hands-on experience on practical problems occuring when working with large scale 3D point clouds in VR and AR applications.',\n",
       " 'We design and study a mixed reality game, PhotoNav, to investigate wearable computing display modalities in which players need to split attention. PhotoNav requires that the player split attention between the physical world and display by using geotagged photographs as hints for navigation. Even though the head-mounted display requires a shorter shift in visual attention, we find players prefer the separation of the handheld. This research contributes ways in which mixed reality game UIs can focus players on the physical environment, leveraging its unique affordances.',\n",
       " 'This chapter presents an overview of the Mixed Reality (MR) paradigm, which proposes to overlay our real-world environment with digital, computer-generated objects. It presents example applications and outlines limitations and solutions for their technical implementation. In MR systems, users perceive both the physical environment around them and digital elements presented through, for example, the use of semitransparent displays. By its very nature, MR is a highly interdisciplinary field engaging signal processing, computer vision, computer graphics, user interfaces, human factors, wearable computing, mobile computing, information visualization, and the design of displays and sensors. This chapter presents potential MR applications, technical challenges in realizing MR systems, as well as issues related to usability and collaboration in MR. It separately presents a section offering a selection of MR projects which have either been partly or fully undertaken at Swiss universities and rounds off with a section on current challenges and trends.',\n",
       " 'Moving in virtual environments has become very common. On the one hand there are e.g. non-interactive route guidance systems which are not adequate for pedestrians. On the other hand interactive applications like Second Life fascinate a broad community.</p> <p>This work presents the information system UBIGIouS that combines reality with virtuality in an interactive mixed reality scenario. It is intended to link Virtual Reality Geographic Information Systems with purely virtual interactive applications. For ubiquitous access, users can participate from their homes by standard, stationary PCs or on site using mobile devices like smartphones. By means of visual building recognition capabilities UBIGIouS offers location-based services with or without positioning via the Global Positioning System GPS.',\n",
       " 'The augmentation of real environments with additional computer generated information (augmented reality, AR), and the addition of virtual contents (mixed reality, MR) are promising interaction paradigms for the support of applications outside the classical office scenario. While the potential of AR for applications such as geo-visualization has been shown using a number of demonstration systems, there are virtually no systems in daily use up to now. This is mainly due to technical reasons. Besides suitable displays, there is a lack of reliable and accurate tracking systems, which are however required for an exact registration of the virtual contents and the real scene. Additionally, there is a lack of experience and tools for the creation of mixed reality applications and often, of intuitive interaction techniques. Especially for applications involving public audience (e.g. exhibitions, museums, public participation), additional requirements apply, such as reliability, robustness, easy adaptation to different users, and the necessity for relatively low operating expenses. In this paper, we introduce the GeoScope, a mixed-reality input/output device, which addresses those problems, especially for public applications. The GeoScope is mounted at a fixed position and consists of a display, oriented towards the user, and a camera, which points at the surroundings. Just as a telescope, the GeoScope can be turned around two axes, the two angles being captured by measuring devices. Together with the known position, a fast and highly precise tracking of the current view direction is possible, allowing the superposition of the real scene, as delivered by the camera, and virtually generated information.',\n",
       " 'This paper proposes a virtual carving system using ToolDevice in a mixed reality (MR) space. By touching and moving the device over real objects, users can carve it virtually. Real-world wood carving with wood carving tools requires several steps such as carving a rough outline, shaping the wood, and carving patterns on its surface. In this paper, we focus on the step of carving patterns on a surface and implement it in our MR carving system.',\n",
       " 'Dieser Artikel beschreibt die digitale Umsetzung eines rollenspielbasierten Brettspiels zur Exploration neuer Interaktionstechniken. Als gemeinsame Mixed-Reality-Spielumgebung dient ein Multitouch-Tisch mit Objekterkennung für haptisch erfassbare Spielelemente (Spielfiguren, Karten, ...). Das System ergänzt die realen Objekte mit multimedialen Informationen gemäß des aktuellen Spielgeschehens. Die Integration tragbarer Endgeräte über eine HTML5 -Schnittstelle ermöglicht private und individualisierte Interaktionsbereiche. Das System vereint unterschiedliche Interaktionstechniken wie Touch-Eingabe und Interaktion mit greifbaren Objekten, um den Zufriedenheitsgrad bei Interaktionen positiv zu beeinflussen. Eine Pilotstudie mit rollenspielerfahrenen Benutzern prüft die Akzeptanz der neuen Spiel- und Interaktionsmöglichkeiten.',\n",
       " 'Dieser Artikel beschreibt die digitale Umsetzung eines rollenspielbasierten Brettspiels zur Exploration neuer Interaktionstechniken. Als gemeinsame Mixed-Reality-Spielumgebung dient ein Multitouch-Tisch mit Objekterkennung für haptisch erfassbare Spielelemente (Spielfiguren, Karten, ...). Das System ergänzt die realen Objekte mit multimedialen Informationen gemäß des aktuellen Spielgeschehens. Die Integration tragbarer Endgeräte über eine HTML5 -Schnittstelle ermöglicht private und individualisierte Interaktionsbereiche. Das System vereint unterschiedliche Interaktionstechniken wie Touch-Eingabe und Interaktion mit greifbaren Objekten, um den Zufriedenheitsgrad bei Interaktionen positiv zu beeinflussen. Eine Pilotstudie mit rollenspielerfahrenen Benutzern prüft die Akzeptanz der neuen Spiel- und Interaktionsmöglichkeiten.',\n",
       " \"Mixed Reality (MR) applications along Milgram's Reality-Virtuality (RV) continuum motivated a number of recent theories on potential constructs and factors describing MR experiences. This paper investigates the impact of incongruencies on the sensation/perception and cognition layers to provoke breaks in plausibility, and the effects these breaks have on spatial and overall presence as prominent constructs of Virtual Reality (VR). We developed a simulated maintenance application to test virtual electrical devices. Participants performed test operations on these devices in a counterbalanced, randomized 2x2 between-subject design in either VR as congruent, or Augmented Reality (AR) as incongruent on the sensation/perception layer. Cognitive incongruency was induced by the absence of traceable power outages, decoupling perceived cause and effect after activating potentially defective devices. Our results indicate significant differences in the plausibility ratings between the VR and AR conditions, hence between congruent/incongruent conditions on the sensation/perception layer. In addition, spatial presence revealed a comparable interaction pattern with the VR vs AR conditions. Both factors decreased for the AR condition (incongruent sensation/perception) compared to VR (congruent sensation/perception) for the congruent cognitive case but increased for the incongruent cognitive case. The results are discussed and put into perspective in the scope of recent theories of MR experiences.\",\n",
       " 'Dieser Artikel beschreibt die digitale Umsetzung eines rollenspielbasierten Brettspiels zur Exploration neuer Interaktionstechniken. Als gemeinsame Mixed-Reality-Spielumgebung dient ein Multitouch-Tisch mit Objekterkennung für haptisch erfassbare Spielelemente (Spielfiguren, Karten, ...). Das System ergänzt die realen Objekte mit multimedialen Informationen gemäß des aktuellen Spielgeschehens. Die Integration tragbarer Endgeräte über eine HTML5 -Schnittstelle ermöglicht private und individualisierte Interaktionsbereiche. Das System vereint unterschiedliche Interaktionstechniken wie Touch-Eingabe und Interaktion mit greifbaren Objekten, um den Zufriedenheitsgrad bei Interaktionen positiv zu beeinflussen. Eine Pilotstudie mit rollenspielerfahrenen Benutzern prüft die Akzeptanz der neuen Spiel- und Interaktionsmöglichkeiten.',\n",
       " 'In this paper we propose the concepts of virtual reflections, lights and shadows to enhance immersion in mixed reality (MR) environments, which focus on merging the real and the virtual world seamlessly. To improve immersion, we augment the virtual objects with real world information regarding the virtual reality (VR) system environment, e.g., CAVE, workbench etc. Real-world objects such as input devices or light sources as well as the position and posture of the user are used to simulate global illumination phenomena, e.g., users can see their own reflections and shadows on virtual objects. Besides the concepts and the implementation of this approach, we describe the system setup. and an example application for this kind of advanced MR system environment.',\n",
       " '›Mixed Reality‹ beschreibt die Kombination aus virtuellen Umgebungen und natürlichen Benutzerschnittstellen. Das Sichtfeld der nutzenden Person wird hierbei über Head Mounted Displays durch Kopfbewegung gesteuert, Datenhandschuhe ermöglichen die Interaktion mit virtuellen Objekten und omnidirektionale Böden erlauben eine unbegrenzte Fortbewegung durch virtuelle Welten mittels natürlichen Gehens. In einer quasi-experimentellen Untersuchung konnte gezeigt werden, dass ›Mixed Reality‹ sich vor allem auf emotionaler und motivationaler Ebene positiv auswirkt. Die Ergebnisse werden vor dem Hintergrund der Frage interpretiert, inwiefern sich durch derart innovative Technologien grundlegend neue Möglichkeiten für die Erziehungswissenschaften ergeben.',\n",
       " 'This paper presents a mixed reality tabletop role-playing game with a novel combination of interaction styles and gameplay mechanics. Our contribution extends previous approaches by abandoning the traditional turn-based gameplay in favor of simultaneous real-time interaction. The increased cognitive and physical load during the simultaneous control of multiple game characters is counteracted by two features: First, certain game characters are equipped with AI-driven capabilities to become semi-autonomous virtual agents. Second, (groups of) these agents can be instructed by high-level commands via a multimodal—speech and gesture—interface.',\n",
       " 'This paper presents a mixed reality tabletop role-playing game with a novel combination of interaction styles and gameplay mechanics. Our contribution extends previous approaches by abandoning the traditional turn-based gameplay in favor of simultaneous real-time interaction. The increased cognitive and physical load during the simultaneous control of multiple game characters is counteracted by two features: First, certain game characters are equipped with AI-driven capabilities to become semi-autonomous virtual agents. Second, (groups of) these agents can be instructed by high-level commands via a multimodal—speech and gesture—interface.',\n",
       " 'Recent developments in the eld of semi-immersive display technologies provide new possibilities for engaging users in interactive three-dimensional virtual environments (VEs). For instance, combining low-cost tracking systems (such as the Microsoft Kinect) and multi-touch interfaces enables inexpensive and easily maintainable interactive setups. The goal of this work is to bring together virtual as well as real objects on a stereoscopic multitouch enabled tabletop surface. Therefore, we present a prototypical implementation of such a mixed reality (MR) space for tangible interaction by extending the smARTbox FLBS12. The smARTbox is a responsive touch-enabled stereoscopic out-of-the-box system that is able to track users and objects above as well as on the surface. We describe the prototypical hardand software setup which extends this setup to a MR space, and highlight design challenges for the several application examples.',\n",
       " 'This paper presents a mixed reality tool developed for the training of the visually impaired based on haptic and auditory feedback. The proposed approach focuses on the development of a highly interactive and extensible Haptic Mixed Reality training system that allows visually impaired to navigate into real size Virtual Reality environments. The system is based on the use of the CyberGrasp haptic device. An efficient collision detection algorithm based on superquadrics is also integrated into the system so as to allow real time collision detection in complex environments. A set of evaluation tests is designed in order to identify the importance of haptic, auditory and multimodal feedback and to compare the MR cane against the existing Virtual Reality cane simulation system.',\n",
       " 'A key component of team coordination in real-world practice involves communication and work execution from multiple perspectives; this is especially true for unmanned robotic system operators. Previously, such communication skills have been challenging to train, requiring many users and/or high-fidelity simulations. The present research develops a mixed reality game using a small robot and alternate roles to engage two players in team communication through information distribution. The robot features imperfect controls, creating challenge and reflecting the real-world context. This paper presents the game design and development challenges, reflecting on the value of mixed reality for training hybrid human-robot teams.',\n",
       " 'Fahrzeugnavigationssysteme sind komplexe Einheiten, die Funktionen zur Ortung, Routenplanung und Zielführung unter Echtzeitanforderungen zur Verfügung stellen. Dabei werden alle Funktionen hinter einem benutzerfreundlichen Interface verborgen. Heutige Fahrzeugnavigationssysteme gehen imWesentlichen auf die ersten iim Jahr 1995 eingeführten Geräte zurück. Diese verwenden meist eine zweidimensionale Karte, gesprochene Anweisungen und die Einblendung von Piktogrammen (z.B. Pfeile). Die jüngsten Entwicklungen gehen dahin, dass Fahrzeugnavigationssysteme zunehmend für allgemeine Plattformen (PCs, PDAs) verfügbar sind. Die Darstellungsformen haben sich jedoch kaum verändert. In dieser Arbeit werden die Einsatzmöglichkeiten von Mixed Reality (MR) Techniken in Fahrzeugnavigationssystemen untersucht. Als Grundlage werden Bilder von Kreuzungen erfasst und mit virtuellen Piktogrammen überlagert. Die Generierung dieser virtuellen Informationen wird halbautomatisch vorgenommen. Das hierfür benötigte Programm wurde im Rahmen dieser Arbeit entwickelt. Im zweiten Teil der Arbeit wird ein Fahrzeugnavigationssystem auf der Basis eines kommerziellen Produktes entwickelt, welches unter realen Bedingungen innerhalb eines Testgebietes die MR-Technik verwendet.',\n",
       " 'Fahrzeugnavigationssysteme sind komplexe Einheiten, die Funktionen zur Ortung, Routenplanung und Zielführung unter Echtzeitanforderungen zur Verfügung stellen. Dabei werden alle Funktionen hinter einem benutzerfreundlichen Interface verborgen. Heutige Fahrzeugnavigationssysteme gehen imWesentlichen auf die ersten iim Jahr 1995 eingeführten Geräte zurück. Diese verwenden meist eine zweidimensionale Karte, gesprochene Anweisungen und die Einblendung von Piktogrammen (z.B. Pfeile). Die jüngsten Entwicklungen gehen dahin, dass Fahrzeugnavigationssysteme zunehmend für allgemeine Plattformen (PCs, PDAs) verfügbar sind. Die Darstellungsformen haben sich jedoch kaum verändert. In dieser Arbeit werden die Einsatzmöglichkeiten von Mixed Reality (MR) Techniken in Fahrzeugnavigationssystemen untersucht. Als Grundlage werden Bilder von Kreuzungen erfasst und mit virtuellen Piktogrammen überlagert. Die Generierung dieser virtuellen Informationen wird halbautomatisch vorgenommen. Das hierfür benötigte Programm wurde im Rahmen dieser Arbeit entwickelt. Im zweiten Teil der Arbeit wird ein Fahrzeugnavigationssystem auf der Basis eines kommerziellen Produktes entwickelt, welches unter realen Bedingungen innerhalb eines Testgebietes die MR-Technik verwendet.',\n",
       " 'This paper introduces an interactive surface concept for Mixed Reality (MR) tabletop games that combines a variable (LCD and/or projection) screen configuration with the detection of finger touches, in-air gestures, and tangibles. It is low-cost and minimally requires an ordinary table, a TV screen, and a Kinect v2 sensor. Existing applications can easily be connected by being compliant to standards. The concept is intended to foster further research on collaborative tabletop situations, not limited to games, but also in- cluding learning, meetings, and social interaction.',\n",
       " 'This paper introduces an interactive surface concept for Mixed Reality (MR) tabletop games that combines a variable (LCD and/or projection) screen configuration with the detection of finger touches, in-air gestures, and tangibles. It is low-cost and minimally requires an ordinary table, a TV screen, and a Kinect v2 sensor. Existing applications can easily be connected by being compliant to standards. The concept is intended to foster further research on collaborative tabletop situations, not limited to games, but also in- cluding learning, meetings, and social interaction.',\n",
       " \"We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered-not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMS-constraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMS-detents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline.\",\n",
       " 'This paper presents an image-based rendering approach to accelerate rendering time of virtual scenes containing a large number of complex high poly count objects. Our approach replaces complex objects by impostors, light-weight image-based representations leveraging geometry and shading related processing costs. In contrast to their classical implementation, our impostors are specifically designed to work in Virtual-, Augmented- and Mixed Reality scenarios (XR for short), as they support stereoscopic rendering to provide correct depth perception. Motion parallax of typical head movements is compensated by using a ray marched parallax correction step. Our approach provides a dynamic run-time recreation of impostors as necessary for larger changes in view position. The dynamic run-time recreation is decoupled from the actual rendering process. Hence, its associated processing cost is therefore distributed over multiple frames. This avoids any unwanted frame drops or latency spikes even for impostors of objects with complex geometry and many polygons. In addition to the significant performance benefit, our impostors compare favorably against the original mesh representation, as geometric and textural temporal aliasing artifacts are heavily suppressed.',\n",
       " 'This paper presents an image-based rendering approach to accelerate rendering time of virtual scenes containing a large number of complex high poly count objects. Our approach replaces complex objects by impostors, light-weight image-based representations leveraging geometry and shading related processing costs. In contrast to their classical implementation, our impostors are specifically designed to work in Virtual-, Augmented- and Mixed Reality scenarios (XR for short), as they support stereoscopic rendering to provide correct depth perception. Motion parallax of typical head movements is compensated by using a ray marched parallax correction step. Our approach provides a dynamic run-time recreation of impostors as necessary for larger changes in view position. The dynamic run-time recreation is decoupled from the actual rendering process. Hence, its associated processing cost is therefore distributed over multiple frames. This avoids any unwanted frame drops or latency spikes even for impostors of objects with complex geometry and many polygons. In addition to the significant performance benefit, our impostors compare favorably against the original mesh representation, as geometric and textural temporal aliasing artifacts are heavily suppressed.',\n",
       " 'Digital public participation formats are an emerging and accessible way to involve diverse groups of citizens in construction projects in their local area. Particularly, mixed reality can help project initiators to visualize the planned changes to the city landscape in an easy and understandable way, enabling people to participate in a creative manner. However, this technology is challenging for most project initiators, as it requires an extensive technical and/or domain experience. Besides that, specialized hardware and experienced staff is required. An easy on-boarding process, which introduces mixed reality step-by-step and offers assistance by external service providers could promote both adoption and usage. In this paper, we present the design process and resulting concept of a configurator for a public participation platform, that aims to guide initiators with different levels of technical knowledge. Besides detailing the design and development process of the prototype, we will present the preliminary results of our evaluation. The interview partners provided positive feedback on the usage of our configurator. Moreover, different approaches are necessary for the public and private sector when configuring and purchasing their participation solution. Finally, we highlight areas that are still in need of further work, such as the compliance with the regulations for public institutions and address further promising areas of research.',\n",
       " 'This thesis studies eye-based user interfaces which integrate information about the user’s perceptual focus-of-attention into multimodal systems to enrich the interaction with the surrounding environment. We examine two new modalities: gaze input and output in the peripheral field of view. All modalities are considered in the whole spectrum of the mixed-reality continuum. We show the added value of these new forms of multimodal interaction in two important application domains: Automotive User Interfaces and Human-Robot Collaboration. We present experiments that analyze gaze under various conditions and help to design a 3D model for peripheral vision. Furthermore, this work presents several new algorithms for eye-based interaction, like deictic reference in mobile scenarios, for non-intrusive user identification, or exploiting the peripheral field view for advanced multimodal presentations. These algorithms have been integrated into a number of software tools for eye-based interaction, which are used to implement 15 use cases for intelligent environment applications. These use cases cover a wide spectrum of applications, from spatial interactions with a rapidly changing environment from within a moving vehicle, to mixed-reality interaction between teams of human and robots.',\n",
       " \"With the advent of ubiquitous and pervasive computing environments, one of promising applications is a guidance system. In this paper, we propose a mobile mixed reality guide system for indoor environments, Chloe@University. A mobile computing device (Sony's Ultra Mobile PC) is hidden inside a jacket and a user selects a destination inside a building through voice commands. A 3D virtual assistant then appears in the see-through HMD and guides him/her to destination. Thus, the user simply follows the virtual guide. Chloe@University also suggests the most suitable virtual character (e.g. human guide, dog, cat, etc.) based on user preferences and profiles. Depending on user profiles, different security levels and authorizations for content are previewed. Concerning indoor location tracking, WiFi, RFID, and sensor-based methods are integrated in this system to have maximum flexibility. Moreover smart and transparent wireless connectivity provides the user terminal with fast and seamless transition among Access Points (APs). Different AR navigation approaches have been studied: Olwal 2006, Elmqvist et al. and Newman et al. work indoors while Bell et al. 2002 and Reitmayr and Drummond 2006 are employed outdoors. Accurate tracking and registration is still an open issue and recently it has mostly been tackled by no single method, but mostly through aggregation of tracking and localization methods, mostly based on handheld AR. A truly wearable, HMD based mobile AR navigation aid for both indoors and outdoors with rich 3D content remains an open issue and a very active field of multi-discipline research.\",\n",
       " \"From analyzing complex socio-technical systems, to evaluating novel interactions, increasingly pervasive sensing technologies provide researchers with new ways to observe the world. This paradigm shift is enabling capture of richer and more diverse data, combining elements from in-depth study of activity and behavior with modern sensors, and providing the means to accelerate sense-making of complex behavioral data. At the same time novel multimodal signal processing and machine learning techniques are equipping us with 'super powers' that enable understanding of these data in real-time, opening up new opportunities for embracing the concept of 'Data Science in the Wild'. In this paper we present what this transition means in the context of Health and Healthcare, focusing on how it leads to the 'UbiScope', a ubiquitous computing microscope for detecting particular health conditions in real-time, promoting reflection on care, and guiding medical practices. Just as the microscope supported key scientific advances, the UbiScope will act as a proxy for understanding and supporting human activity and inform specific interventions in the years ahead.\",\n",
       " 'The \"Where?\" is quite important for Mixed Reality applications: Where is the user looking at? Where should augmentations be displayed? The location of the overt visual attention of the user can be used both to disambiguate referent objects and to inform an intelligent view management of the user interface. While the vertical and horizontal orientation of attention is quite commonly used, e.g. derived from the orientation of the head, only knowledge about the distance allows for an intrinsic measurement of the location of the attention. This contribution reviews our latest results on detecting the location of attention in 3D space using binocular eye tracking.',\n",
       " \"This work proposes a novel algorithm to register cone-beam computed tomography (CBCT) volumes and 3D optical (RGBD) camera views. The co-registered real-time RGBD camera and CBCT imaging enable a novel augmented reality solution for orthopedic surgeries, which allows arbitrary views using digitally reconstructed radiographs overlaid on the reconstructed patient's surface without the need to move the C-arm.\",\n",
       " 'The rapid development of 3D scanning technology combined with state-of-the-art mapping algorithms allows to capture 3D point clouds with high resolution and accuracy. The high amount of data collected with LiDAR, RGB-D cameras or generated through SfM approaches makes the direct use of the recorded data for realistic rendering and simulation problematic. Therefore, these point clouds have to be transformed into representations that fulfill the computational requirements for VR and AR setups. In this tutorial participants will be introduced to state-of-the-art methods in point cloud processing and surface reconstruction with open source software to learn the benefits for AR and VR applications by interleaved presentations, software demonstrations and software trials. The focus lies on 3D point cloud data structures (range images, octrees, k-d trees) and algorithms, and their implementation in C/C++. Surface reconstruction using Marching Cubes and other meshing methods will play another central role. Reference material for subtopics like 3D point cloud registration and SLAM, calibration, filtering, segmentation, meshing, and large scale surface reconstruction will be provided. Participants are invited to bring their Linux, MacOS or Windows laptops to gain hands-on experience on practical problems occuring when working with large scale 3D point clouds in VR and AR applications.',\n",
       " 'The interactive scenarios realized in the two prototypes of Virtual Human require an approach that allows humans and virtual characters to interact naturally and flexibly. In this article we present how the autonomous control of the virtual characters and the interpretation of user interactions is realized in the Conversational Dialogue Engine (CDE) framework. For each virtual and real interlocutor one CDE is responsible for dialogue processing. We will introduce the knowledge needed for the CDE-approach and present the modules of a CDE. The real-time requirement resulted in the integrated processing of deliberative and reactive processing, which is needed, e.g., to generate an appropriate nonverbal behavior of virtual characters.',\n",
       " 'The interactive scenarios realized in the two prototypes of Virtual Human require an approach that allows humans and virtual characters to interact naturally and flexibly. In this article we present how the autonomous control of the virtual characters and the interpretation of user interactions is realized in the Conversational Dialogue Engine (CDE) framework. For each virtual and real interlocutor one CDE is responsible for dialogue processing. We will introduce the knowledge needed for the CDE-approach and present the modules of a CDE. The real-time requirement resulted in the integrated processing of deliberative and reactive processing, which is needed, e.g., to generate an appropriate nonverbal behavior of virtual characters.',\n",
       " 'This paper describes a system, which enables collaboration in a hybrid team consisting of a robot, physically present humans and remote humans, where the latter are connected via Virtual Reality. This setup spans the whole continuum between Physical and Virtual Reality, including Augmented Reality. The work presented herein, describes how such a scattered, hybrid team can interact and cooperate in a virtual representation of a factory, using eye-, head-, hand- and gesture-tracking as multimodal control and communication input.',\n",
       " 'Wem zum ersten Mal der Begriff Erweiterte Realität (Augmented Reality, AR) und die zugehörigen Prinzipien erklärt werden, antwortet meistens folgendermaßen: ,,Das gibt es doch schon in den Kinofilmen, die im Computer nachbearbeitet werden . Der Vergleich zu Kinofilmen ist allerdings nur teilweise richtig. Um die Unterschiede genauer zu erklären, beginnt dieses Buch mit der Aufschlüsselung der grundlegenden Paradigmen der Augmented Reality. Die nachfolgenden Kapitel beziehen schrittweise einzelne Technologien und Komponenten von AR-Systemen ein und entwickeln dann ein exemplarisches AR-System.',\n",
       " 'Our starting point for developing the Studierstube system was the belief that augmented reality, the less obtrusive cousin of virtual reality, has a better chance of becoming a viable user interface for applications requiring manipulation of complex three-dimensiona information as a daily routine. In essence, we are searching for a 3-D user interface metaphor as powerful as the desktop metaphor for 2-D. At the heart of the Studierstube system, collaborative augmented reality is used to embed computer-generated images into the real work environment In the #rst part of this paper, we review the user interface of the initial Studierstube system, in particular the implementation of collaborative augmented reality , and the Personal Interaction Panel, a two-handed interface for interaction with the system. In the second part, an extended Studierstube system based on a heterogeneous distributed architecture is presented. This system allows the user to combine multiple approaches--- augmented reality, projection displays, and ubiquitous computing---to the interface as needed. The environment is controlled by the Personal Interaction Panel, a twohanded, pen-and-pad interface that has versatile uses for interacting with the virtual environment. Studierstube also borrows elements from the desktop, such as multitasking and multi-windowing. The resulting software architecture is a user interface management system for complex augmented reality applications. The presentation is complemented by selected application examples 1',\n",
       " 'This paper gives a concise presentation of human PC collaboration and its different late applications. It further talks about Augmented Reality, the ongoing innovative improvements, the uses of enlarged reality in different fields and its working with the contextual investigation of the latest blow in the gaming scene, PokÃ©mon go, which is a game dependent on Augmented Reality. Bharath. D | Dr. S. Vengateshkumar \"Human Computer Interface-Augmented Reality\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-6 , October 2019, URL: https://www.ijtsrd.com/papers/ijtsrd29317.pdf Paper URL: https://www.ijtsrd.com/computer-science/other/29317/human-computer-interface-augmented-reality/bharath-d',\n",
       " 'This tutorial discusses the Spatial Augmented Reality (SAR) concept, its advantages and limitations. It will present examples of state-of-the-art display configurations, appropriate real-time rendering techniques, details about hardware and software implementations, and current areas of application. Specifically, it will describe techniques for optical combination using single/multiple spatially aligned mirror-beam splitters, image sources, transparent screens and optical holograms. Furthermore, it presents techniques for projector-based augmentation of geometrically complex and textured display surfaces, and (along with optical combination) methods for achieving consistent illumination and occlusion effects. Emerging technologies that have the potential of enhancing future augmented reality displays will be surveyed.',\n",
       " 'This paper introduces a novel graphics rendering pipeline applied to augmented reality, based on a real time ray tracing paradigm. Ray tracing techniques process pixels independently from each other, allowing an easy integration with image-based tracking techniques, contrary to traditional projection-based rasterization graphics systems, e.g. OpenGL. Therefore, by associating our highly optimized ray tracer with an augmented reality framework, the proposed pipeline is capable to provide high quality rendering with real time interaction between virtual and real objects, such as occlusions, soft shadows, custom shaders, reflections and self-reflections, some of these features only available in our rendering pipeline. As proof of concept, we present a case study with the ARToolKitPlus library and the Microsoft Kinect hardware, both integrated in our pipeline. Furthermore, we show the performance and visual results in high definition of the novel pipeline on modern graphics cards, presenting occlusion and recursive reflection effects between virtual and real objects without the latter ones needing to be previously modeled when using Kinect. Furthermore, an adaptive soft shadow sampling algorithm for ray tracing is presented, generating high quality shadows in real time for most scenes.',\n",
       " 'Augmented Reality AR is the technology that overlaps virtual objects onto real world objects. It has three main features the combination of the real world and the virtual world, real time interaction, and 3D registration. The algorithms used to produce graphical images and other sensor based inputs on real world objects uses the camera of your device. The shortest route and graphics information in 3D is not notified in normal maps. To avoid such problems we have developed a 3D virtual environment that gives graphics and contains more information about a particular place. This project is done by using the â€œUNITYâ€\\x9d application, the engine can be used to create three dimensional, two dimensional which helps to view all these graphics and routes in a 3D view. R. Mohana Priya | Subash. R | Yogesh. R | Vignesh. M | Gopi. V Äugmented Reality Map\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-3 , April 2020, URL: https://www.ijtsrd.com/papers/ijtsrd30220.pdf',\n",
       " 'Augmented reality is an innovation which allows a user to the computer simulated environment, regardless of whether that condition is a reproduction of this present reality or a conjured up universe. It is the way to encountering, feeling and contacting the past, present and whats to come. It is the medium of making our very own reality, our own customized reality. It could go from making a computer game to having a virtual walk around the universe, from strolling through our very own fantasy house to encountering a stroll on an outsider planet. With computer generated reality, we can encounter the scariest and exhausting circumstances by playing safe and with a learning point of view. Not many individuals, be that as it may, truly realize what VR is, the thing that it is fundamental standards and its open issues are. In this paper a chronicled outline of computer generated reality is displayed, essential wording and classes of VR frameworks are recorded. A savvy investigation of normal VR frameworks is done and finds the difficulties of Virtual Reality. Augmented reality, in which virtual substance is reliably planned with grandstands of genuine scenes, is a creating zone of natural arrangement. With the climb of individual cellphones prepared for making charming augmented reality conditions, the tremendous ability of AR has begun to be examined. This paper audits the present forefront in expanded reality. It delineates work performed in different application territories and clears up the leaving issues experienced when building extended reality applications considering the ergonomic besides, specialized confinements of cell phones. Pratibha Jha | Sapna Yadav \"Virtual and Augmented Reality: An Overview\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-3 , April 2019, URL: https://www.ijtsrd.com/papers/ijtsrd23351.pdf',\n",
       " 'Computerized applications are used at greater extent that helps the training in medical field. Augmented reality applications possess an interactive virtual layer on top of reality. The use of augmented reality applications acts as a boon to medical education because they combine digital parts with the practical learning environment. The aim of this research is to investigate the scope of augmented reality applications in medical professionals training 1 This technology is different from virtual reality, in which the user is immersed in a virtual world generated by the computer. The AR system brings the computer into the user world by augmenting the real environment with virtual objects. 2 In Augmented Reality, physical and artificial objects are mixed together in a hybrid space where the user can move without constraints. This paper aims to provide information of current technologies and benefits of augmented reality and to describe the benefits and open issues. 3 Vaishnavi D. Deolekar | Pratibha M. Deshmukh\"Case Study of Augmented Reality Applications in Medical Field\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-4 , June 2018, URL: http://www.ijtsrd.com/papers/ijtsrd15714.pdf http://www.ijtsrd.com/medicine/other/15714/case-study-of-augmented-reality-applications-in-medical-field/vaishnavi-d-deolekar',\n",
       " 'Ubiquitous augmented reality is an emerging human-computer interaction technology, aris- ing from the convergence of augmented reality and ubiquitous computing. Augmented reality allows interaction with virtual objects spatially registered in the user’s real environment, in order to provide information, facilitate collaboration and control machines. As the comput- ing and interaction devices necessary for augmented reality become ubiquitously available, opportunities arise for improved interaction and new applications. Building ubiquitous augmented reality systems presents three software engineering chal- lenges. First, the system must cope with uncertainty regarding the software components; the users’ mobility changes the availability of distributed devices. Second, during development, the system’s desired behavior is ill-defined, as appropriate interaction metaphors are still be- ing researched and users’ preferences change. Third, the system must maintain near-real-time performance to create a convincing user experience. This dissertation presents a new architectural style, the adaptive service dependency architec- ture, to address these challenges. The architecture deals with component uncertainty, using middleware for decentralized ser- vice management to dynamically adapt the system’s structure. It builds on existing architec- tural approaches, such as loosely coupled services, service discovery, reflection, and data flow architectures, but additionally considers the interdependencies between distributed services and uses them for system adaption. With the development technique of design at run time, users and developers address ill- defined requirements by changing the system’s behavior while it is running, facilitating the creation of new applications. This is based on existing agile development methods, but takes additional advantage of the architecture’s adaptive and reflective properties. Distributed middleware maintains the required performance by decoupling multimedia data flow from system reconfiguration. It combines existing communication and service discovery technologies, but additionally deals with the distributed coordination of networks of interde- pendent services. Several prototype systems for real-world ubiquitous augmented reality applications have been built using the adaptive service dependency architecture, decentralized middleware, and design at run time. Feedback from users and developers, as well as performance tests, show the concepts’ usefulness for building ubiquitous augmented reality systems.',\n",
       " 'Augmented Reality (AR, dt. erweiterte Realität) ist eine noch wenig verbreitete neue Technologie, die die reale Umwelt mit computergenerierten Informationen ergänzen oder bereichern (augmentieren) kann. Die Informationen werden dem Benutzer bedarfsgerecht und kontextbezogen dargestellt und können interaktiv genutzt und weiterverarbeitet werden. Ebenso wie bei Virtual Reality-Systemen (VR, dt. virtuelle Realität) ist es für eine intuitive Nutzbarkeit dieser Funktionalität erforderlich, diese neue Technologie aus zwei Perspektiven zu gestalten. Aus technozentrierter Sicht betrifft dies vor allem die Entwicklung einer Systemplattform für die Funktionalität der Hard- und Software. Aus benutzerzentrierter Perspektive sind neben ergonomischen Fragen der Informationsdarstellung und visuellen Wahrnehmung des Menschen, die Gestaltung der Benutzungsschnittstellen und systemergonomische Fragestellungen der Nutzung dieser neuen Technologie in den unterschiedlichen Anwendungsfeldern und Arbeitskontexten unter Einbezug der zukünftigen Benutzer und ihrer Anforderungen zu berücksichtigen. Dieser Beitrag gibt einen Überblick zu Augmented Reality als neue Form der MenschTechnik-lnteraktion.',\n",
       " 'Any significant real-world application of mobile augmented reality will require a large model of location-bound data. While it may appear that a natural approach is to develop application-specific data formats and management strategies, we have found that such an approach actually prevents reuse of the data and ultimately produces additional complexity in developing the application. In contrast we describe a three-tier architecture to manage a common data model for a set of applications. It is inspired by current Internet application frameworks and consists of a central storage layer using a common data model, a transformation layer responsible for filtering and adapting the data to the requirements of a particular applications on request, and finally of the applications itself. We demonstrate our architecture in a scenario consisting of two multi-user capable mobile AR applications for collaborative navigation and annotation in a city environment.',\n",
       " 'Neuronavigation has become an essential neurosurgical tool in pursuing minimal invasiveness and maximal safety, even though it has several technical limitations. Augmented reality (AR) neuronavigation is a significant advance, providing a real-time updated 3D virtual model of anatomical details, overlaid on the real surgical field. Currently, only a few AR systems have been tested in a clinical setting. The aim is to review such devices. We performed a PubMed search of reports restricted to human studies of in vivo applications of AR in any neurosurgical procedure using the search terms Augmented reality and Neurosurgery. Eligibility assessment was performed independently by two reviewers in an unblinded standardized manner. The systems were qualitatively evaluated on the basis of the following: neurosurgical subspecialty of application, pathology of treated lesions and lesion locations, real data source, virtual data source, tracking modality, registration technique, visualization processing, display type, and perception location. Eighteen studies were included during the period 1996 to September 30, 2015. The AR systems were grouped by the real data source: microscope (8), hand- or head-held cameras (4), direct patient view (2), endoscope (1), and X-ray fluoroscopy (1) head-mounted display (1). A total of 195 lesions were treated: 75 (38.46 \\\\%) were neoplastic, 77 (39.48 \\\\%) neurovascular, and 1 (0.51 \\\\%) hydrocephalus, and 42 (21.53 \\\\%) were undetermined. Current literature confirms that AR is a reliable and versatile tool when performing minimally invasive approaches in a wide range of neurosurgical diseases, although prospective randomized studies are not yet available and technical improvements are needed.',\n",
       " 'Recent user interface concepts, such as multimedia, multimodal, wearable, ubiquitous, tangible, or augmented-reality-based (AR) interfaces, each cover different approaches that are all needed to support complex human&#x2013;computer interaction. Increasingly, an overarching approach towards building what we call ubiquitous augmented reality (UAR) user interfaces that include all of the just mentioned concepts will be required. To this end, we present a user interface architecture that can form a sound basis for combining several of these concepts into complex systems. We explain in this paper the fundamentals of DWARF&#x2019;s user interface framework (DWARF standing for distributed wearable augmented reality framework) and an implementation of this architecture. Finally, we present several examples that show how the framework can form the basis of prototypical applications.',\n",
       " 'A useful function of augmented reality (AR) systems is their ability to visualize occluded infrastructure directly in a user’s view of the environment. This is especially important for our application context, which utilizes mobile AR for navigation and other operations in an urban environment. A key problem in the AR field is how to best depict occluded objects in such a way that the viewer can correctly infer the depth relationships between different physical and virtual objects. Showing a single occluded object with no depth context presents an ambiguous picture to the user. But showing all occluded objects in the environments leads to the “Superman’s X-ray vision ” problem, in which the user sees too much information to make sense of the depth relationships of objects. Our efforts differ qualitatively from previous work in AR occlusion, because our application domain involves farfield occluded objects, which are tens of meters distant from the user. Previous work has focused on near-field occluded objects, which are within or just beyond arm’s reach, and which use different perceptual cues. We designed and evaluated a number of sets of display attributes. We then conducted a user study to determine which representations best express occlusion relationships among far-field objects. We identify a drawing style and opacity settings that enable the user to accurately interpret three layers of occluded objects, even in the absence of perspective constraints. 1',\n",
       " 'The development of mobile Augmented Reality application became increasingly popular over the last few years. However, many of the existing solutions build on the reuse of available standard metaphors for visualization and interaction without considering the manifold contextual factors of their use. Within this workshop we want to discuss theoretical design approaches and practical tools which should help developers to make more informed choices when exploring the design space of Augmented Reality interfaces in mobile contexts.',\n",
       " 'In 1997, Azuma published a survey on augmented reality (AR). Our goal is to complement, rather than replace, the original survey by presenting representative examples of the new advances. We refer one to the original survey for descriptions of potential applications (such as medical visualization, maintenance and repair of complex equipment, annotation, and path planning); summaries of AR system characteristics (such as the advantages and disadvantages of optical and video approaches to blending virtual and real, problems in display focus and contrast, and system portability); and an introduction to the crucial problem of registration, including sources of registration error and error-reduction strategies.',\n",
       " 'Current technology and trend have made designing and implementation of embedded systems for home automation has increased the capabilities and features. There is a demand for smart home automation access via virtual reality. The proposed system allows the owner to access via virtual reality and controls the home appliances. This is very easy to access. In this paper, we present the design and implementation of a low cost but yet flexible, feasible and secure virtual reality based home automation system. In our proposed system we will try to control light, motor and 230v supply. This work also demonstrates the use of virtual reality in the context of the appliance control. In this project we have used image processing techniques for image acquisition, pre processing, image segmentation and feature extraction. The user commands are captured by the camera and transmitted through wireless communication to the receiver end. Thus our project has a huge scope for training and development business using virtual image and setting up a virtual environment. Devi. M | Renukasrinidhi. N | Swarnamaalika. N | Mrs. P. RekhaÄn intelligent device control system using augmented reality and zigbee technology\" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: http://www.ijtsrd.com/papers/ijtsrd11435.pdf http://www.ijtsrd.com/engineering/computer-engineering/11435/an-intelligent-device-control-system-using-augmented-reality-and-zigbee-technology/devi-m',\n",
       " 'Für industrielle Arbeitsprozesse in Entwicklung, Produktion und Service wurden im BMBF geförderten Mensch-Technik-Interaktions-Leitprojekt ARVIKA Augmented-Reality-Technologien benutzerorientiert und anwendungsgetrieben erforscht. Charakteristisch für Augmented Reality (AR) ist die Anreicherung des Sichtfeldes des Betrachters mit rechnergenerierten virtuellen Objekten, um damit Produkt oder Prozessinformationen situationsgerecht im Kontext zur wahrgenommenen Realität intuitiv nutzbar zu machen. Diese innovative Art der Mensch-Technik-Interaktion erfordert neben der anwendungsorientierten Entwicklung der AR-Basistechnologien insbesondere die Berücksichtigung von Benutzeranforderungen bei der ergonomischen Gestaltung von AR-Systemen. Die Projektideen wurden in für die deutsche Industrie relevanten Anwendungsfeldern wie Automobil- und Flugzeugbau, Maschinen- und Anlagenbau realisiert. Eine besondere Perspektive war damit für mittelständische Unternehmen verbunden, die durch verbesserte Diagnose- und Wartungskompetenz flexibler und effizienter agieren können und damit eine Stärkung im globalen Wettbewerb erfahren. Konkret erfolgte im Rahmen von ARVIKA die Konzentration bei der Entwicklung auf das Automobil, bei der Produktion auf das Automobil und Flugzeug und beim Service auf die Kraftwerksanlagentechnik sowie Werkzeug- und Produktionsmaschinen. Dadurch wurden wesentliche industrielle Einsatzfälle von AR abgedeckt und die notwendige anwendungsorientierte Tiefe zur Verifikation dieser neuartigen Technik erreicht. Diese inhaltlichen Schwerpunkte waren in einen Prozess der benutzerzentrierten Systemgestaltung eingebettet, die das Projekt in allen Phasen begleitet und sich auf arbeitswissenschaftliche Methoden stützt. Durch die Einbeziehung potentieller zukünftiger Benutzer wurde sichergestellt, dass die entwickelten AR-Systeme den Anforderungen von Mitarbeitern, Arbeitsprozessen und Märkten entsprechen, die Benutzungsschnittstellen ergonomisch gestaltet wurden und Verbesserungen der Arbeitsorganisation bei den Projektpartnern durch AR-Funktionalitäten in den prototypischen Umsetzungen nachgewiesen werden konnten und somit auch ein Beleg der wirtschaftlichen Effekte stattfand.',\n",
       " \"Color blindness is a highly prevalent vision impairment that inhibits people's ability to understand colors. Although classified as a mild disability, color blindness has important effects on the daily activity of people, preventing them from performing their tasks in the most natural and effective ways. In order to address this issue we developed Chroma, a wearable augmented-reality system based on Google Glass that allows users to see a filtered image of the current scene in real-time. Chroma automatically adapts the scene-view based on the type of color blindness, and features dedicated algorithms for color saliency. Based on interviews with 23 people with color blindness we implemented four modes to help colorblind individuals distinguish colors they usually can't see. Although Glass still has important limitations, initial tests of Chroma in the lab show that colorblind individuals using Chroma can improve their color recognition in a variety of real-world activities. The deployment of Chroma on a wearable augmented-reality device makes it an effective digital aid with the potential to augment everyday activities, effectively providing access to different color dimensions for colorblind people.\",\n",
       " 'One of the unique applications of Mixed and Augmented Reality (MR/AR) systems is that hidden and occluded objects can be readily visualized. We call this specialized use of MR/AR, Obscured Information Visualization (0I D. In this paper, we describe the beginning of a research program designed to develop such visualizations through the use of principles derived from perceptual psychology and cognitive science. In this paper we surveyed the cognitive science literature as it applies to such visualization tasks, described experimental questions derived from these cognitive principles, and generated general guidelines that can be used in designing future OIV systems (as well improving AR displays more generally). Here we also report the results from an experiment that utilized a functioning AR-OIV system: we found that in a relative depth judgment, subjects reported rendered objects as being in front of real-world objects, except when additional occlusion and motion cues were presented together.',\n",
       " 'In this paper we report on an initial survey of user evaluation techniques used in Augmented Reality (AR) research. To identify all papers which include AR evaluations we reviewed research publications between the years 1993 and 2007 from online databases of selected scientific publishers. Starting with a total of 6071 publications we filtered the articles in several steps which resulted in 165 AR related publications with user evaluations. These publications were classified in two different ways: according to the evaluation type used following an earlier literature survey classification scheme; and according to the evaluation methods or approach used. We present the results of out literature survey, provide a comprehensive list of references of the selected publications, and discuss some possible research opportunities for future work.',\n",
       " 'Tracking ist wohl eines der grössten Probleme in Augmented Reality aktuell. Es wurden viele verschiedene Möglichkeiten dafür vorgeschlagen, aber ich werde mich hier hauptsächlich auf Marker-basiertes Tracking beschränken und da zuerst ein bisschen wie es funktioniert, danach was ein guter Marker haben soll und schliesslich ein Ausblick auf die Möglichkeiten von AR an einem Beispiel das auf Markerbasierten Tracking beruht.',\n",
       " 'Neuronavigation has become an essential neurosurgical tool in pursuing minimal invasiveness and maximal safety, even though it has several technical limitations. Augmented reality (AR) neuronavigation is a significant advance, providing a real-time updated 3D virtual model of anatomical details, overlaid on the real surgical field. Currently, only a few AR systems have been tested in a clinical setting. The aim is to review such devices. We performed a PubMed search of reports restricted to human studies of in vivo applications of AR in any neurosurgical procedure using the search terms Augmented reality and Neurosurgery. Eligibility assessment was performed independently by two reviewers in an unblinded standardized manner. The systems were qualitatively evaluated on the basis of the following: neurosurgical subspecialty of application, pathology of treated lesions and lesion locations, real data source, virtual data source, tracking modality, registration technique, visualization processing, display type, and perception location. Eighteen studies were included during the period 1996 to September 30, 2015. The AR systems were grouped by the real data source: microscope (8), hand- or head-held cameras (4), direct patient view (2), endoscope (1), and X-ray fluoroscopy (1) head-mounted display (1). A total of 195 lesions were treated: 75 (38.46 \\\\%) were neoplastic, 77 (39.48 \\\\%) neurovascular, and 1 (0.51 \\\\%) hydrocephalus, and 42 (21.53 \\\\%) were undetermined. Current literature confirms that AR is a reliable and versatile tool when performing minimally invasive approaches in a wide range of neurosurgical diseases, although prospective randomized studies are not yet available and technical improvements are needed.',\n",
       " 'AR&#039;s growth and prog reg have be1 re1T kable In the late 1990s,se e ral confenffi62 onARbe09I including the Inte national Workshop and Symposium onAugmeI e Re6I93 , the Inte national Symposiumon Mixe Re9797 , and the DeTE30fiz Augme2 e Re0939 Environmez9 workshop.Some we719Ifiz9T organizations forme that focuse on AR, notably the Mixe Re06 ity Systefi Lab in Japan and the Arvika consortium in Ge many. A software toolkit (the ARToolkit) for rapidly building AR applications is nowfre0 y available at http:// www.hitl.washington.etfi6TTI0fiz h/shareTTI0fizT3 Be cause of the we61 h ofne wde e6321fizT0 thisfie0 ne0I an update surve y toguide andedfi629I e furthe re se9T h in this e citingaren Our goal hel is to comple6627 rat thanrefi0T31 the original surve y by pre9667fiz re9667fiz ative eiv ple of the ne wadvancez We re e you to the original surve forde93 iptions of pote6TE2 applications (such asme0T2T visualization, mainteintfi andrefi63 of complee uipme -fi annotation, and path plann',\n",
       " 'This paper presents and evaluates a set of pictorial depth cues for far-field outdoor mobile augmented reality (AR). We examine the problem of accurately placing virtual annotations at physical target points from a static point of view. While it is easy to line up annotations with a target point&#039;s projection in the view plane, finding the correct distance for the annotation is difficult if the target point is not represented in an environment model. We have found that AR depth cues, such as vertical and horizontal shadow planes, a small top-down map, or color encodings of relative depth, have a positive impact on a user&#039;s ability to align a 3D cursor with physical objects at various distances. These cues aid the user&#039;s depth perception and estimation by providing information about the 3D cursor&#039;s distance and its relationship in 3-space to any features that may already have been annotated. We conducted a user study that measures the effects of different depth cues for both absolute 3D cursor placement as well as placement relative to a small number of marked reference points, whose distances are known. Our study provides insight about mobile AR users &#039; ability to judge distances both absolutely and relatively, and we identify techniques that successfully enhance their performance. 1.',\n",
       " \"With the explosive growth in mobile phone usage and rapid rise in search engine technologies over the last decade, augmented reality (AR) is poised to be one of this decade's most disruptive technologies, as the information that is constantly flowing around us is brought into view, in real-time, through augmented reality. In this cutting-edge book, the authors outline and discuss never-before-published information about augmented reality and its capabilities. With coverage of mobile, desktop, developers, security, challenges, and gaming,this book gives you a comprehensive understanding of what augmented reality is, what it can do, what is in store for the future and most importantly: how to benefit from using AR in our lives and careers.\",\n",
       " \"Understanding Augmented Reality addresses the elements that are required to create augmented reality experiences. The technology that supports augmented reality will come and go, evolve and change. The underlying principles for creating exciting, useful augmented reality experiences are timeless. Augmented reality designed from a purely technological perspective will lead to an AR experience that is novel and fun for one-time consumption - but is no more than a toy. Imagine a filmmaking book that discussed cameras and special effects software, but ignored cinematography and storytelling! In order to create compelling augmented reality experiences that stand the test of time and cause the participant in the AR experience to focus on the content of the experience - rather than the technology - one must consider how to maximally exploit the affordances of the medium. This book addresses core conceptual issues regarding the medium of augmented reality as well as the technology required to support compelling augmented reality. By addressing AR as a medium at the conceptual level in addition to the technological level, the reader will learn to conceive of AR applications that are not limited by today's technology. At the same time, ample examples are provided that show what is possible with current technology.\",\n",
       " 'What are the consequences of mobility for augmented reality? This paper explores some of the issues that I believe will be raised by the development and future commonplace adoption of mobile, wearable, augmented reality systems. These include: social influences on tracking accuracy, the importance of appearance and comfort, an increase in collaborative applications, integration with other devices, and implications for personal privacy',\n",
       " 'In this paper, we propose the use of specific system architecture, based on mobile device, for navigation in urban environments. The aim of this work is to assess how virtual and augmented reality interface paradigms can provide enhanced location based services using real-time techniques in the context of these two different technologies. The virtual reality interface is based on faithful graphical representation of the localities of interest, coupled with sensory information on the location and orientation of the user, while the augmented reality interface uses computer vision techniques to capture patterns from the real environment and overlay additional way-finding information, aligned with real imagery, in real-time. The knowledge obtained from the evaluation of the virtual reality navigational experience has been used to inform the design of the augmented reality interface. Initial results of the user testing of the experimental augmented reality system for navigation are presented.',\n",
       " 'Augmented Reality (AR)-systems systems are often refused by potential users even if their technical performance is good. Their problem is that they are not designed in a user-friendly way. The experience made with an AR-supported assembly scenario has shown that workers have appreciated the AR-supported assembly when some basic usability aspects have been taken into account.',\n",
       " \"This paper surveys the current state-of-the-art of technology, systems and applications in Augmented Reality. It describes work performed by many different research groups, the purpose behind each new Augmented Reality system, and the difficulties and problems encountered when building some Augmented Reality applications. It surveys mobile augmented reality systems challenges and requirements for successful mobile systems. This paper summarizes the current applications of Augmented Reality and speculates on future applications and where current research will lead Augmented Reality's development. Challenges augmented reality is facing in each of these applications to go from the laboratories to the industry, as well as the future challenges we can forecast are also discussed in this paper. Section 1 gives an introduction to what Augmented Reality is and the motivations for developing this technology. Section 2 discusses Augmented Reality Technologies with computer vision methods, AR devices, interfaces and systems, and visualization tools. The mobile and wireless systems for Augmented Reality are discussed in Section 3. Four classes of current applications that have been explored are described in Section 4. These applications were chosen as they are the most famous type of applications encountered when researching AR apps. The future of augmented reality and the challenges they will be facing are discussed in Section 5.\",\n",
       " 'this paper first appeared in 1 + S. Julier, Y. Baillot and D. Brown are with ITT AES / Virtual Reality Laboratory, Naval Research Laboratory, Washington DC. M. Lanzagorta is with Scientific and Engineering Solutions',\n",
       " 'We present EMMIE (Environment Management for Multi-user Information Environments), a prototype experimental user interface to a collaborative augmented environment. Users share a 3D virtual space and manipulate virtual objects that represent information to be discussed. We refer to EMMIE as a hybrid user interface because it combines a variety of different technologies and techniques, including virtual elements such as 3D widgets, and physical objects such as tracked displays and input devices. See-through head worn displays overlay the virtual environment on the physical environment, visualizing the pervasive \"virtual ether\" within which all interaction occurs.Our prototype includes additional 2D and 3D displays, ranging from palm-sized to wall-sized, allowing the most appropriate one to be used for any task. Objects can be moved among displays (including across dimensionalities) through drag & drop.In analogy to 2D window managers, we describe a prototype implementation of a shared 3D environment manager that is distributed across displays, machines, and operating systems. We also discuss two methods we are exploring for handling information privacy in such an environment.',\n",
       " 'Dieser Beitrag stellt einen Augmented-Reality-Ansatz zur Unterstützung von Montageprozessen in der Produktion vor. Neben einem Überblick zu den damit zusammenhängenden Gestaltungsbereichen für die Mensch-Technik-Interaktion wird ein an ISO 13407 angelehntes Vorgehen vorgestellt, um die benutzerzentrierte Gestaltung der Technik zu gewährleisten. Als konkreter Anwendungsfall aus der Flugzeugindustrie wird die prototypische Umsetzung eines Augmented-Reality-Systems zur Unterstützung der Kabelbaumkonfektionierung näher beschrieben, das einer Evaluation mit den potenziellen Nutzern unterzogen wurde.',\n",
       " 'Diese Arbeit verbindet Techniken und Vorgehensweisen aus den Bereichen Location Based Services und Augmented Reality um ein Konzept zu entwickeln, welches die soziale Interaktion und somit die Zusammenarbeit zwischen Personen fördern soll. Hierbei wird die These vertreten, dass ein gemeinsamer örtlicher Kontext eine wichtige Basis für die soziale Interaktion ist, die das Internet zur Zeit nur unzureichend abbilden kann. Im Rahmen dieser Arbeit wurde eine Applikation für die Nutzung auf Android Smartphones entwickelt, anhand derer das entwickelte Konzept erfahrbar gemacht und evaluiert werden soll.',\n",
       " 'We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible. We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to',\n",
       " 'Augmented Reality (AR) is a form of human-machine interaction where information is presented in the field of view of an individual e. g. through a head mounted display, thus augmenting his or her perception of reality. This occurs in a context-sensitive manner, i. e. in accordance with and derived from the observed object, such as a part, tool or machine, or his or her location. In this way, the real-world field of view of a skilled worker, technician or design engineer is augmented with superimposed notes to present information that is relevant to this individual. ARVIKA develops this technology or applications in the fields of design, production, and service in the automotive and aerospace industries, for power and processing plants and for machine tools and production machinery. This technology offers special dimensions for mid-sized enterprises that can leverage an improved diagnostic and maintenance competence to increase their flexibility and efficiency, thereby strengthening their global competitive position. ARVIKA is primarily designed to implement an Augmented Reality system for mobile use in industrial applications. This article presents the milestones that have been achieved after a project duration of a full two years: A basic system for Augmented Reality systems was developed based on a specially designed software architecture. Initial application-specific prototypes have been created on this basis and have been evaluated in usability tests. The solutions that can be demonstrated today indicate a good progress of the project in terms of the basic technologies such as Augmented Reality (marker-based tracking), human machine interaction as well as the AR-oriented information delivery and workflow, but they also show the challenges for further development. Significant improvements that need to be achieved during the second phase of the project include markerless tracking, the engineering e. g. of product models and AR scenes and the availability of documents that can be used in AR.',\n",
       " 'Augmented Reality (AR) is a form of human-machine interaction where information is presented in the field of view of an individual e. g. through a head mounted display, thus augmenting his or her perception of reality. This occurs in a context-sensitive manner, i. e. in accordance with and derived from the observed object, such as a part, tool or machine, or his or her location. In this way, the real-world field of view of a skilled worker, technician or design engineer is augmented with superimposed notes to present information that is relevant to this individual. ARVIKA develops this technology or applications in the fields of design, production, and service in the automotive and aerospace industries, for power and processing plants and for machine tools and production machinery. This technology offers special dimensions for mid-sized enterprises that can leverage an improved diagnostic and maintenance competence to increase their flexibility and efficiency, thereby strengthening their global competitive position. ARVIKA is primarily designed to implement an Augmented Reality system for mobile use in industrial applications. This article presents the milestones that have been achieved after a project duration of a full two years: A basic system for Augmented Reality systems was developed based on a specially designed software architecture. Initial application-specific prototypes have been created on this basis and have been evaluated in usability tests. The solutions that can be demonstrated today indicate a good progress of the project in terms of the basic technologies such as Augmented Reality (marker-based tracking), human machine interaction as well as the AR-oriented information delivery and workflow, but they also show the challenges for further development. Significant improvements that need to be achieved during the second phase of the project include markerless tracking, the engineering e. g. of product models and AR scenes and the availability of documents that can be used in AR.',\n",
       " 'In this paper we report on an initial survey of user evaluation techniques used in Augmented Reality (AR) research. To identify all papers which include AR evaluations we reviewed research publications between the years 1993 and 2007 from online databases of selected scientific publishers. Starting with a total of 6071 publications we filtered the articles in several steps which resulted in 165 AR related publications with user evaluations. These publications were classified in two different ways: according to the evaluation type used following an earlier literature survey classification scheme; and according to the evaluation methods or approach used. We present the results of out literature survey, provide a comprehensive list of references of the selected publications, and discuss some possible research opportunities for future work.',\n",
       " 'Augmented Reality (AR) can naturally complement mobile computing on wearable devices by providing an intuitive interface to a three-dimensional information space embedded within physical reality. However, existing AR systems like MARS 1 or Tinmith 2, which require a user to wear a notebook computer in a backpack and a head-mounted display (HMD) are expensive, fragile and inconvenient to wear. Thin-client approaches using a Tablet PC or Personal Digital Assistant (PDA) merely as a portable display 34 require a dedicated server infrastructure and limit mobility. We believe there is a need for an unconstrained, infrastructureindependent AR display running to fill the gap in situations where traditional backpack systems are too costly and cumbersome, but thin client implementations exhibit inadequate deployability, scalability or interactive behavior. Particular examples include sporadic use over lengthy time spans, in between which devices must be stowed away, mixed indoor/outdoor use in wide-area environments, and massively multi-user application scenarios. This has motivated us to develop a state of the art AR framework targeting lightweight handheld displays.',\n",
       " 'The article describes the development and ergonomic evaluation of an augmented reality (AR) welding helmet. The system provides an augmented user interface with supporting information relevant to the welding process. The experimental studies focused on hand-eye coordination of welders and nonwelders with two prototypes of the augmented reality welding helmet. The first prototype operated at 16 frames per second, whereas the second, improved version had 20 frames per second. In addition, the hand-eye coordination while wearing the welding helmet with video see-through head-mounted display was compared to a performance with natural vision, without any helmet. Experimental results showed significant influence of helmet and occupation on hand-eye coordination. Subjective assessment revealed better rating for stereo perception for the system with the higher frame rate, whereas no significant difference in performance was found between the two frame rates.',\n",
       " 'Das Thema Augmented Reality kann insbesondere vor dem Hintergrund der benutzerzentrierten Gestaltung fruchtbar gemacht werden: Beginnend beim ersten großen deutschen Mensch-Technik-Interaktions-Leitprojekt ÄRVIKA - Augmented Reality für Entwicklung, Produktion und Service\", über Projekte zur Unterstützung von Schweißprozessen im Schiffsbau und Anwendungen in der Medizintechnik, ergonomische Untersuchungen zu Head-Mounted Displays bis hin zur Überwachung technischer Anlagen mit mobilen Inspektionsrobotern stehen dabei methodisch stets die Benutzer und ihre Aufgaben im Zentrum der Gestaltung. Der Vortrag umfasst einen Querschnitt aus verschiedenen Projekten und stellt in diesem Kontext Methoden zur benutzerzentrierten Gestaltung von Augmented-Reality-Systemen vor.',\n",
       " 'As augmented reality and a gravity sensor is of growing interest, siginificant developement is being made on related technology, which allows application of the technology in a variety of areas with greater expectations. In applying Context-aware to augmented reality, it can make useful programs. A traning system suggested in this study helps a user to understand an effcienct training method using augmented reality and make sure if his exercise is being done propery based on the data collected by a gravity sensor. Therefore, this research aims to suggest an efficient training environment that can enhance previous training methods by applying augmented reality and a gravity sensor.',\n",
       " \"The computational capability of mobile phones has been rapidly increasing, to the point where augmented reality has become feasible on cell phones. We present an approach to indoor localization and pose estimation in order to support augmented reality applications on a mobile phone platform. Using the embedded camera, the application localizes the device in a familiar environment and determines its orientation. Once the 6 DOF pose is determined, 3D virtual objects from a database can be projected into the image and displayed for the mobile user. Off-line data acquisition consists of acquiring images at different locations in the environment. The online pose estimation is done by a feature-based matching between the cell phone image and an image selected from the precomputed database using the phone's sensors (accelerometer and magnetometer). The application enables the user both to visualize virtual objects in the camera image and to localize the user in a familiar environment. We describe in detail the process of building the database and the pose estimation algorithm used on the mobile phone. We evaluate the algorithm performance as well as its accuracy in terms of reprojection distance of the 3D virtual objects in the cell phone image.\",\n",
       " \"The computational capability of mobile phones has been rapidly increasing, to the point where augmented reality has become feasible on cell phones. We present an approach to indoor localization and pose estimation in order to support augmented reality applications on a mobile phone platform. Using the embedded camera, the application localizes the device in a familiar environment and determines its orientation. Once the 6 DOF pose is determined, 3D virtual objects from a database can be projected into the image and displayed for the mobile user. Off-line data acquisition consists of acquiring images at different locations in the environment. The online pose estimation is done by a feature-based matching between the cell phone image and an image selected from the precomputed database using the phone's sensors (accelerometer and magnetometer). The application enables the user both to visualize virtual objects in the camera image and to localize the user in a familiar environment. We describe in detail the process of building the database and the pose estimation algorithm used on the mobile phone. We evaluate the algorithm performance as well as its accuracy in terms of reprojection distance of the 3D virtual objects in the cell phone image.\",\n",
       " 'Computing paradigm is moving toward context-aware and ubiquitous computing in which devices, software agents, and services are all expected to seamlessly integrate and cooperate in support of human objectives. Augmented reality (AR) can naturally complement ubiquitous computing by providing an intuitive and collaborative interface to a three-dimensional information space embedded within physical reality. This paper presents a framework and its applications for the convergence of context-awareness and augmented reality, which can support a rich set of ubiquitous services and immersive interactions. The framework provides a common data model for different types of context information from external sensors, applications and users. It also offers the software framework to acquire, interpret and disseminate context information. Further, it utilizes augmented reality for providing immersive interactions by embedding virtual models onto physical models, which realizes bi-augmentation between physical and virtual spaces.',\n",
       " 'In this paper, the authors examine the state of the art in augmented reality (AR) for mobile learning. Previous work in the field of mobile learning has included AR as a component of a wider toolkit but little has been done to discuss the phenomenon in detail or to examine in a balanced fashion its potential for learning, identifying both positive and negative aspects. The authors seek to provide a working definition of AR and to examine how it can be embedded within situated learning in outdoor settings. The authors classify it according to key aspects (device/technology, mode of interaction/learning design, type of media, personal or shared experiences, whether the experience is portable or static, and the learning activities/outcomes). The authors discuss the technical and pedagogical challenges presented by AR, before looking at ways in which it can be used for learning. Finally, the paper looks ahead to AR technologies that may be employed in the future.',\n",
       " 'In this paper, the authors examine the state of the art in augmented reality (AR) for mobile learning. Previous work in the field of mobile learning has included AR as a component of a wider toolkit but little has been done to discuss the phenomenon in detail or to examine in a balanced fashion its potential for learning, identifying both positive and negative aspects. The authors seek to provide a working definition of AR and to examine how it can be embedded within situated learning in outdoor settings. The authors classify it according to key aspects (device/technology, mode of interaction/learning design, type of media, personal or shared experiences, whether the experience is portable or static, and the learning activities/outcomes). The authors discuss the technical and pedagogical challenges presented by AR, before looking at ways in which it can be used for learning. Finally, the paper looks ahead to AR technologies that may be employed in the future.',\n",
       " 'Computing paradigm is moving toward context-aware and ubiquitous computing in which devices, software agents, and services are all expected to seamlessly integrate and cooperate in support of human objectives. Augmented reality (AR) can naturally complement ubiquitous computing by providing an intuitive and collaborative interface to a three-dimensional information space embedded within physical reality. This paper presents a framework and its applications for the convergence of context-awareness and augmented reality, which can support a rich set of ubiquitous services and immersive interactions. The framework provides a common data model for different types of context information from external sensors, applications and users. It also offers the software framework to acquire, interpret and disseminate context information. Further, it utilizes augmented reality for providing immersive interactions by embedding virtual models onto physical models, which realizes bi-augmentation between physical and virtual spaces.',\n",
       " 'Die Breite der Anwendungsfelder von Augmented Reality, die gegenwärtig Gegenstand von Forschung und Entwicklung in Wissenschaft und Industrie sind, macht deutlich, dass mit dieser Technologie ein hohes Potential für eine verbesserte Effizienz in vielen Arbeitsprozessen verbunden wird. Über die Anreicherung der realen Umwelt mit computergenerierten Informationen entstehen außerdem neue Möglichkeiten der Unterstützung des Menschen mit seinem Arbeitsumfeld. Diese Informationen werden dem Benutzer bedarfsgerecht und kontextbezogen dargestellt und können interaktiv genutzt und weiterverarbeitet werden. Diese Monographie fasst aus benutzerzentrierter Gestaltungsperspektive aktuelle Ergebnisse des BMBF-Leitprojektes ARVIKA hinsichtlich grundlagenergonomischer Fragen der Displays und Informationsdarstellung, des Usability Engineerings der Benutzungsschnittstellen und systemergonomischer Fragestellungen der Nutzung dieser neuen Technologie in den unterschiedlichen Anwendungsfeldern und Arbeitskontexten zusammen.',\n",
       " 'https://www.ijtsrd.com/humanities-and-the-arts/education/50111/augmented-reality-ar-in-the-classroom/dr-bhavya-bhushan',\n",
       " 'Purpose – The purpose of this paper is to introduce mobile augmented reality applications for library uses and next generation library services.Design/methodology/approach – Examples are drawn from museum and archives informatics, computer science applied research, and computer vision research as well as original research and development work from the Undergraduate Library at the University of Illinois.Findings – Mobile augmented reality uses include augmenting physical book stacks browsing, library navigation, optical character recognition, facial recognition, and building identification mobile software for compelling library experiences.Originality/value – The paper suggests uses of mobile augmented reality applications in library settings and models a demonstration prototype interface.',\n",
       " 'Context-aware, ubiquitous computing is a vision of our future computing lifestyle in which computer systems seamlessly integrate into our everyday lives, providing services and information in anywhere and anytime fashion. Augmented reality (AR) can naturally complement ubiquitous computing by providing an intuitive and collaborative visualization and simulation interface to a three-dimensional information space embedded within physical reality. This paper presents a service framework and its applications for providing context-aware and adaptable 3D visualization and collaboration services for ubiquitous cars (U-cars) using augmented reality, which can support a rich set of ubiquitous car services and collaboration services for distributed maintenance and repair. It utilizes augmented reality for providing visual interactions by superimposing virtual models of car components or sub-assemblies onto real cars, which realizes bi-augmentation between physical and virtual models. It also offers a context processing module to acquire, interpret and disseminate context information. In particular, the context processing module considers user’s preferences for providing customer’s context adaptable services. The prototype system has been implemented to support 3D animation, text-to-speech (TTS), augmented manual, and pre- and post-augmentation services in ubiquitous car service environments.',\n",
       " \"Communication is the most useful tool to impart knowledge, understand ideas, clarify thoughts and expressions, organize plan and manage every single day-to-day activity. Although there are different modes of communication, physical barrier always affects the clarity of the message due to the absence of body language and facial expressions. These barriers are overcome by video calling, which is technically the most advance mode of communication at present. The proposed work concentrates around the concept of video calling in a more natural and seamless way using Augmented Reality (AR). AR can be helpful in giving the users an experience of physical presence in each other's environment. Our work provides an entirely new platform for video calling, wherein the users can enjoy the privilege of their own virtual space to interact with the individual's environment. Moreover, there is no limitation of sharing the same screen space. Any number of participants can be accommodated over a single conference without having to compromise the screen size.\",\n",
       " 'Accurate assessment of nutrition information is an important part in the prevention and treatment of a multitude of diseases, but remains a challenging task. We present a novel mobile augmented reality application, which assists users in the nutrition assessment of their meals. Using the realtime camera image as a guide, the user overlays a 3D form of the food. Additionally the user selects the food type. The corresponding nutrition information is automatically computed. Thus accurate volume estimation is required for accurate nutrition information assessment. This work presents an evaluation of our mobile augmented reality approaches for portion estimation and offers a comparison to conventional portion estimation approaches. The comparison is performed on the basis of a user study (n=28). The quality of nutrition assessment is measured based on the error in energy units. In the results of the evaluation one of our mobile augmented reality approaches significantly outperforms all other methods. Additionally we present results on the efficiency and effectiveness of the approaches.',\n",
       " 'Augmented reality is a significant emerging trend that is digitally enhancing our world. It superimposes sounds, videos, and graphics onto an existing environment. It is an interactive experience that brings digital content to the real world. The AR technology is useful in numerous fields including education because it enables an interactive experience with the real world. Augmented reality offers various benefits in education. It is gradually becoming the future of education. This paper provides various applications of AR technology in education.',\n",
       " 'This article deals with embodied user interfaces for handheld augmented reality games, which consist of both physical and virtual components. We have developed a number of spatial interaction techniques that optically capture the device′s movement and orientation relative to a visual marker. Such physical interactions in 3D space enable manipulative control of mobile games. In addition to acting as a physical controller that recognizes multiple game-dependent gestures, the mobile device augments the camera view with graphical overlays. We describe three game prototypes that use ubiquitous product packaging and other passive media as backgrounds for handheld augmentation. The prototypes can be realized on widely available off-the-shelf hardware and require only minimal setup and infrastructure support.',\n",
       " 'Digitization and the growing capabilities of data networks enable companies to perform tasks via remote support, which previously required service personnel to travel. But which mixed reality method leads to better results regarding human factors, grounding and per- formance criteria? This paper reports on a collaborative user study, in which a local worker is guided by a remote expert with the help of different augmented reality methods, specifically see-through HMD, spatial projection, and video-mixing tablet. The performed task is an controller exchange in a switch cabinet of an industrial robot, a task rather typical for failure detection within the field. Our study was conducted in collaboration with a technician school of which 50 technician apprentices participated in our study. Our results show clear advantages of using augmented reality (AR) versus traditional conditions (audio, video, screenshot) to enable remote support. It further gives significant indications for using a projection based AR method.',\n",
       " 'Television and movie images have been altered ever since it was technically possible. Nowadays embedding advertisements, or incorporating text and graphics in TV scenes, are common practice, but they can not be considered as integrated part of the scene. The introduction of new services for interactive augmented television is discussed in this paper. We analyse the main aspects related with the whole chain of augmented reality production. Interactivity is one of the most important added values of the digital television: This paper aims to break the model where all TV viewers receive the same final image. Thus, we introduce and discuss the new concept of interactive augmented television, i. e. real time composition of video and computer graphics - e.g. a real scene and freely selectable images or spatial rendered objects - edited and customized by the end user within the context of the user\\'s set top box and TV receiver. We demonstrate a sample application introducing \"Interactive Augmented Television\" for sport broadcasts additionally with 3D virtual objects in order to enhance or alter the presentation of the match with a new interface. We also introduce a pure virtual world where the user can select the camera position.',\n",
       " 'Digitization and the growing capabilities of data networks enable companies to perform tasks via remote support, which previously required service personnel to travel. But which mixed reality method leads to better results regarding human factors, grounding and per- formance criteria? This paper reports on a collaborative user study, in which a local worker is guided by a remote expert with the help of different augmented reality methods, specifically see-through HMD, spatial projection, and video-mixing tablet. The performed task is an controller exchange in a switch cabinet of an industrial robot, a task rather typical for failure detection within the field. Our study was conducted in collaboration with a technician school of which 50 technician apprentices participated in our study. Our results show clear advantages of using augmented reality (AR) versus traditional conditions (audio, video, screenshot) to enable remote support. It further gives significant indications for using a projection based AR method.',\n",
       " 'Augmented Reality (AR) allows the real time blending of exhibited objects with digital information such as 3D models, still images, video clips, and web pages. The blending presents exhibition visitors with a more interesting and exciting experience. This paper focuses on the creation and presentation of artworks using mobile AR technology.',\n",
       " 'This paper focuses on the distributed architecture of the collaborative augmented reality system Studierstube. The system allows multiple users to experience a shared 3D workspace populated by multiple applications using see-through head mounted displays or other presentation media such as projection systems. The system design is based on a distributed shared scene graph that alleviates the application programmer from explicitly considering distribution, and avoids a separation of graphical and application data. The idea of unifying all system data in the scene graph is taken to its logical consequence by implementing application instances as nodes in the scene graph. Through the distributed shared scene graph mechanism, consistency of scene graph replicas and the contained application nodes is assured. Multi-user 3D widgets allow concurrent interaction with minimal coordination effort from the application. Special interest is paid to migration of application nodes from host to host allowing dynamic workgroup management, such as load balancing, late joining and early exit of hosts, and some firms of ubiquitous computing',\n",
       " 'Electronic games have been used to stimulate cognitive functions such as attention, concentration and memory. This paper presents GenVirtual, which is an augmented reality musical game and is proposed to help people with learning disabilities. The intention is to help the patient in the following skills: creativity, attention, memory (storage and retrieval), planning, concentration, ready-response, hearing and visual perception, and motor coordination. The therapist has flexibility to place the musical and visual elements, allowing him to create different scenarios to each patient. GenVirtual uses Augmented Reality technology to allow people with physical disorders to interact with the game. Patients with no fingers can also play this game. GenVirtual was evaluated by a music therapist who considered it as a facilitating and motivating game to the learning process and that it has the potential to improve the life of the people with special needs.',\n",
       " 'Electronic games have been used to stimulate cognitive functions such as attention, concentration and memory. This paper presents GenVirtual, which is an augmented reality musical game and is proposed to help people with learning disabilities. The intention is to help the patient in the following skills: creativity, attention, memory (storage and retrieval), planning, concentration, ready-response, hearing and visual perception, and motor coordination. The therapist has flexibility to place the musical and visual elements, allowing him to create different scenarios to each patient. GenVirtual uses Augmented Reality technology to allow people with physical disorders to interact with the game. Patients with no fingers can also play this game. GenVirtual was evaluated by a music therapist who considered it as a facilitating and motivating game to the learning process and that it has the potential to improve the life of the people with special needs.',\n",
       " 'Accurate assessment of nutrition information is an important part in the prevention and treatment of a multitude of diseases, but remains a challenging task. We present a novel mobile augmented reality application, which assists users in the nutrition assessment of their meals. The user sketches the 3D form of the food and selects the food type. The corresponding nutrition information is automatically computed.',\n",
       " 'Augmented Reality (AR) ist eine in der praktischen Anwendung immer noch wenig verbreitete interaktive Technologie, die die reale Umwelt so mit computergenerierten Informationen bedarfsgerecht und kontextbezogen ergänzen oder bereichern (augmentieren) kann, dass für den Benutzer der Eindruck einer Verschmelzung dieser Informationen mit der wahrgenommenen Realität entsteht. Neben einem Überblick zu den damit zusammenhängenden Gestaltungsbereichen für die Mensch-Maschine-Interaktion wird ein an ISO 13407 angelehntes Vorgehen vorgestellt, das eine aufgaben- und benutzergerechte Systemgestaltung gewährleisten soll. Dieses systematische Vorgehen wird im Vortrag anhand von Anwendungsbeispielen konkretisiert. Beispielsweise wurden in der Analysephase die Anforderungen an AR-Anwendungen partizipativ und szenarienbasiert in verschiedenen Fokusgruppen erhoben, bevor sie zu Use Cases zusammengefasst wurden. Mit dieser benutzerzentrierten Perspektive werden außerdem grundlagenergonomische Fragen der Displays und Informationsdarstellung, das Usability Engineering der Benutzungsschnittstellen und systemergonomische Fragestellungen der AR-Technologienutzung in unterschiedlichen Anwendungsfeldern und Arbeitskontexten aufgegriffen.',\n",
       " 'Augmented Reality (AR) can naturally complement mobile computing on wearable devices by providing an intuitive interface to a three-dimensional information space embedded within physical reality. Unfortunately, current wearable AR systems are relatively complex, expensive, fragile and heavy, rendering them unfit for large-scale deployment involving untrained users outside constrained laboratory environments. Consequently, the scale of collaborative multi-user experiments have not yet exceeded a handful of participants. In this paper, we present a system architecture for interactive, infrastructure-independent multi-user AR applications running on off-the-shelf handheld devices. We implemented a four-user interactive game installation as an evaluation setup to encourage playful engagement of participants in a cooperative task. Over the course of five weeks, more than five thousand visitors from a wide range of professional and socio-demographic backgrounds interacted with our system at four different locations.',\n",
       " 'This paper introduces an augmented reality authoring tool that allows users to edit and publish an examination application with the Augmented Reality (AR) interface. The AR authoring system in this paper is constructed with the help of ARToolKit, a widely used open source for AR. This unique system can help users with no programming knowledge to build AR applications quickly and efficiently. 2D text, 3D content editing, and finger-based interaction are the three major components found in the AR authoring environment. AR content created by the user is stored in a separate txt file for easy sharing, modification, and reusability. A user testing of the published examination application was also conducted, and participants in the testing offered generally positive feedback.',\n",
       " 'We describe an augmented reality (AR) system that allows multiple participants to interact with 2D and 3D data using tangible user interfaces. The system features face-to-face communication, collaborative viewing and manipulation of 3D models, and seamless access to 2D desktop applications within the shared 3D space. All virtual content, including 3D models and 2D desktop windows, is attached to tracked physical objects in order to leverage the efficiencies of natural two-handed manipulation. The presence of 2D desktop space within 3D facilitates data exchange between the two realms, enables control of 3D information by 2D applications, and generally increases productivity by providing access to familiar tools. We present a general concept for a collaborative tangible AR system, including a comprehensive set of interaction techniques, a distributed hardware setup, and a component-based software architecture that can be flexibly configured using XML. We show the validity of our concept with an implementation of an application scenario from the automotive industry.',\n",
       " 'Augmented Reality (AR) ist eine junge Technologie, die es zusätzlich zur betrachteten Realität gestattet, situationsgerecht virtuelle Informationen (z.B. Produkt- oder Prozessinformationen) in das Sichtfeld des Betrachters einzublenden. Während der Evaluation eines für den Produktionsbereich entwickelten Prototyps wurde in einem Usability Test das AR-User-Interface-Konzept untersucht und dessen Selbstbeschreibungsfähigkeit festgestellt. Darüber hinaus konnten durch einbeziehung der späteren Anwender wichtige Verbesserungsvorschläge des Systems auf dem Weg zur Praxistauglichkeit erhoben werden, welche die Grundlage für die Entwicklung späterer Systeme darstellen.',\n",
       " 'This paper provides a multi-disciplinary overview of the research issues and achievements in the field of Big Data and its visualization techniques and tools. The main aim is to summarize challenges in visualization methods for existing Big Data, as well as to offer novel solutions for issues related to the current state of Big Data Visualization. This paper provides a classification of existing data types, analytical methods, visualization techniques and tools, with a particular emphasis placed on surveying the evolution of visualization methodology over the past years. Based on the results, we reveal disadvantages of existing visualization methods. Despite the technological development of the modern world, human involvement (interaction), judgment and logical thinking are necessary while working with Big Data. Therefore, the role of human perceptional limitations involving large amounts of information is evaluated. Based on the results, a non-traditional approach is proposed: we discuss how the capabilities of Augmented Reality and Virtual Reality could be applied to the field of Big Data Visualization. We discuss the promising utility of Mixed Reality technology integration with applications in Big Data Visualization. Placing the most essential data in the central area of the human visual field in Mixed Reality would allow one to obtain the presented information in a short period of time without significant data losses due to human perceptual issues. Furthermore, we discuss the impacts of new technologies, such as Virtual Reality displays and Augmented Reality helmets on the Big Data visualization as well as to the classification of the main challenges of integrating the technology.',\n",
       " 'Upon the popularity of 3C devices, the visual creatures are all around us, such the online game, touch pad, video and animation. Therefore, the text-based web page will no longer satisfy users. With the popularity of webcam, digital camera, stereoscopic glasses, or head-mounted display, the user interface becomes more visual and multi-dimensional. For the consideration of 3D and visual display in the research of web user interface design, Augmented Reality technology providing the convenient tools and impressive effects becomes the hot topic. Augmented Reality effect enables users to represent parts of the digital objects on top of the physical surroundings. The easy operation with webcam greatly improving the visual representation of web pages becomes the interest of our research. Therefore, we apply Augmented Reality technology for developing a city tour web site to collect the opinions of users. Therefore, the website stickiness is an important measurement. The major tasks of the work include the exploration of Augmented Reality technology and the evaluation of the outputs of Augmented Reality. The feedback opinions of users are valuable references for improving AR application in the work. As a result, the AR increasing the visual and interactive effects of web page encourages users to stay longer and more than 80\\\\% of users are willing to return for visiting the website soon. Moreover, several valuable conclusions about Augmented Reality technology in web user interface design are also provided for further practical references.',\n",
       " 'The range of most Virtual Reality and Augmented Reality systems is usually constrained to laboratories by the limited range of a single expensive 6DOF tracking system. Our Ubiquitous Tracking (Ubitrack) approach seeks to overcome this limitation, not by extending the range of a single tracker, but by dynamically incorporating a number of heterogeneous sensors. The Ubitrack tool set consists of Spatial Relationship Graphs (SRGs), which allow for a formal specification of tracking environments, and Spatial Relationship Patterns, which are used to derive data flow configurations by systematic analysis of the SRG. This formal framework supports a distributed software architecture, where clients efficiently produce, transform, transmit and consume tracking data, coordinated by a Ubitrack server. In this document, we discuss both theoretical and implementational aspects and illustrate the new possibilities on a scenario, where a mobile AR user is guided through a building using a diverse and changing set of sensors.',\n",
       " 'In diesem Beitrag wird die Realisierung einer Versuchsumgebung beschrieben, die bei einem guten Verhältnis von Kosten, Aufwand und Genauigkeit eine Indoorlokalisierung mit Bluetooth-Low-Energy-Beacons ermöglicht. Es wird eine prototypische Indoornavigation mit einer Durchsichtdatenbrille vorgestellt, die dem Nutzer kontaktanaloge Navigationsanweisungen in 3-D entlang seines Zielpfades einblendet. Eine Nutzerevaluation zeigt, dass insbesondere die händefreie Interaktion als nützlich angesehen wird und die Gestaltung einer Augmented-Reality-Indoornavigation Ungenauigkeiten bei der Lokalisierung besonders berücksichtigen sollte, wenn relativ enge Wege genutzt werden.',\n",
       " 'Numerous experimental studies on the design of user interfaces of common desktop monitors exist, whereas empirical studies concerning the design of an Augmented Reality user interface are unknown. Therefore, recommendations relating to the optimal representation size of virtual information for an head mounted display with see through mode, which is used for Augmented Reality, cannot be given at the moment. In order to apply this new technology successfully to an industrial area, such information is urgently necessary. For this reason, three different kinds of displays were tested in this study regarding human information perception. Thus, the smallest target size necessary to provide successful information processing could be determined for different tasks. This study gives exemplary results obtained through tests with the prototype of a \"Virtual Retinal Display\".',\n",
       " 'Bei Augmented Reality (AR) handelt es sich um eine neue Technologie, die es gestattet, in das reale Sichtfeld von Benutzern zusätzliche virtuelle Informationen einzublenden. Um neben einer reinen Technikentwicklung auch die zukünftigen Benutzer von Anfang an zu berücksichtigen, wurdeim BMBF-geförderten Leitprojekt ARVIKA die benutzerzentrierte Systemgestaltung als zentraler Bestandteil des Projektes verankert. In diesem Beitrag werden neben der Vorgehensweise beim Usability Engineering im Projekt ARVIKA erste Ergebnisse aus der Evaluationsphase der AR-Benutzerschnittstellen dargestellt. Diese zeigen, dass AR zwar große Potentiale für die Praxis aufweist, jedoch wesentliche Bestandteile der heute gängigen Interaktionskonzepte desktopbasierter Systeme hinsichtlich ihrer AR-Tauglichkeit überarbeitet bzw. vollständig neu gestaltet werden müssen. Neben Problemen mit den bislang entwickelten AR-Benutzerschnittstellen werden Lösungsansätze dargestellt und so die Grundlagen für die Entwicklungen in der zweiten Projektphase aus benutzerzentrierter Sicht gegeben.',\n",
       " 'This paper describes the development and evaluation of a graphical user interface for a wearable augmented reality (AR) welding system. The AR-system enables the user to perceive his close environment more precisely than ordinary welding helmets and to receive additional online information about the welding process itself. During the usability test, single icons as well as user interfaces on an industrial PC and on two different head mounted displays - i-glasses video see-through and MicroOptical look-around display - were tested. Various methods were employed for this purpose, such as observation, thinking aloud, interviews and walkthroughs. The results indicated a positive attitude of the users towards the innovative AR-system. The participants expressed their comments and recommendations, which would be considered for further improvements and design of the system.',\n",
       " \"Augmented Reality (AR) is a technology merging real world with computer generated virtual objects in the user's field of view. This technology can also support work persons to execute their manual assembly tasks. For assembly as a new AR-application field only few heuristic guidelines are available for a user centered design. Whereas Virtual Reality (VR) needs the complete model of virtual objects AR for assembly support can use simple icons. Three icons and two different display technologies were evaluated in an experiment. A new icon combining arrow and torus and the look-around display technology support assembly tasks best.\",\n",
       " 'In this paper we describe a system for bringing usergenerated content to mobile augmented reality clients, taking in consideration the metadata required for visualizing it, at a sensor based tracking solution. Our proposal assumes that content is stored in multiple external Internet services, simply treated as a #x201C;cloud #x201D;, thus making the mobile client service agnostic. A prototype implementation was created for the Image Space service and the learnings of integrating with the popular Flickr service are discussed.',\n",
       " \"Although augmented reality (AR) research dates back to the early 1960s, the technology seems to have come to fruition only recently---nearly 50 years after its invention. Researchers have solved many technical challenges during that time, finally allowing practical access to this user interface. While early AR research focused on head-mounted displays and backpack computers, it now encompasses a variety of enabling technologies, including camera phones and other handhelds, advanced projector-camera systems, and AR-extended professional devices, such as x-ray scanners. It is this evolution that has finally made it possible to use AR in our daily lives. Although AR's practical applications have not yet reached the level that many people involved in the field have dreamed of, the number of these applications is steadily increasing. Beyond what has been published about AR in the scientific literature and the lay press, current research in the field poses some exciting questions: What's real about augmented reality? What role does it play for all of us today, and what impact will it potentially have on society in the future? Will there be a killer app for AR, or will the AR bubble finally burst? Currently, the areas in which AR seems to do very well include entertainment, education, advertising/marketing, and medicine. Where will it go from here?\",\n",
       " 'Augmented reality on mobile phones – i.e. applications where users look at the live image of the device’s video camera and the scene that they see is enriched by 3D virtual objects – provides great potential in areas such as cultural heritage, entertainment, and tourism. However, current interaction concepts are often limited to pure 2D pointing and clicking on the device’s screen. This paper explores different interaction approaches that rely on multimodal sensor input and aim at providing a richer, more complex, and engaging interaction experience. We present a user study that investigates the usefulness of our approaches, verify their usability, and identify limitations as well as possibilities for interaction development for mobile augmented reality.',\n",
       " \"One claim of Learning-enhancing technologies is to support and exploit benefits from distance learning and remote collaboration. On the other hand, several approaches to learning emphasize the importance of hands-on experience. Unfortunately, these two goals don't go well together with taditional learning techniques. Even though TEL technologies can alleviate this problem, it is not sufficiently solved yet - remote collaboration usually comes at the cost of losing direct hands-on access. The ARiSE project aims at bringing Augmented Reality to School Environments, a technology that can potentially bridge the gap between these goals. The project has designed, implemented and evaluated a pedagogical reference scenario where students work hands-on together over large distances. The paper describes the AR learning approach we followed and discusses its potantial and implementation. It concludes with findings obtained during field tests.\",\n",
       " 'Numerous Studies and new applications like Google Expeditions or Anatomy 4D point to a great potential of Augmented and Virtual Reality for its use for educational purposes. However, related research concerning technology integration in the classroom has shown that a valuable medium alone does not automatically lead to its successful use. It is therefore the aim of the paper to present an approach by which competencies of pre-service teachers for a successful and appropriate integration of Augmented and Virtual Reality-applications in the classroom are fostered - the own design of Augmented and Virtual Reality applications. We will begin with a short discussion of prominent findings and related work in regard to teaching and learning with and about Augmented and Virtual Reality in the first part before introducing the main goals and aspects of the presented pedagogical concept and the seminar in the second part of the paper. In the third part, two applications created in the seminar and qualitative data from an explorative study based on focus group interviews and participant observation will be presented. We close with a discussion of our findings and an outlook to future work.',\n",
       " 'This paper describes research on a wearable Augmented Reality (AR) welding support system. \"Blind welding\" caused by current protecitve welding helmets is to be replaced by AR supported views of the welding process. In this development the question of 2D to 3D viewing aids arises. For the experiment an integrated helmet with two High-Dynamic Range Camerias (HDRC) and a closed view stereoscopic head-mounted display was used. The experimental task was to position pegs on a pegboard. Alignment angles of the cameras and inter-camera separation distances were varied under the different experimental conditions. It was found that a convergent alignment of the cameras resulted in less mistakes and higher speed. An effect of the inter-camera separation was found in 3D-perception, suggesting the advantage of smaller distances.',\n",
       " 'We present an approach combining the AR-based presentation of product attributes in a physical retail store with recommendations for items only available online. The system supports users’ decisionmaking process by offering functions for comparing product features between items, both physical and online, and by providing recommendations based on selecting in-store products. The physical products may thus serve as anchors for forming the user’s preferences, also offering a richer and more engaging experience when exploring the products hands-on. Both objective product attributes as well as the visual appearance of a physical product are employed for generating recommendations from the online space. In this way, the advantages of online and in-store shopping can be combined, creating novel multi-channel opportunities for businesses. An empirical evaluation showed that the comparison and recommendation functions were appreciated by users, and hinted some possible benefits of a hybrid physical-online shopping support system. Despite the limitations of the study, there is sufficient evidence to consider this a viable approach worth to be further explored.',\n",
       " 'In this paper the development of a intuitive user interface for augmented reality welding application is described. The new interface will provide a better view of the welders work environment as well as some supporting information, such as overlaid CAD graphics, important parameters of the welding machine etc. The development of the graphical user interface has been an iterative process and has undergone different phases, e.g. testing prototypes with end-users. In this paper a usability test is described, which was carried out in order to check different types of supporting tools for easier navigation through the menu.',\n",
       " 'Das umfassende Lehrbuch bietet Studierenden eine anschauliche Begleit- und Nachschlaglektüre zu Lehrveranstaltungen, die Virtual Reality / Augmented Reality (VR/AR) thematisieren, z.B. im Bereich Informatik, Medien oder Natur- und Ingenieurwissenschaften. Der modulare Aufbau des Buches gestattet es, sowohl die Reihenfolge der Themen den Anforderungen der jeweiligen Unterrichtseinheit anzupassen als auch eine spezifische Auswahl für ein individuelles Selbststudium zu treffen. Die Leser erhalten die Grundlagen, um selbst VR/AR-Systeme zu realisieren oder zu erweitern, User Interfaces und Anwendungen mit Methoden der VR/AR zu verbessern sowie ein vertieftes Verständnis für die Nutzung von VR/AR zu entwickeln. Neben einem theoretischen Fundament vermittelt das Lehrbuch praxisnahe Inhalte. So erhalten auch potenzielle Anwender in Forschung und Industrie einen wertvollen und hinreichend tiefen Einblick in die faszinierenden Welten von VR/AR sowie ihre Möglichkeiten und Grenzen.',\n",
       " 'This paper presents the use of a wearable computer system to visualise outdoor architectural features using augmented reality. The paper examines the question - How does one visualise a design for a building, modification to a building, or extension to an existing building relative to its physical surroundings? The solution presented to this problem is to use a mobile augmented reality platform to visualise the design in spatial context of its final physical surroundings. The paper describes...',\n",
       " \"The appearance of avatars can potentially alter changes in their users' perception and behavior. Based on this finding, approaches to support the therapy of body perception disturbances in eating or body weight disorders by mixed reality (MR) systems gain in importance. However, the methodological heterogeneity of previous research has made it difficult to assess the suitability of different MR systems for therapeutic use in these areas. The effects of MR system properties and related psychometric factors on body-related perceptions have so far remained unclear. We developed an interactive virtual mirror embodiment application to investigate the differences between an augmented reality see-through head-mounted-display (HMD) and a virtual reality HMD on the before-mentioned factors. Additionally, we considered the influence of the participant's body-mass-index (BMI) and the BMI difference between participants and their avatars on the estimations. The 54 normal-weight female participants significantly underestimated the weight of their photorealistic, generic avatar in both conditions. Body weight estimations were significantly predicted by the participants' BMI and the BMI difference. We also observed partially significant differences in presence and tendencies for differences in virtual body ownership between the systems. Our results offer new insights into the relationships of body weight perception in different MR environments and provide new perspectives for the development of therapeutic applications.\",\n",
       " 'Augmented reality (AR), a useful visualization technique, is reviewed based literatures. The AR research methods and applications are surveyed since AR was first developed over forty years ago. Recent and future AR researches are proposed which could help researchers of decide which topics should be developed when they are beginning their own researches in the field.',\n",
       " 'This article investigates the effects of different XR displays on the perception and plausibility of personalized virtual humans. We compared immersive virtual reality (VR), video see-through augmented reality (VST AR), and optical see-through AR (OST AR). The personalized virtual alter egos were generated by state-of-the-art photogrammetry methods. 42 participants were repeatedly exposed to animated versions of their 3D-reconstructed virtual alter egos in each of the three XR display conditions. The reconstructed virtual alter egos were additionally modified in body weight for each repetition. We show that the display types lead to different degrees of incongruence between the renderings of the virtual humans and the presentation of the respective environmental backgrounds, leading to significant effects of perceived mismatches as part of a plausibility measurement. The device-related effects were further partly confirmed by subjective misestimations of the modified body weight and the measured spatial presence. Here, the exceedingly incongruent OST AR condition leads to the significantly highest weight misestimations as well as to the lowest perceived spatial presence. However, similar effects could not be confirmed for the affective appraisal (i.e., humanness, eeriness, or attractiveness) of the virtual humans, giving rise to the assumption that these factors might be unrelated to each other.',\n",
       " \"The appearance of avatars can potentially alter changes in their users' perception and behavior. Based on this finding, approaches to support the therapy of body perception disturbances in eating or body weight disorders by mixed reality (MR) systems gain in importance. However, the methodological heterogeneity of previous research has made it difficult to assess the suitability of different MR systems for therapeutic use in these areas. The effects of MR system properties and related psychometric factors on body-related perceptions have so far remained unclear. We developed an interactive virtual mirror embodiment application to investigate the differences between an augmented reality see-through head-mounted-display (HMD) and a virtual reality HMD on the before-mentioned factors. Additionally, we considered the influence of the participant's body-mass-index (BMI) and the BMI difference between participants and their avatars on the estimations. The 54 normal-weight female participants significantly underestimated the weight of their photorealistic, generic avatar in both conditions. Body weight estimations were significantly predicted by the participants' BMI and the BMI difference. We also observed partially significant differences in presence and tendencies for differences in virtual body ownership between the systems. Our results offer new insights into the relationships of body weight perception in different MR environments and provide new perspectives for the development of therapeutic applications.\",\n",
       " 'This paper presents a framework to achieve real-time augmented reality applications. We propose a framework based on the visual servoing approach well known in robotics. We consider pose or viewpoint computation as a similar problem to visual servoing. It allows one to take advantage of all the research that has been carried out in this domain in the past. The proposed method features simplicity, accuracy, efficiency, and scalability wrt. to the camera model as well as wrt. the features extracted from the image. We illustrate the efficiency of our approach on augmented reality applications with various real image sequences.',\n",
       " 'While the knowledge economy has reshaped the world, schools lag behind in producing appropriate learning for this social change. Science education needs to prepare students for a future world in which multiple representations are the norm and adults are required to think like scientists. Location-based augmented reality games offer an opportunity to create a post-progressive pedagogy in which students are not only immersed in authentic scientific inquiry, but also required to perform in adult scientific discourses. This cross-case comparison as a component of a design-based research study investigates three cases (roughly 28 students total) where an Augmented Reality curriculum, Mad City Mystery, was used to support learning in environmental science. We investigate whether augmented reality games on handhelds can be used to engage students in scientific thinking (particularly argumentation), how game structures affect students’ thinking, the impact of role playing on learning, and the role of the physical environment in shaping learning. We argue that such games hold potential for engaging students in meaningful scientific argumentation. Through game play, players are required to develop narrative accounts of scientific phenomena, a process that requires them to develop and argue scientific explanations. We argue that specific game features scaffold this thinking process, creating supports for student thinking non-existent in most inquiry-based learning environments.',\n",
       " 'The rapid development of 3D scanning technology combined with state-of-the-art mapping algorithms allows to capture 3D point clouds with high resolution and accuracy. The high amount of data collected with LiDAR, RGB-D cameras or generated through SfM approaches makes the direct use of the recorded data for realistic rendering and simulation problematic. Therefore, these point clouds have to be transformed into representations that fulfill the computational requirements for VR and AR setups. In this tutorial participants will be introduced to state-of-the-art methods in point cloud processing and surface reconstruction with open source software to learn the benefits for AR and VR applications by interleaved presentations, software demonstrations and software trials. The focus lies on 3D point cloud data structures (range images, octrees, k-d trees) and algorithms, and their implementation in C/C++. Surface reconstruction using Marching Cubes and other meshing methods will play another central role. Reference material for subtopics like 3D point cloud registration and SLAM, calibration, filtering, segmentation, meshing, and large scale surface reconstruction will be provided. Participants are invited to bring their Linux, MacOS or Windows laptops to gain hands-on experience on practical problems occuring when working with large scale 3D point clouds in VR and AR applications.',\n",
       " 'Augmented Reality enhances user perception by overlaying real world information with virtual computer-generated information. The aims of the 6 th Sense project are to improve real-time interaction between the real environment and the virtual world and to maximize the user experience in mobile Augmented Reality. To achieve these objectives a generic framework constituted of two main layers is proposed. The End-to-End Adaptation Layer adapts in real-time the parameters of the Augmented Reality system to provide the user with the best possible experience despite the varying operating conditions such as the transmission link and user head motion. The Generic Augmented Reality Layer encompasses solutions to the problem of overlaying adequate information in the real scene and manages multimodal interaction with the virtual environment.',\n",
       " 'Recent user interface concepts, such as multimedia, multimodal, wearable, ubiquitous, tangible, or augmented-reality-based (AR) interfaces, each cover different approaches that are all needed to support complex human–computer interaction. Increasingly, an overarching approach towards building what we call ubiquitous augmented reality (UAR) user interfaces that include all of the just mentioned concepts will be required. To this end, we present a user interface architecture that can form a sound basis for combining several of these concepts into complex systems. We explain in this paper the fundamentals of DWARF’s user interface framework (DWARF standing for distributed wearable augmented reality framework) and an implementation of this architecture. Finally, we present several examples that show how the framework can form the basis of prototypical applications.',\n",
       " 'In the Shared Space project, we explore, innovate, design and evaluate future computing environments that will radically enhance interaction between human and computers as well as interaction between humans mediated by computers. In particular, we investigate how augmented reality enhanced by physical and spatial 3D user interfaces can be used to develop effective face-to-face collaborative computing environments. How will we interact in such collaborative spaces? How will we interact with each other? What new applications can be developed using this technology? These are the questions that we are trying to answer in research on Shared Space. The paper provides a short overview of Shared Space, its directions, technologies and applications',\n",
       " 'This book constitutes the refereed proceedings of the 14th International Conference on Virtual Reality and Augmented Reality, EuroVR 2017, held in Laval, France, in December 2017. The 10 full papers and 2 short papers presented were carefully reviewed and selected from 36 submissions. The papers are organized in four topical sections: interaction models and user studies, visual and haptic real-time rendering, perception and cognition, and rehabilitation and safety.',\n",
       " \"Teleoperated robot-assisted surgical systems provide surgeons with improved precision, dexterity, and visualization over traditional minimally invasive surgery. The addition of haptic (force and/or tactile) feedback has been proposed as a way to further enhance the performance of these systems. However, due to limitations in sensing and control technologies, implementing direct haptic feedback to the surgeon's hands remains impractical for clinical application. A new, intuitive augmented reality system for presentation of force information through sensory substitution has been developed and evaluated. The augmented reality system consists of force-sensing robotic instruments, a kinematic tool tracker, and a graphic display that overlays a visual representation of force levels on top of the moving instrument tips. The system is integrated with the da Vinci Surgical System (Intuitive Surgical, Inc.) and tested by several users in a phantom knot tying task. The augmented reality system decreases the number of broken sutures, decreases the number of loose knots, and results in more consistent application of forces.\",\n",
       " 'Augmented Reality (AR) for assembly processes is a new kind of computer support for a traditional industrial domain. This new application of AR-technology is called ARsembly. The intention of this article is to describe a typical scenario for assembly and service personnel and how they might be supported by AR. For this purpose, tasks with different degrees of difficulty were selected from an authentic assembly process. In addition, two other kinds of assembly support media (a paper manual and a tutorial by an expert) were examined in order to compare them with ARsembly. The results showed that the assembly times varied according to the different support conditions. AR-support proved to be more suitable for difficult tasks than the paper manual, whereas for easier tasks, the use of a paper manual did not differ significantly from AR-support. Tasks done under the guidance of an expert, were completed most rapidly. Some of the information obtained in this investigation also indicated important considerations for improving future ARsembly applications.',\n",
       " 'Durch die fortschreitende Technisierung und Automatisierung industrieller Prozesse im Zuge der vierten industriellen Revolution stellt sich immer häufiger die Frage, was der Mensch in der Fabrik von Morgen für eine Rolle spielt. Damit er nicht als Fremdkörper, sondern als steuernder sowie als überwachender Akteur in diese modernen Prozesse eingebunden wird, bedarf es innovativer und adaptiver Benutzerschnittstellen. Augmented Reality (AR) erlaubt es, dem Benutzer Informationen situationsabhängig dort anzuzeigen, wo sie benötigt werden: Direkt im Blickfeld und auf dem betreffenden Objekt. Innerhalb des Forschungsprojektes SmARPro wird ein System entwickelt, das Informationen kontext- und rollenbasiert aufbereitet und über Augmented Reality dem Benutzer zur Verfügung stellt.',\n",
       " 'Augmented Reality is on the rise with consumer-grade smart glasses becoming available in recent years. Those interested in deploying these head-mounted displays need to understand better the effect technology has on the end user. One key aspect potentially hindering the use is motion sickness, a known problem inherited from virtual reality, which so far remains under-explored. In this paper we address this problem by conducting an experiment with 142 subjects in three different industries: aviation, medical, and space. We evaluate whether the Microsoft HoloLens, an augmented reality head-mounted display, causes simulator sickness and how different symptom groups contribute to it (nausea, oculomotor and disorientation). Our findings suggest that the Microsoft HoloLens causes across all participants only negligible symptoms of simulator sickness. Most consumers who use it will face no symptoms while only few experience minimal discomfort in the training environments we tested it in.',\n",
       " 'An Augmented Reality (AR) composite layup tool was created using low cost, off the shelf components and software to prove and demonstrate the application of AR in a manufacturing environment. The project tested different tracking technologies in order to ascertain their practicality within an industrial environment. By developing an understanding of the challenges faced in implementing such an application, the project further demonstrates the potential for cost and waste reduction. The experimental setup is at a lower price point than existing all in one solutions, thus increasing access to the technology. The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement n&#x00B0; 283336 - REFORM.',\n",
       " 'In this paper, we present an Augmented Reality (AR) system for aiding field workers of utility companies in outdoor tasks such as maintenance, planning or surveying of underground infrastructure. Our work addresses these issues using spatial interaction and visualization techniques for mobile AR applications and as well as for a new mobile device design. We also present results from evaluations of the prototype application for underground infrastructure spanning various user groups. Our application has been driven by feedback from industrial collaborators in the utility sector, and includes a translation tool for automatically importing data from utility company databases of underground assets.',\n",
       " 'When blending virtual and physical content, certain incongruencies emerge from hardware limitations, inaccurate tracking, or different appearances of virtual and physical content. They restrain us from perceiving virtual and physical content as one experience. Hence, it is crucial to investigate these issues to determine how they influence our experience. We present a virtualized augmented reality simulation that can systematically examine single incongruencies or different configurations.',\n",
       " 'Augmented Reality - a new kind of machine interaction - uses so-called Head-Mounted Displays (HMD) to overlay the real world with virtual information. The use of monocular HMDs opens the user the possibility to use it on the left or on the right eye. HMD manufacturers recommend to use these on the dominant eye. According to this recommendation the influence of the dominant eye has been evaluated in an experiment.',\n",
       " 'Augmented reality and spatial information manipulation is being increasingly used as part of environ- ment integrated form factors and wearable device such as head-mounted displays. The integration of this exciting technology in many aspects of peoples’ lives is transforming the way we understand computing, pushing the boundaries of Spatial Interfaces into virtual but embedded environments. We think that the time is ripe for a renewed discussion about the role of Augmented Reality within Spatial Interfaces. With this SIG we want to expand the discussion related to Spatial Interfaces and the way they impact interaction with the world in two areas. First, we aim to critically discuss the definition of Spatial Interfaces and outline the common components that build such interfaces in today’s world. Second, we would like the community to reflect on the path ahead and focus on the potential of what kind of experiences can Spatial Interfaces achieve today',\n",
       " \"Augmented Reality (AR) and Virtual Reality (VR) are two developing technologies with enormous promise to change the way education is provided. The purpose of this research is to investigate the fundamental distinctions between AR and VR in the context of course instruction. Educators can acquire insights into how these technologies might be effectively integrated into the classroom by evaluating their specific traits, affordances, and limits. The research begins by defining AR and VR and then compares their key ideas. AR superimposes digital information on the real-world environment to improve the user's impression of reality, whereas VR immerses users in a simulated environment to create a sensation of presence and immersion. These contrasts lay the groundwork for comprehending their disparate uses in education. The study then goes into the educational benefits and difficulties of AR and VR. AR allows students to interact with digital information in real time, encouraging engagement, collaboration, and contextualized learning. It could bridge the gap between abstract ideas and real-world applications. VR, on the other hand, provides an immersive and regulated environment that allows students to explore complicated scenarios, imitate real-world circumstances, and acquire important skills in a safe and cost-effective manner. The research looks at the technology needs, accessibility concerns, and implementation techniques for AR and VR in educational settings. It emphasizes the significance of pedagogical design, content creation, and teacher training to maximize these technologies' educational impact. This review paper investigates the differences between Augmented Reality (AR) and Virtual Reality (VR) in the context of course education. AR and VR have emerged as significant tools for boosting the learning experience as technology continues to alter education. Understanding the distinct characteristics and benefits of each technology is critical for educators to make educated judgments about how to include them in their teaching practices. This paper examines AR and VR in depth, concentrating on their distinctions, uses, and possible influence in educational contexts.\",\n",
       " 'Electronic skins equipped with artificial receptors are able to extend our perception beyond the modalities that have naturally evolved. These synthetic receptors offer complimentary information on our surroundings and endow us with novel means of manipulating physical or even virtual objects. We realize highly compliant magnetosensitive skins with directional perception that enable magnetic cognition, body position tracking, and touchless object manipulation. Transfer printing of eight high-performance spin valve sensors arranged into two Wheatstone bridges onto 1.7-μm-thick polyimide foils ensures mechanical imperceptibility. This resembles a new class of interactive devices extracting information from the surroundings through magnetic tags. We demonstrate this concept in augmented reality systems with virtual knob-turning functions and the operation of virtual dialing pads, based on the interaction with magnetic fields. This technology will enable a cornucopia of applications from navigation, motion tracking in robotics, regenerative medicine, and sports and gaming to interaction in supplemented reality.',\n",
       " 'We present a scenario-based laboratory study with 40 participants comparing two augmented reality (AR) glasses in a travelling scenario. In a within-subject design, the binocular Epson Moverio BT-200 and the monocular Vuzix M100 were compared with regard to performance, acceptance, workload and preference. While performance was equal with both glasses, the Epson Moverio BT-200 glasses got higher acceptance and lower workload ratings and were preferred by the majority of the participants. The findings provide knowledge on human factors in AR glasses usage.',\n",
       " 'ARETE (Augmented Reality Interactive Educational System) is a European project, which aims to develop an interactive toolkit for Augmented Reality (AR) content. In this work in progress paper, we describe the system which is currently being developed. The ARETE system follows human-centered interaction design practices and has a strong focus in interactive, multi-user and multi-lingual technologies. On completion, the system will be evaluated across three pilot studies. The three pilots will involve more than 3000 students across ten European countries and they will assess the impact of usage of the ARETE system and the educational value of AR for English literacy skills, STEM (Science, Technology, Engineering and Math) skills and in the implementation of Positive Behaviour Support in Schools (PBIS). ARETE will also be involved in the implementation of new standards for the creation of cross platform content and services as well as the creation of learning experience data repositories.',\n",
       " 'ARETE (Augmented Reality Interactive Educational System) is a European project, which aims to develop an interactive toolkit for Augmented Reality (AR) content. In this work in progress paper, we describe the system which is currently being developed. The ARETE system follows human-centered interaction design practices and has a strong focus in interactive, multi-user and multi-lingual technologies. On completion, the system will be evaluated across three pilot studies. The three pilots will involve more than 3000 students across ten European countries and they will assess the impact of usage of the ARETE system and the educational value of AR for English literacy skills, STEM (Science, Technology, Engineering and Math) skills and in the implementation of Positive Behaviour Support in Schools (PBIS). ARETE will also be involved in the implementation of new standards for the creation of cross platform content and services as well as the creation of learning experience data repositories.',\n",
       " 'Remote maintenance of industrial manipulators often is performed via telephone support. Recent approaches in the context of the \\\\u0027Industry 4.0\\\\u0027 consider internet technologies and Augmented Reality (AR) to enhance situation awareness between external experts and local service technicians. We present two AR-based case studies: First, a mobile AR architecture based on optical see through glasses is used for an on-site local repair task. Second, a remote architecture based on a portable tablet PC and a high precision tracking system is used to realize an off-site expert access. The to-be-serviced machine is visualized inside of a large area similar to a machinery hall and can be inspected by the experts walking around this virtual plant using the tablet and perspectively correct rendering to understand the production process and the operation context. Both methods have been evaluated in first user studies.',\n",
       " 'The use of augmented reality (AR) in formal education could prove a key component in future learning environments that are richly populated with a blend of hardware and software applications. However, relatively little is known about the potential of this technology to support teaching and learning with groups of young children in the classroom. Analysis of teacher–child dialogue in a comparative study between use of an AR virtual mirror interface and more traditional science teaching methods for 10-year-old children, revealed that the children using AR were less engaged than those using traditional resources. We suggest four design requirements that need to be considered if AR is to be successfully adopted into classroom practice. These requirements are: flexible content that teachers can adapt to the needs of their children, guided exploration so learning opportunities can be maximised, in a limited time, and attention to the needs of institutional and curricular requirements.',\n",
       " 'Augmented Reality is an advance technology that enhances the real world by overlaying digital data on top of it. When Augmented Reality (AR) experience is delivered on mobile devices it is termed as mobile augmented reality (MAR). MAR is state-of-the-art technology that has completely revolutionized the way of accessing and interacting with information thus invoking new experiences for users all around the world. This article is an effort to summarize the current research regarding user experience of MAR. Mobile AR Publications of past 10 years are identified for a preliminary review from prominent online databases and digital libraries. The aim of this study is to identify the areas of User Experience (UX) that lack research. We present a classification of present UX research in MAR domain. Research findings and possible opportunities for future research are also discussed.',\n",
       " 'We are on the verge of ubiquitously adopting Augmented Reality (AR) technologies to enhance our perception and help us see, hear, and feel our environments in new and enriched ways. AR will support us in fields such as education, maintenance, design and reconnaissance, to name but a few. This paper describes the field of AR, including a brief definition and development history, the enabling technologies and their characteristics. It surveys the state of the art by reviewing some recent applications of AR technology as well as some known limitations regarding human factors in the use of AR systems that developers will need to overcome.',\n",
       " 'Augmented Reality (AR) allows for a connection between real and virtual worlds, thus providing a high potential for Special Needs Education (SNE). We developed an educational application called Fancy Fruits to teach disabled children the components of regional fruits and vegetables. The app includes marker-based AR elements connecting the real situation with virtual information. To evaluate the application, a field study was conducted. Eleven children with mental disabilities took part in the study. The results show a high enjoyment of the participants. The study also validated the app’s child-friendly design.',\n",
       " \"Augmented Reality (AR) allows for a connection between real and virtual worlds, thus providing a high potential for Special Needs Education (SNE). We developed an educational application called Fancy Fruits to teach disabled children the components of regional fruits and vegetables. The app includes marker-based AR elements connecting the real situation with virtual information. To evaluate the application, a field study was conducted. Eleven children with mental disabilities took part in the study. The results show a high enjoyment of the participants. The study also validated the app's child-friendly design.\",\n",
       " \"Augmented Reality (AR) allows for a connection between real and virtual worlds, thus providing a high potential for Special Needs Education (SNE). We developed an educational application called Fancy Fruits to teach disabled children the components of regional fruits and vegetables. The app includes marker-based AR elements connecting the real situation with virtual information. To evaluate the application, a field study was conducted. Eleven children with mental disabilities took part in the study. The results show a high enjoyment of the participants. The study also validated the app's child-friendly design.\",\n",
       " \"Augmented Reality (AR) allows for a connection between real and virtual worlds, thus providing a high potential for Special Needs Education (SNE). We developed an educational application called Fancy Fruits to teach disabled children the components of regional fruits and vegetables. The app includes marker-based AR elements connecting the real situation with virtual information. To evaluate the application, a field study was conducted. Eleven children with mental disabilities took part in the study. The results show a high enjoyment of the participants. The study also validated the app's child-friendly design.\",\n",
       " 'In health care, there are still many devices with poorly designed user interfaces that can lead to user errors. Especially in acute care, an error can lead to critical conditions in patients. Previous research has shown that the use of augmented reality can help to better monitor the condition of patients and better detect unforeseen events. The system created in this work is intended to aid in the detection of changes in patient and equipment-data in order to increase detection of critical conditions or errors.',\n",
       " 'In health care, there are still many devices with poorly designed user interfaces that can lead to user errors. Especially in acute care, an error can lead to critical conditions in patients. Previous research has shown that the use of augmented reality can help to better monitor the condition of patients and better detect unforeseen events. The system created in this work is intended to aid in the detection of changes in patient and equipment-data in order to increase detection of critical conditions or errors.',\n",
       " 'In health care, there are still many devices with poorly designed user interfaces that can lead to user errors. Especially in acute care, an error can lead to critical conditions in patients. Previous research has shown that the use of augmented reality can help to better monitor the condition of patients and better detect unforeseen events. The system created in this work is intended to aid in the detection of changes in patient and equipment-data in order to increase detection of critical conditions or errors.',\n",
       " 'In health care, there are still many devices with poorly designed user interfaces that can lead to user errors. Especially in acute care, an error can lead to critical conditions in patients. Previous research has shown that the use of augmented reality can help to better monitor the condition of patients and better detect unforeseen events. The system created in this work is intended to aid in the detection of changes in patient and equipment-data in order to increase detection of critical conditions or errors.',\n",
       " 'In health care, there are still many devices with poorly designed user interfaces that can lead to user errors. Especially in acute care, an error can lead to critical conditions in patients. Previous research has shown that the use of augmented reality can help to better monitor the condition of patients and better detect unforeseen events. The system created in this work is intended to aid in the detection of changes in patient and equipment-data in order to increase detection of critical conditions or errors.',\n",
       " \"According to Google Trends, mobile augmented reality (AR) apps will, after a brief hype-associated peak and decline, show again a more steady growth in the near future. Indeed, a review of currently available mobile AR apps suggests that they are embracing practical uses instead of simply showing off the capabilities of AR. Many of these apps are making information more ubiquitous without making the user necessarily aware of the data's origins, using graphics to represent multiple datasets. There has been however little research focusing on the trends in AR and the types of data visualisations used in mobile AR. The purpose of this paper is to fill that void by presenting an analysis of currently popular mobile AR apps on the market. Our findings from this small-scale investigation give an indication of the types of visualisation styles used in current mobile AR apps and highlight suggestions for how to make the information presented more relevant, for example by using better filtering methods.\",\n",
       " 'Augmented Reality uses Head Mounted Displays (HMD) to overlay the real word with additional virtual information. Virtual Retinal Displays (VRD), a new display technology, no longer need Liquid Crystal Displays (LCD). The VRD technology addresses the retina directly with a single laser stream of pixels. Empirical studies concerning the user’s informational strain of this new VRD technology are unknown. Various papers have shown, that the Heart Rate Variability (HRV) is a valid indicator for the user’s informational strain. An empirical test revealed no difference in the user’s HRV between the VRD technology and the LCD technology. Consequently, there seems to be an comparable users informational strain regarding the display types.',\n",
       " \"Augmented Reality uses Head Mounted Displays (HMD) to overlay the real word with additional virtual information. Virtual Retinal Displays (VRD), a new display technology, no longer requires Liquid Crystal Displays (LCD). VRD technology addresses the retina directly with a single laser stream of pixels. There are no studies on the user's informational strain in this new VRD technology. Various papers have shown that Heart Rate Variability (HRV) is a valid indicator for the user's informational strain. An empirical test revealed no difference in the user's HRV between VRD technology and LCD technology. Consequently, there seems to be a comparable user informational strain regarding the display types.\",\n",
       " 'Ubiquitous computing is a challenging area that allows us to further our understanding and techniques of context-aware and adaptive systems. Among the challenges is the general problem of capturing the larger context in interaction from the perspective of user modeling and human–computer interaction (HCI). The imperative to address this issue is great considering the emergence of ubiquitous and mobile computing environments. This paper provides an account of our addressing the specific problem of supporting functionality as well as the experience design issues related to museum visits through user modeling in combination with an audio augmented reality and tangible user interface system. This paper details our deployment and evaluation of ec(h)o – an augmented audio reality system for museums. We explore the possibility of supporting a context-aware adaptive system by linking environment, interaction objects and users at an abstract semantic level instead of at the content level. From the user modeling perspective ec(h)o is a knowledge-based recommender system. In this paper we present our findings from user testing and how our approach works well with an audio and tangible user interface within a ubiquitous computing system. We conclude by showing where further research is needed.',\n",
       " 'Nestor is a real-time recognition and camera pose estimation system for planar shapes. The system allows shapes that carry contextual meanings for humans to be used as Augmented Reality (AR) tracking targets. The user can teach the system new shapes in real time. New shapes can be shown to the system frontally, or they can be automatically rectified according to previously learned shapes. Shapes can be automatically assigned virtual content by classification according to a shape class library. Nestor performs shape recognition by analyzing contour structures and generating projective-invariant signatures from their concavities. The concavities are further used to extract features for pose estimation and tracking. Pose refinement is carried out by minimizing the reprojection error between sample points on each image contour and its library counterpart. Sample points are matched by evolving an active contour in real time. Our experiments show that the system provides stable and accurate registration, and runs at interactive frame rates on a Nokia N95 mobile phone.',\n",
       " \"While the knowledge economy has reshaped the world, schools lag behind in producing appropriate learning for this social change. Science education needs to prepare students for a future world in which multiple representations are the norm and adults are required to â€œthink like scientists.â€� Location-based augmented reality games offer an opportunity to create a â€œpost-progressiveâ€� pedagogy in which students are not only immersed in authentic scientific inquiry, but also required to perform in adult scientific discourses. This cross-case comparison as a component of a design-based research study investigates three cases (roughly 28 students total) where an Augmented Reality curriculum, Mad City Mystery, was used to support learning in environmental science. We investigate whether augmented reality games on handhelds can be used to engage students in scientific thinking (particularly argumentation), how game structures affect students' thinking, the impact of role playing on learning, and the role of the physical environment in shaping learning. We argue that such games hold potential for engaging students in meaningful scientific argumentation. Through game play, players are required to develop narrative accounts of scientific phenomena, a process that requires them to develop and argue scientific explanations. We argue that specific game features scaffold this thinking process, creating supports for student thinking non-existent in most inquiry-based learning environments.\",\n",
       " 'As we develop computing platforms for augmented reality (AR) head-mounted display (HMDs) technologies for social or workplace environments, understanding how users interact with notifications in immersive environments has become crucial. We researched effectiveness and user preferences of different interaction modalities for notifications, along with two types of notification display methods. In our study, participants were immersed in a simulated cooking environment using an AR-HMD, where they had to fulfill customer orders. During the cooking process, participants received notifications related to customer orders and ingredient updates. They were given three interaction modes for those notifications: voice commands, eye gaze and dwell, and hand gestures. To manage multiple notifications at once, we also researched two different notification list displays, one attached to the user’s hand and one in the world. Results indicate that participants preferred using their hands to interact with notifications and having the list of notifications attached to their hands. Voice and gaze interaction was perceived as having lower usability than touch.',\n",
       " 'Die soziale Akzeptanz ist neben der praktischen Akzeptanz ein wichtiger Bestandteil der Akzeptanz eines Systems durch die Nutzer. Es ist möglich, dass trotz hoher praktischer Akzeptanz ein System nicht genutzt wird, da dieses sozial nicht akzeptabel ist. Für Augmented-Reality-Datenbrillen (AR-Datenbrillen) wurden schon verschiedene Faktoren determiniert, welche die soziale Akzeptanz beeinflussen können, um zu vermeiden, dass diese von den Nutzern abgelehnt werden. Ein wichtiger Faktor, welcher die soziale Akzeptanz einer Datenbrille beeinflussen kann, ist die Interaktionsart. Es ist anzunehmen, dass die Verwendung unterschiedlicher Interaktionsarten in dem gleichen sozialen Kontext mit dem gleichen Gesamtsystem nicht in der gleichen sozialen Akzeptanz resultiert. Nachfolgend wird eine Studie mit 10 Probanden und 6 aktuell auf AR-Datenbrillen verwendeten Interaktionsarten durchgeführt, um deren soziale Akzeptanz in verschiedenen Orts- und Zuschauerkontexten vergleichend zu evaluieren. Die Studie zeigt, dass die Interaktionsarten, insbesondere diverse Arten der Sprachinteraktion, mit steigender Distanz der sozialen Beziehung zu dem Zuschauer in der Bewertung degradieren und, dass der Ort des voraussichtlichen Einsatzes der Interaktionsart berücksichtigt werden muss.',\n",
       " 'In der hier vorgestellten Laborstudie wurden zwei Arten markerbasierter Augmented-Reality-Ergänzungen zu einem Lehrbuch mit einer konventionellen Ergänzung in Papierform verglichen. Hierbei wurden drei Unterthemen aus der Ausbildung zum Kraftfahrzeugmechatroniker ausbalanciert um je eine Art der Zusatzinformation ergänzt. Dabei wurde der Lernerfolg mit Multiple-Choice-Aufgaben kontrolliert sowie die Gebrauchstauglichkeit (SUS), die Beanspruchung (NASA-rTLX), die räumliche Vorstellung (RV) und das Nutzererlebnis (UEQ) mit Fragebögen im Within-Subject Design (N = 24) erhoben. Die Ergebnisse zeigen ein höheres Nutzererlebnis und eine bessere räumliche Vorstellung bei Verwendung der AR im Vergleich zur konventionellen Ergänzung bei jeweils vergleichbarer niedriger Beanspruchung, hoher Gebrauchstauglichkeit sowie Lernleistung.',\n",
       " 'Location-based based augmented',\n",
       " 'Im Rahmen des Projektes RadAR+ wurde eine Augmented-Reality-(AR)-Reiseassistenzanwendung für eine Datenbrille entwickelt und prototypisch umgesetzt, welche ein Routing für den öffentlichen Verkehr sowie eine AR-Fußgängernavigation vereint und dadurch eine vollständige Navigation vom Start- zum Zielort ermöglicht. Der Prototyp wurde in einem Echtweltszenario mit 37 Probanden vergleichend zur herkömmlichen Unterstützung evaluiert. Es konnte gezeigt werden, dass die Gebrauchstauglichkeit der AR-Reiseassistenzanwendung von den Probanden als \"gut\" bewertet wird. Des Weiteren ist feststellbar, dass keine signifikanten Unterschiede zwischen der subjektiv empfundenen Beanspruchung beim Prototyp und der herkömmlichen Unterstützung bestehen. Jedoch gab es eine signifikant geringere Schrittzahl bei der Gruppe, die das Reiseassistenzsystem nutzte.',\n",
       " 'This paper presents an image-based rendering approach to accelerate rendering time of virtual scenes containing a large number of complex high poly count objects. Our approach replaces complex objects by impostors, light-weight image-based representations leveraging geometry and shading related processing costs. In contrast to their classical implementation, our impostors are specifically designed to work in Virtual-, Augmented- and Mixed Reality scenarios (XR for short), as they support stereoscopic rendering to provide correct depth perception. Motion parallax of typical head movements is compensated by using a ray marched parallax correction step. Our approach provides a dynamic run-time recreation of impostors as necessary for larger changes in view position. The dynamic run-time recreation is decoupled from the actual rendering process. Hence, its associated processing cost is therefore distributed over multiple frames. This avoids any unwanted frame drops or latency spikes even for impostors of objects with complex geometry and many polygons. In addition to the significant performance benefit, our impostors compare favorably against the original mesh representation, as geometric and textural temporal aliasing artifacts are heavily suppressed.',\n",
       " 'This paper presents an image-based rendering approach to accelerate rendering time of virtual scenes containing a large number of complex high poly count objects. Our approach replaces complex objects by impostors, light-weight image-based representations leveraging geometry and shading related processing costs. In contrast to their classical implementation, our impostors are specifically designed to work in Virtual-, Augmented- and Mixed Reality scenarios (XR for short), as they support stereoscopic rendering to provide correct depth perception. Motion parallax of typical head movements is compensated by using a ray marched parallax correction step. Our approach provides a dynamic run-time recreation of impostors as necessary for larger changes in view position. The dynamic run-time recreation is decoupled from the actual rendering process. Hence, its associated processing cost is therefore distributed over multiple frames. This avoids any unwanted frame drops or latency spikes even for impostors of objects with complex geometry and many polygons. In addition to the significant performance benefit, our impostors compare favorably against the original mesh representation, as geometric and textural temporal aliasing artifacts are heavily suppressed.',\n",
       " 'Separating and recycling waste is an important topic to protect our environment and achieve a more sustainable future. However, recycling also is a complex process, as each type of waste needs a specific recycling method. This comes along with multiple recycling containers, each relevant for one specific type of waste. Ensuring a correct recycling process therefore not only requires specific infrastructure, but also a respective attitude and knowledge of the population. Stressing the need for an accessible educational opportunity addressing waste separation, we present a mobile Augmented Reality (AR) application that guides a user through a recycling process and hence scaffolds the learning of proper recycling of each type of waste. The app further provides a prototypical implementation of a product scanner, that identifies the waste type based on a marker and assists the recycling on a case-by-case decision. Using self-determination theory as a framework, we integrated gamification elements, aiming for enhanced need satisfaction, motivation and user experience. In a user study, we compared the gamified version to a control version, with both app versions yielding a high acceptance, user experience, and waste separation behavior. This indicates the importance of providing easy-to-use mobile apps allowing for a learning and assistance of proper recycling.',\n",
       " 'Augmented Reality(AR) tools are currently primarily targeted at programmers, making designing for AR challenging and time-consuming. We developed an interactive prototype, PintAR, that enables the authoring and rapid-prototyping of situated experiences by allowing designers to bring their ideas to life using a digital pen for sketching and a head-mounted display for visualizing and interacting with virtual content. In this paper, we explore the versatility such a tool could provide through case studies of a researcher, an artist, a ballerina, and a clinician.',\n",
       " \"Mirror self-reflection can help us to develop a deeper understanding and appreciation of our body. Due to technological advancements, holographic augmented reality (AR) mirrors can create realistic visualizations of virtual humans that can represent one's appearance in an altered way while remaining in a familiar environment. Further developing those mirrors opens a new field for use in everyday life. In this work, we outline possible future scenarios where AR mirrors can empower individuals to visualize their emotions, thought patterns, and discrepancies related to their physical body and mental body image. Thus, AR mirrors can encourage their self-reflection, promote a positive and healthy relationship with their bodies, or motivate them to take action to improve their well-being.\",\n",
       " 'We investigate how reading text in augmented reality (AR) glasses and the simultaneous execution of three real-world tasks interfere with each other. The three tasks are a visual stimulus-response task (VSRT), a simple walking task and a walking obstacle course. Also, we investigate the effects of different AR text positions on primary task and reading performance as well as subjective preference. We propose a novel out of sight body-locked text placement for AR text presentation to be used in dual-task situations and compare it to head-locked text placement, each in two heights. AR reading affected performance in all tasks and reading speed was affected in all dual-task conditions. Participants subjectively preferred the body-locked text presentation, while objective measures do not reflect that preference. Differences between the tasks and several interaction effects between task and AR text placement demonstrate the necessity to carefully consider the context of use when designing AR reading UIs. The presented study with 12 participants provides insights into the effects of AR glasses usage in dual-task situations and several design recommendations are derived from the results.',\n",
       " 'Speed has become a way of life. We are asymptotically piling data. Speed can be achieved with new design processes, techniques, and Technology. Innovations AR and VR are just some of the many forms of technologies that will play a key role in shaping the Architecture and Planning of tomorrow, making it future-ready and ushering in a new age of innovation. AR and VR in Architecture & Planning were introduced as assisting tools and has helped generate multiple design options, expanded possibilities of visualization, and provided us with more enhanced, detailed, and specific experience in real-time; enabling us to see the resultsof work on hand well before the commencement of the project. These tools are further developed for city development decisions, helping citizens interact with local authorities, access public services, and plan their commute. After reviewing multiple research papers, it had been observed that each one is moving forward with the changes brought by it, without entirely understanding its role. This paper provides a summary of theappliance of AR & VR in architecture and planning.',\n",
       " 'Creating pedagogically sound, interactive Augmented Reality (AR) experiences supporting situated and experiential learning remains a challenge to teachers without programming skills. To integrate AR in the everyday classroom, teachers need to be capable of designing their own immersive experiences for their students, which is why an analysis of existing authoring toolkits is necessary to identify suitable tools for educational application development and future research directions in terms of educational AR. We identified “easy access”, “GUI-based design”, and “interactive contents” as needs of teachers for designing AR content for the classroom. Based on these needs, we conducted a literature review of 835 documents. Of 80 relevant articles, we included 43 peer-reviewed articles from ACM Digital Library, DBLP, IEEExplore, Scopus, Web of Science, Google Scholar, and miscellaneous other sources in our analysis. We identified 69 different AR authoring toolkits and classified these with regard to their accessibility, their degree of required programming knowledge, and their interactivity. The results show a divergent research landscape with a lack of empirical evaluation. Of 26 openly accessible toolkits, we identified five toolkits addressing the defined needs of teachers for designing interactive AR experiences for the classroom without requiring extensive programming knowledge. We conclude that there are only few tools for the straightforward design of educational AR experiences addressing the needs of teachers and suggest using research-informed and evidence-based criteria for developing AR authoring toolkits for education.',\n",
       " 'Augmented Reality (AR) und Virtual Reality (VR) finden zunehmend Eingang in die Bildungspraxis. Mit ihrem Einsatz sind sowohl Potentiale als auch mögliche Problemlagen für Lehr- und Lernprozesse verbunden. Daher ist es Aufgabe der Lehrerbildung, (angehenden) Lehrpersonen einen Kompetenzerwerb für die Einbindung von AR und VR in Lehr- und Lernprozesse zu ermöglichen. Vor diesem Hintergrund wurde ein interdisziplinäres Konzept für die Hochschullehre entwickelt und hinsichtlich der Zielerreichung empirisch evaluiert. Im Beitrag werden zunächst bedeutsame Gestaltungsaspekte des Konzepts sowie erste Befunde aus einer Pilotuntersuchung vorgestellt. Im Anschluss werden handlungspraktische Erfahrungen der interdisziplinären Zusammenarbeit reflektiert und diskutiert.',\n",
       " 'Augmented Reality (AR) und Virtual Reality (VR) finden zunehmend Eingang in die Bildungspraxis. Mit ihrem Einsatz sind sowohl Potentiale als auch mögliche Problemlagen für Lehr- und Lernprozesse verbunden. Daher ist es Aufgabe der Lehrerbildung, (angehenden) Lehrpersonen einen Kompetenzerwerb für die Einbindung von AR und VR in Lehr- und Lernprozesse zu ermöglichen. Vor diesem Hintergrund wurde ein interdisziplinäres Konzept für die Hochschullehre entwickelt und hinsichtlich der Zielerreichung empirisch evaluiert. Im Beitrag werden zunächst bedeutsame Gestaltungsaspekte des Konzepts sowie erste Befunde aus einer Pilotuntersuchung vorgestellt. Im Anschluss werden handlungspraktische Erfahrungen der interdisziplinären Zusammenarbeit reflektiert und diskutiert.',\n",
       " 'Augmented Reality (AR) und Virtual Reality (VR) finden zunehmend Eingang in die Bildungspraxis. Mit ihrem Einsatz sind sowohl Potentiale als auch mögliche Problemlagen für Lehr- und Lernprozesse verbunden. Daher ist es Aufgabe der Lehrerbildung, (angehenden) Lehrpersonen einen Kompetenzerwerb für die Einbindung von AR und VR in Lehr- und Lernprozesse zu ermöglichen. Vor diesem Hintergrund wurde ein interdisziplinäres Konzept für die Hochschullehre entwickelt und hinsichtlich der Zielerreichung empirisch evaluiert. Im Beitrag werden zunächst bedeutsame Gestaltungsaspekte des Konzepts sowie erste Befunde aus einer Pilotuntersuchung vorgestellt. Im Anschluss werden handlungspraktische Erfahrungen der interdisziplinären Zusammenarbeit reflektiert und diskutiert.',\n",
       " 'Augmented Reality (AR) glasses may be used in diverse mobile and multitasking contexts, for example, while walking. In such contexts, it is particularly important to display information without obscuring essential areas of central vision. Information can, thus, be presented to the peripheral vision. The objective of the present study was to investigate how peripheral visual cues should be displayed in AR to achieve efficient perception during walking. We conducted a pilot study and tested three different versions of directional cues presented while walking or standing still: simply popping up, moving towards the indicated direction, or changing color. The results indicated that the perception of peripheral cues in AR is generally less efficient while walking than standing still. Within the walking condition, the color-changing cue was perceived best.',\n",
       " 'Durch eine zunehmend flexibilisierte Produktion und relativ hohe Personalfluktuation bei einfachen Tätigkeiten wird das Anlernen neuer Mitarbeitender bzw. für neue oder geänderte Produktionsprozesse zunehmend relevanter. Konventionelle Anlernmethoden, bspw. durch Demonstration oder Arbeitspläne, haben Schwächen (bspw. binden sie zusätzliche Arbeitskraft oder sind kognitiv anspruchsvoll), die durch ein Augmented-Reality-(AR-)basiertes Assistenzsystem kompensiert werden können. Ein AR-Assistenzsystem zum Anlernen eines tatsächlichen industriellen Montageprozesses wurde in einer vergleichenden Studie mit 16 Versuchspersonen gegenüber einer konventionellen Papieranleitung evaluiert. Bzgl. Gebrauchstauglichkeit und Beanspruchung deuten sich die Stärken des AR-Ansatzes an. In den Subskalen Anstrengung und Frustration zeigt sich eine statistisch signifikante Reduktion der Beanspruchung. Bezüglich der Montagezeiten konnten statistisch signifikante Vorteile erzielt werden. Ergänzend wurde in einer zweiten Studie eine Videoanleitung mit der AR-Anleitung verglichen. Praktische Relevanz: Die Digitalisierung der Produktion erlaubt nach Kundenwunsch individualisierte Produkte, die mit einer flexibilisierten Produktion einhergehen. Einfache Montageprozesse ändern sich daher häufiger vollständig oder in Teilen. Außerdem wechseln Mitarbeitende aufgrund der monotonen und repetitiven Tätigkeiten häufiger. All das führt zu einem Bedarf an regelmäßigen Anlernprozessen. Ein AR-Assistenzsystem erscheint als aussichtsreicher Ansatz, wobei die Gebrauchstauglichkeit besonders im Fokus stehen sollte, da wenig technische Kenntnisse vorausgesetzt werden können.',\n",
       " 'Mobilität präsentiert sich in Form von Alltags- und Versorgungsmobilität, als Freizeit-, Messe- und Touristenverkehr sowie in vielschichtigen Wirtschaftsbewegungen von Unternehmen und Dienstleistern. Mobilität ist ein Grundbedürfnis und sichert neben der Teilhabe am öffentlichen Leben die Erreichbarkeit des Arbeitsplatzes, also die Teilhabe am ökonomischen Leben. Die Sicherung von Mobilität sowie eine effiziente Ausgestaltung zur Information und Nutzung unter den technischen Rahmenbedingungen sind Aufgaben mit hoher gesellschaftlicher Verantwortung, zu deren Lösung das Projekt RadAR+ einen wesentlichen Beitrag leistete - durch die Entwicklung eines adaptiv lernenden Mobilitätsagenten unter Verwendung von Augmented Reality.',\n",
       " 'This thesis explores potentials of applying spatial visuo-proprioceptive conflicts of the real hand to 3D user interaction in Augmented Reality. A generic framework is proposed which can generate, manage and reduce sensory conflicts at hand level while providing a continuous interaction cycle. Technically, the system is based on a video see-through head-mounted display that allows for embedding the real hand into a virtual scene and to visually manipulate its position in 3D. Two novel methods are introduced on top of this basis: an intuitive virtual object touching paradigm and a hand-displacement-based active pseudo-haptics technique. Both approaches are studied with respect to their benefits, limitations, effects on the behaviour of the user and consequences for the design of Virtual Environments. It is demonstrated that new forms of human-computer interaction are possible exploiting the described visuomotor conflicts of the hand. Promising future perspectives are presented.',\n",
       " 'According to research findings, Augmented Reality (AR) can have a positive impact on student STEM learning and knowledge acquisition. However, sound concepts for assessing this impact especially in an international sample are scarce, even though an international comparative perspective opens up important viewpoints and helps understand individual and national conditions of the usefulness within specific concepts. This paper will introduce the research methodology applied in the European H2020 ARETE project to systematically assess and analyze an intervention utilizing AR in STEM classes in elementary education across Europe, compared within the background work of this research area. This methodology allows for a grounded data collection approach in order to bring forward important research results on a European level.',\n",
       " 'Die Layoutplanung stellt ein Paradebeispiel für die Interdisziplinarität der Fabrikplanung dar. Eine übergreifende Zusammenarbeit bietet Möglichkeiten, Ideen und Erfahrungen möglichst vieler Personen in die Planung miteinzubeziehen, Synergien zu nutzen, eine schnellere Abstimmung des Planungsstands zwischen den Fachbereichen zu realisieren und somit die Effektivität sowie die Effizienz im Planungsprozess zu verbessern. Die Entwicklung leistungsfähiger Rechentechnik sowie die zunehmende Verbreitung mobiler Endgeräte (z.B. Smartphones, Tablet-Computer) eröffnen neue Potenziale und Möglichkeiten zur Nutzung von Augmented Reality (AR) im Anwendungsfeld der Fabrikplanung. Dieser Beitrag beschreibt einen Ansatz zur Anwendung von AR in der partizipativen Layoutplanung.',\n",
       " 'Against the background of the research-based assumption that student motivation is a key factor for successful teaching and learning processes, the following paper presents results from a study evaluating the role of student motivation in Augmented Reality (AR)-enhanced gamified STEM learning settings. In the study, an international sample of 1988 primary school students and 91 teachers used an AR-enhanced gamified STEM learning app (intervention group) or traditional teaching and learning approaches (control group). Student motivation and performance were assessed in a pre and post test design and teachers’ opinions were recorded using online surveys. The results indicate that students and teachers in the intervention group considered the motivational impact of the AR-enhanced gamified app significantly higher, compared to the ratings for alternative media in the control group. The product-moment correlation between motivation and further variables, such as student attitudes and self-efficacy, showed significant, medium strong results which are comparably high in intervention and control group. Only in the intervention group, motivation and learning achievement were correlated. Students and teachers in the intervention group tended to agree on the motivational impact of the study phase while a correlation between the opinions of teachers and students in the control group could not be confirmed. Overall, the study provides evidence of the pedagogical usefulness of AR-enhanced gamified STEM learning applications in terms of supporting student motivation, based on findings from a large international sample.',\n",
       " 'According to research findings, Augmented Reality (AR) can have a positive impact on student STEM learning and knowledge acquisition. However, sound concepts for assessing this impact especially in an international sample are scarce, even though an international comparative perspective opens up important viewpoints and helps understand individual and national conditions of the usefulness within specific concepts. This paper will introduce the research methodology applied in the European H2020 ARETE project to systematically assess and analyze an intervention utilizing AR in STEM classes in elementary education across Europe, compared within the background work of this research area. This methodology allows for a grounded data collection approach in order to bring forward important research results on a European level.',\n",
       " 'Research findings suggest that Augmented Reality (AR) can be a beneficial tool for teaching and learning practices but brings about certain challenges at the same time. Teachers are central stakeholders in the educational use of AR; given their daily classroom practice, they are also experts in the evaluation of the advantages and disadvantages of respective innovative educational technologies. Yet, few studies systematically analysed teachers’ opinions and experiences about the advantages and problems in the educational use of AR technologies so far. In response to this desideratum, the following paper presents results from a systematic and category-based qualitative content analysis based on an interview study. The input was collected from a heterogeneous and international convenience sample of n=11 teachers after they had piloted one out of three AR-enhanced learning apps in their daily classroom practice. The results confirm that increased student motivation and classroom engagement and advanced learning achievements are considered the predominant advantages and that issues with software and technology were perceived as the biggest problems in the pilot study. Furthermore, dimensions of motivation and classroom engagement are considered in detail to achieve an in-depth analysis and to bring forward important recommendations for further developments and for teachers’ future educational use of AR technologies.',\n",
       " 'In order to support the decision-making process of industry on how to implement Augmented Reality (AR) in production, this article wants to provide guidance through a set of comparative user studies. The results are obtained from the feedback of 160 participants who performed the same repair task on a switch cabinet of an industrial robot. The studies compare several AR instruction applications on different display devices (head-mounted display, handheld tablet PC and projection-based spatial AR) with baseline conditions (paper instructions and phone support), both in a single-user and a collaborative setting. Next to insights on the performance of the individual device types for the single mode operation, the study is able to show significant indications on AR techniques are being especially helpful in a collaborative setting.',\n",
       " 'Augmented reality (AR) environments are suffering from a limited workspace. In addition, registration issues are also increased by the use of a mobile camera on the user that provides a first-person perspective (1PP). Using several fixed cameras reduces the registration issues and, depending on their location, the workspace could also be enlarged. In this case of an extended workspace, it has been shown that third-person perspective (3PP) is sometimes preferred by the user. Based on the previous hypotheses, we developed a system working with several fixed cameras that can provide 3PP to a user wearing a video see-through HMD. Our system uses an intelligent switch to propose our best view to the user, i.e. avoiding markers occlusion and taking into account user displacements. We present in this paper, such a system, its decision algorithm, and the discussion of obtained results that seem to be very promising within the AR domain.',\n",
       " \"The embodiment of avatars in virtual reality (VR) is a promising tool for enhancing the user's mental health. A great example is the treatment of body image disturbances, where eliciting a full-body illusion can help identify, visualize, and modulate persisting misperceptions. Augmented reality (AR) could complement recent advances in the field by incorporating real elements, such as the therapist or the user's real body, into therapeutic scenarios. However, research on the use of AR in this context is very sparse. Therefore, we present a holographic AR mirror system based on an optical see-through (OST) device and markerless body tracking, collect valuable qualitative feedback regarding its user experience, and compare quantitative results regarding presence, embodiment, and body weight perception to similar systems using video see-through (VST) AR and VR. For our OST AR system, a total of 27 normal-weight female participants provided predominantly positive feedback on display properties (field of view, luminosity, and transparency of virtual objects), body tracking, and the perception of the avatar’s appearance and movements. In the quantitative comparison to the VST AR and VR systems, participants reported significantly lower feelings of presence, while they estimated the body weight of the generic avatar significantly higher when using our OST AR system. For virtual body ownership and agency, we found only partially significant differences. In summary, our study shows the general applicability of OST AR in the given context offering huge potential in future therapeutic scenarios. However, the comparative evaluation between OST AR, VST AR, and VR also revealed significant differences in relevant measures. Future work is mandatory to corroborate our findings and to classify the significance in a therapeutic context.\",\n",
       " \"The embodiment of avatars in virtual reality (VR) is a promising tool for enhancing the user's mental health. A great example is the treatment of body image disturbances, where eliciting a full-body illusion can help identify, visualize, and modulate persisting misperceptions. Augmented reality (AR) could complement recent advances in the field by incorporating real elements, such as the therapist or the user's real body, into therapeutic scenarios. However, research on the use of AR in this context is very sparse. Therefore, we present a holographic AR mirror system based on an optical see-through (OST) device and markerless body tracking, collect valuable qualitative feedback regarding its user experience, and compare quantitative results regarding presence, embodiment, and body weight perception to similar systems using video see-through (VST) AR and VR. For our OST AR system, a total of 27 normal-weight female participants provided predominantly positive feedback on display properties (field of view, luminosity, and transparency of virtual objects), body tracking, and the perception of the avatar’s appearance and movements. In the quantitative comparison to the VST AR and VR systems, participants reported significantly lower feelings of presence, while they estimated the body weight of the generic avatar significantly higher when using our OST AR system. For virtual body ownership and agency, we found only partially significant differences. In summary, our study shows the general applicability of OST AR in the given context offering huge potential in future therapeutic scenarios. However, the comparative evaluation between OST AR, VST AR, and VR also revealed significant differences in relevant measures. Future work is mandatory to corroborate our findings and to classify the significance in a therapeutic context.\",\n",
       " 'Recent work has demonstrated that fall risk can be attributed to cognitive as well as motor deficits. Indeed, everyday walking in complex environments utilizes executive function, dual tasking, planning and scanning, all while walking forward. Pilot studies suggest that a multi-modal intervention that combines treadmill training to target motor function and a virtual reality obstacle course to address the cognitive components of fall risk may be used to successfully address the motor-cognitive interactions that are fundamental for fall risk reduction. The proposed randomized controlled trial will evaluate the effects of treadmill training augmented with virtual reality on fall risk.',\n",
       " 'Ziel des Leitprojektes ist die benutzerfreundliche und anwendungsgetriebene Entwicklung von Augmented-Reality-Technologien zur Unterstützung von Arbeitsprozessen in Entwicklung, Produktion und Service für komplexe technische Produkte und Anlagen. Hierbei werden arbeitswissenschaftliche Methoden angewendet, um sicherzustellen, dass die entwickelten AR-Systeme den Anforderungen von Mitarbeitern und Arbeitsprozessen entsprechen, die Benutzungsschnittstellen ergonomisch gestaltet sind und in die technische Spezifikation einfließen kann, durch AR-Funktionalitäten Verbesserungen der Arbeitsorganisation ermöglicht und die wirtschaftlichen Effekte dargelegt werden sowie letztlich innovative Trainingsverfahren entstehen, die für Aus- und Fortbildung genutzt werden können.',\n",
       " 'Background: Imprecise carbohydrate counting as a measure to guide the treatment of diabetes may be a source of errors resulting in problems in glycemic control. Exact measurements can be tedious, leading most patients to estimate their carbohydrate intake. In the presented pilot study a smartphone application (BEAR), that guides the estimation of the amounts of carbohydrates, was used by a group of diabetic patients. Methods: Eight adult patients with diabetes mellitus type 1 were recruited for the study. At the beginning of the study patients were introduced to BEAR in sessions lasting 45 minutes per patient. Patients redraw the real food in 3D on the smartphone screen. Based on a selected food type and the 3D form created using BEAR an estimation of carbohydrate content is calculated. Patients were supplied with the application on their personal smartphone or a loaner device and were instructed to use the application in real-world context during the study period. For evaluation purpose a test measuring carbohydrate estimation quality was designed and performed at the beginning and the end of the study. Results: In 44% of the estimations performed at the end of the study the error reduced by at least 6 grams of carbohydrate. This improvement occurred albeit several problems with the usage of BEAR were reported. Conclusions: Despite user interaction problems in this group of patients the provided intervention resulted in a reduction in the absolute error of carbohydrate estimation. Intervention with smartphone applications to assist carbohydrate counting apparently results in more accurate estimations.',\n",
       " 'This article compares two state-of-the-art text input techniques between non-stationary virtual reality (VR) and video see-through augmented reality (VST AR) use-cases as XR display condition. The developed contact-based mid-air virtual tap and wordgesture (swipe) keyboard provide established support functions for text correction, word suggestions, capitalization, and punctuation. A user evaluation with 64 participants revealed that XR displays and input techniques strongly affect text entry performance, while subjective measures are only influenced by the input techniques. We found significantly higher usability and user experience ratings for tap keyboards compared to swipe keyboards in both VR and VST AR. Task load was also lower for tap keyboards. In terms of performance, both input techniques were significantly faster in VR than in VST AR. Further, the tap keyboard was significantly faster than the swipe keyboard in VR. Participants showed a significant learning effect with only ten sentences typed per condition. Our results are consistent with previous work in VR and optical see-through (OST) AR, but additionally provide novel insights into usability and performance of the selected text input techniques for VST AR. The significant differences in subjective and objective measures emphasize the importance of specific evaluations for each possible combination of input techniques and XR displays to provide reusable, reliable, and high-quality text input solutions. With our work, we form a foundation for future research and XR workspaces. Our reference implementation is publicly available to encourage replicability and reuse in future XR workspaces.',\n",
       " 'The paper describes two experiments for investigating the influence of different levels of camera displacement on hand-eye coordination while using a video see-through head-mounted display. During the first experiment 15 camera positions with five levels of height displacement and three levels of depth displacement were compared in four different tasks. Using a two-way ANOVA the comparison of the calculated performance characteristic values showed significant influence of height displacement on hand-eye coordination. In conclusion cameras should be placed above or below eye level, but by no more than 35 mm, in order to preserve hand-eye coordination. In the second experiment, a mirror system was used to check hand-eye coordination in an exemplary medical task allowing the cameras to be placed virtually at eye level. A significant decrease in accuracy was found while using the head-mounted display compared to direct view. Finally, the mirror system was compared to the 15 camera positions using the data from the same tasks. Significant differences in performance were found between the mirror system and eye level position as well as the position slightly below eye level. The results of the experiment provide design recommendations for developers and users of video see-through systems.',\n",
       " 'The rapid development of 3D scanning technology combined with state-of-the-art mapping algorithms allows to capture 3D point clouds with high resolution and accuracy. The high amount of data collected with LiDAR, RGB-D cameras or generated through SfM approaches makes the direct use of the recorded data for realistic rendering and simulation problematic. Therefore, these point clouds have to be transformed into representations that fulfill the computational requirements for VR and AR setups. In this tutorial participants will be introduced to state-of-the-art methods in point cloud processing and surface reconstruction with open source software to learn the benefits for AR and VR applications by interleaved presentations, software demonstrations and software trials. The focus lies on 3D point cloud data structures (range images, octrees, k-d trees) and algorithms, and their implementation in C/C++. Surface reconstruction using Marching Cubes and other meshing methods will play another central role. Reference material for subtopics like 3D point cloud registration and SLAM, calibration, filtering, segmentation, meshing, and large scale surface reconstruction will be provided. Participants are invited to bring their Linux, MacOS or Windows laptops to gain hands-on experience on practical problems occuring when working with large scale 3D point clouds in VR and AR applications.',\n",
       " 'Bisher haben wir uns größtenteils mit der Darstellung von Informationen befasst. Wir haben gesehen, wie 3-D-Grafik erzeugt wird und welche Arten von Displays es gibt, um sie sichtbar zu machen. Weiterhin haben wir gesehen, was Tracking ist und wie man damit Systeme aufsetzen kann, die virtuelle Objekte an der gewünschten Position darstellen. Mit anderen Worten kann man sagen, dass wir fast ausschließlich die Ausgabe von Informationen dargestellt haben. Darum befasst sich dieses Kapitel mit der anderen Seite, der Eingabe von Daten und Informationen in ein AR-System. Zuerst werfen wir einen Blick auf Arten und Möglichkeiten der Eingabe, um uns danach mit der Interaktion mit AR-Systemen zu befassen. Auch dieses Kapitel beschließen wir mit einer Übung. Wir werden unser eigenes System so erweitern, dass wir nicht nur Objekte im Raum bewegen, sondern auch mit ihnen interagieren können.',\n",
       " 'Neben der Darstellung von virtuellen Objekten ist die Lagebestimmung der zweite zentrale Bestandteil von AR-Systemen. Die Lage des Betrachters, und oft auch die Lage wichtiger Gegenstände in der Umgebung oder der Ort, an dem virtuelle Objekte erscheinen sollen, müssen dem AR-System zur Verfügung stehen. Der Prozess der Lagebestimmung wird gemeinhin als Tracking bezeichnet.',\n",
       " 'Der Autor befasst sich mit der Überlagerung der realen Welt durch computergenerierte virtuelle Objekte. Die drei grundlegenden Bausteine dieser erweiterten Realität, Darstellung, Tracking und Benutzerinteraktion, werden Schritt für Schritt eingeführt und miteinander in Zusammenhang gebracht. Ein Überblick über Anwendungen in Forschung und Industrie liefert Anregungen für die Entwicklung eigener Systeme.',\n",
       " 'Die letzten Kapitel stellten die drei großen Bereiche der AR und verschiedene Anwendungen vor. Wir haben gesehen, wie ein AR-System dem Betrachter Informationen dargestellt, wie durch den Einsatz von Trackingsystemen die Positionierung von Objekten automatisiert wird und schließlich, wie wir mit AR Systemen interagieren können. Wer aber beginnt, seine eigenen großen Pläne in einem eigenen System umzusetzen, wird schnell feststellen, dass eine Sache fehlt. Das Trackingsystem hat eine eng begrenzte Reichweite – wie können wir uns mit einem AR-System in einem größeren Gebiet bewegen?',\n",
       " 'Die letzten Kapitel haben die drei zentralen Elemente der AR studiert. Damit steht der Entwicklung eines eigenen AR-Systems eigentlich nichts mehr im Wege. Um Anregungen für eigene Anwendungen zu schaffen, stellt dieses Kapitel ein Spektrum bereits existierender AR-Anwendungen vor. Sie reichen von Prototypen und Forschungssystemen hin zu Systemen, die wirklich in bestimmten Bereichen genutzt werden. Bei jedem vorgestellten System wird, soweit möglich, der Bezug zu den in diesem Buch vorgestellten Techniken aufgebaut. Im Weiteren werden Erfahrungen der Entwickler des jeweiligen Systems wiedergegeben.',\n",
       " 'Dieses Kapitel behandelt die zwei für die Darstellung von virtuellen Objekten notwendigen Hauptelemente, die softwaretechnischen Grundlagen für den Umgang mit räumlichen Strukturen und die Hardware, auf der die Ausgaben dargestellt werden. Zunächst einmal werden fundamentale Grundlagen der Computergrafik vorgestellt, anschließend werden weiterführende Konzepte wie Szenengraphen angesprochen. Aufbauend auf der Erstellung von 3-D-Objekten werden verschiedene Anzeigesysteme und -displays vorgestellt. Daran anschließend vertieft eine Übung den bearbeiteten Stoff. Dabei wird der Monitor des eigenen Rechners zu einem AR-Display gemacht. Die Übung überlagert ein statisches Bild mit einer AR-Darstellung. Das Kapitel schließt mit einem Überblick über nicht-visuelle Anzeigen, wobei vor allem auf akustische und haptische Anzeigen eingegangen wird.',\n",
       " 'Das vorhergehende Kapitel kann derart aufgefasst werden, dass wir bald in einer augmentierten Welt leben werden und jederzeit von weiteren Informationen und zusätzlichen 3-D-Objekten umgeben sind. Man neigt dazu, das Bild der Holobench aus Star Trek in die Realität zu übertragen. Virtuelle Objekte schmiegen sich perfekt in die Realität ein und sind nicht mehr von anderen zu unterscheiden. Die Zukunft ist nicht vorherzusehen, aber in den kommenden Jahren werden virtuelle Objekte noch als solche erkennbar sein. Sie werden nicht genau platziert sein, werden hier und da im Bild springen und nicht die plastische Tiefe von echten Gegenständen haben. Wie bei der VR sind die Erwartungen sehr hoch gesteckt und mit den in Augenblick zu Verfügung stehenden Mitteln nicht erreichbar. Es mag zu erwarten sein, dass die Kosten für qualitativ hochwertige Ausrüstung und der zeitliche Aufwand, um ein AR-System in Betrieb zu nehmen, in den kommenden Jahren sinken. Die Leistung von Grafikkarten und Prozessoren scheint in immer neue Dimensionen vorzustoßen und AR Frameworks werden langsam erwachsen.',\n",
       " 'Im Produktionsbereich bilden Montagetätigkeiten am Ende der Produktionskette einen sehr wichtigen Bestandteil der Wertschöpfungskette. Landau et al. (2001) schreiben von über einer Million Montagearbeitsplätzen in der Fahrzeug- und Elektroindustrie sowie im Maschinen und Anlagenbau. Auch die manuelle Montage ist in vielen Bereichen der Industrie weit verbreitet, sie stellt einen wesentlichen Bestandteil der Produktions- und Servicebereiche der in diesem Sektor arbeitenden Unternehmen dar. Mit dem Aufkommen von Augmented Reality (AR) als neuer Form der Mensch-Rechner-Interaktion eröffnen sich Potentiale zur Unterstützung an Montagearbeitsplätzen, die sich auch auf die Arbeitsorganisation auswirken. Gegenstand dieses Kapitels ist die Untersuchung von AR-Systemen unter möglichst realistischen Bedingungen an realen Montagearbeitsplätzen. Im Gegensatz zu den bei Wiedenmaier (2004) beschrieben quantitativen Untersuchungen zur Einsetzbarkeit von AR-Technologien in der manuellen Montage und der quantitativen Evaluation von verbesserten Darstellungsmöglichkeiten von virtuellen Objekten gehen die hier gewählten Beispiele auf einem qualitativen Niveau einen Schritt weiter. In den beiden Montageszenarien werden alle Arbeitstätigkeiten und damit die gesamten Arbeitsplätze neu gestaltet und mit den zukünftigen Arbeitspersonen qualitativ evaluiert. Ein weiterer interessanter Ansatz ist der einer Produktion vorgelagerte Bereich der kooperativen Anlagenplanung. Dabei können Kooperationspartner aus verschiedenen, räumlich verteilten Unternehmensteilen oder einzelnen Unternehmen eine Produktionsanlage planen. Der Evaluationsfokus liegt im Bereich der Wirtschaftlichkeit eines Einsatzes von AR-Netcollaboration. Im Gegensatz zu den beiden Montageszenarien, bei denen aufgrund der permanenten Arbeit mit den AR-Systemen das Usability Engineering im Mittelpunkt steht, wird hier vor allem die Zeit- und Kostenersparnis in den Vordergrund gestellt. Dennoch darf auf bei der kooperativen Anlagenplanung die Benutzbarkeit nicht außer Acht gelassen werden.']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean\n",
    "pattern = \"\\\\\\\\textquotedbl|\\r|\\n|\\t|\\\\xa0|\\s{3,}|`|'{2,}\"\n",
    "df_clean['abstract'] = df_clean['abstract'].str.strip().replace(pattern, ' ', regex=True)\n",
    "df_clean['abstract'] = df_clean['abstract'].str.strip().replace('\\s{2,}', ' ', regex=True)\n",
    "\n",
    "# Check\n",
    "df_clean[(df_clean['abstract'].notna())]['abstract'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for relevant **PyTerrier** columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "      <th>intraHash</th>\n",
       "      <th>label</th>\n",
       "      <th>user</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "      <th>changeDate</th>\n",
       "      <th>count</th>\n",
       "      <th>...</th>\n",
       "      <th>pages</th>\n",
       "      <th>abstract</th>\n",
       "      <th>isbn</th>\n",
       "      <th>bibtexKey</th>\n",
       "      <th>journal</th>\n",
       "      <th>series</th>\n",
       "      <th>volume</th>\n",
       "      <th>number</th>\n",
       "      <th>ee</th>\n",
       "      <th>search_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Publication</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2e033978aa497...</td>\n",
       "      <td>[greenbib]</td>\n",
       "      <td>e033978aa4971fb861cc33beb8c3bb7c</td>\n",
       "      <td>How to start a library makerspace</td>\n",
       "      <td>bibgreen</td>\n",
       "      <td></td>\n",
       "      <td>2023-02-05 21:50:14</td>\n",
       "      <td>2023-02-16 11:47:43</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3-18</td>\n",
       "      <td>You may have heard the term makerspace and won...</td>\n",
       "      <td>9780838915042</td>\n",
       "      <td>Bronkar.2017</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Publication</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2fb48ac294725...</td>\n",
       "      <td>[assessment, outcomes, makerspace, learningana...</td>\n",
       "      <td>fb48ac2947255e4aca45f9dcc913a382</td>\n",
       "      <td>Informing Makerspace Outcomes Through a Lingui...</td>\n",
       "      <td>ereidt</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-18 21:35:21</td>\n",
       "      <td>2020-02-18 21:35:21</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>A growing body of research focuses on what out...</td>\n",
       "      <td>None</td>\n",
       "      <td>Oliver2020</td>\n",
       "      <td>International Journal of Science and Mathemati...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Publication</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2789d019603b4...</td>\n",
       "      <td>[greenbib]</td>\n",
       "      <td>789d019603b437609dec3272a28f24a9</td>\n",
       "      <td>The makerspace librarian's sourcebook</td>\n",
       "      <td>bibgreen</td>\n",
       "      <td></td>\n",
       "      <td>2023-02-05 21:52:55</td>\n",
       "      <td>2023-02-05 21:52:55</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>The Makerspace Librarian's Sourcebook is an es...</td>\n",
       "      <td>9780838915042</td>\n",
       "      <td>Kroski.2017</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Publication</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2309634b7a919...</td>\n",
       "      <td>[greenbib]</td>\n",
       "      <td>309634b7a919e39e8f8f3289b783ff57</td>\n",
       "      <td>Terrific makerspace projects: A practical guid...</td>\n",
       "      <td>bibgreen</td>\n",
       "      <td></td>\n",
       "      <td>2023-02-05 16:28:34</td>\n",
       "      <td>2023-02-05 16:28:34</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Step-by-step instructions to guide you through...</td>\n",
       "      <td>9781538131824</td>\n",
       "      <td>Denzer.2020</td>\n",
       "      <td>None</td>\n",
       "      <td>Practical guides for librarians</td>\n",
       "      <td>no. 67</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Publication</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/21c020a4a195d...</td>\n",
       "      <td>[greenbib]</td>\n",
       "      <td>1c020a4a195d8f9dcad4ed295bfbf0e5</td>\n",
       "      <td>Sustainability: Keeping the library makerspace...</td>\n",
       "      <td>bibgreen</td>\n",
       "      <td></td>\n",
       "      <td>2023-02-16 11:58:46</td>\n",
       "      <td>2023-02-16 11:58:46</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>325-344</td>\n",
       "      <td>None</td>\n",
       "      <td>9780838915042</td>\n",
       "      <td>Ginsberg.2017</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           type                                                 id  \\\n",
       "38  Publication  https://www.bibsonomy.org/bibtex/2e033978aa497...   \n",
       "39  Publication  https://www.bibsonomy.org/bibtex/2fb48ac294725...   \n",
       "40  Publication  https://www.bibsonomy.org/bibtex/2789d019603b4...   \n",
       "41  Publication  https://www.bibsonomy.org/bibtex/2309634b7a919...   \n",
       "42  Publication  https://www.bibsonomy.org/bibtex/21c020a4a195d...   \n",
       "\n",
       "                                                 tags  \\\n",
       "38                                         [greenbib]   \n",
       "39  [assessment, outcomes, makerspace, learningana...   \n",
       "40                                         [greenbib]   \n",
       "41                                         [greenbib]   \n",
       "42                                         [greenbib]   \n",
       "\n",
       "                           intraHash  \\\n",
       "38  e033978aa4971fb861cc33beb8c3bb7c   \n",
       "39  fb48ac2947255e4aca45f9dcc913a382   \n",
       "40  789d019603b437609dec3272a28f24a9   \n",
       "41  309634b7a919e39e8f8f3289b783ff57   \n",
       "42  1c020a4a195d8f9dcad4ed295bfbf0e5   \n",
       "\n",
       "                                                label      user description  \\\n",
       "38                  How to start a library makerspace  bibgreen               \n",
       "39  Informing Makerspace Outcomes Through a Lingui...    ereidt               \n",
       "40              The makerspace librarian's sourcebook  bibgreen               \n",
       "41  Terrific makerspace projects: A practical guid...  bibgreen               \n",
       "42  Sustainability: Keeping the library makerspace...  bibgreen               \n",
       "\n",
       "                  date           changeDate  count  ...    pages  \\\n",
       "38 2023-02-05 21:50:14  2023-02-16 11:47:43      1  ...     3-18   \n",
       "39 2020-02-18 21:35:21  2020-02-18 21:35:21      1  ...     None   \n",
       "40 2023-02-05 21:52:55  2023-02-05 21:52:55      1  ...     None   \n",
       "41 2023-02-05 16:28:34  2023-02-05 16:28:34      1  ...     None   \n",
       "42 2023-02-16 11:58:46  2023-02-16 11:58:46      1  ...  325-344   \n",
       "\n",
       "                                             abstract           isbn  \\\n",
       "38  You may have heard the term makerspace and won...  9780838915042   \n",
       "39  A growing body of research focuses on what out...           None   \n",
       "40  The Makerspace Librarian's Sourcebook is an es...  9780838915042   \n",
       "41  Step-by-step instructions to guide you through...  9781538131824   \n",
       "42                                               None  9780838915042   \n",
       "\n",
       "        bibtexKey                                            journal  \\\n",
       "38   Bronkar.2017                                               None   \n",
       "39     Oliver2020  International Journal of Science and Mathemati...   \n",
       "40    Kroski.2017                                               None   \n",
       "41    Denzer.2020                                               None   \n",
       "42  Ginsberg.2017                                               None   \n",
       "\n",
       "                             series  volume number    ee search_string  \n",
       "38                             None    None   None  None    makerspace  \n",
       "39                             None    None   None  None    makerspace  \n",
       "40                             None    None   None  None    makerspace  \n",
       "41  Practical guides for librarians  no. 67   None  None    makerspace  \n",
       "42                             None    None   None  None    makerspace  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only publications first\n",
    "df_filtered = df_clean.query('type == \"Publication\"')\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_string</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>tags</th>\n",
       "      <th>pub-type</th>\n",
       "      <th>publisher</th>\n",
       "      <th>isbn</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>makerspace</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2e033978aa497...</td>\n",
       "      <td>[Cherie Bronkar]</td>\n",
       "      <td>How to start a library makerspace</td>\n",
       "      <td>2017</td>\n",
       "      <td>You may have heard the term makerspace and won...</td>\n",
       "      <td>[greenbib]</td>\n",
       "      <td>incollection</td>\n",
       "      <td>ALA Editions</td>\n",
       "      <td>9780838915042</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>makerspace</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2fb48ac294725...</td>\n",
       "      <td>[Kevin M. Oliver, Jennifer K. Houchins, Robert...</td>\n",
       "      <td>Informing Makerspace Outcomes Through a Lingui...</td>\n",
       "      <td>2020</td>\n",
       "      <td>A growing body of research focuses on what out...</td>\n",
       "      <td>[assessment, outcomes, makerspace, learningana...</td>\n",
       "      <td>article</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://doi.org/10.1007/s10763-020-10060-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>makerspace</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2789d019603b4...</td>\n",
       "      <td>None</td>\n",
       "      <td>The makerspace librarian's sourcebook</td>\n",
       "      <td>2017</td>\n",
       "      <td>The Makerspace Librarian's Sourcebook is an es...</td>\n",
       "      <td>[greenbib]</td>\n",
       "      <td>standard</td>\n",
       "      <td>ALA Editions</td>\n",
       "      <td>9780838915042</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   search_string                                                 id  \\\n",
       "38    makerspace  https://www.bibsonomy.org/bibtex/2e033978aa497...   \n",
       "39    makerspace  https://www.bibsonomy.org/bibtex/2fb48ac294725...   \n",
       "40    makerspace  https://www.bibsonomy.org/bibtex/2789d019603b4...   \n",
       "\n",
       "                                               author  \\\n",
       "38                                   [Cherie Bronkar]   \n",
       "39  [Kevin M. Oliver, Jennifer K. Houchins, Robert...   \n",
       "40                                               None   \n",
       "\n",
       "                                                label  year  \\\n",
       "38                  How to start a library makerspace  2017   \n",
       "39  Informing Makerspace Outcomes Through a Lingui...  2020   \n",
       "40              The makerspace librarian's sourcebook  2017   \n",
       "\n",
       "                                             abstract  \\\n",
       "38  You may have heard the term makerspace and won...   \n",
       "39  A growing body of research focuses on what out...   \n",
       "40  The Makerspace Librarian's Sourcebook is an es...   \n",
       "\n",
       "                                                 tags      pub-type  \\\n",
       "38                                         [greenbib]  incollection   \n",
       "39  [assessment, outcomes, makerspace, learningana...       article   \n",
       "40                                         [greenbib]      standard   \n",
       "\n",
       "       publisher           isbn                                         url  \n",
       "38  ALA Editions  9780838915042                                              \n",
       "39          None           None  https://doi.org/10.1007/s10763-020-10060-2  \n",
       "40  ALA Editions  9780838915042                                              "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for relevant columns\n",
    "columns = ['search_string', 'id', 'author', \n",
    "           'label', 'year', 'abstract', 'tags', \n",
    "           'pub-type', 'publisher', 'isbn', 'url']\n",
    "\n",
    "df_filtered = df_filtered[columns]\n",
    "df_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename columns for easier **PyTerrier** import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_string</th>\n",
       "      <th>docno</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>tags</th>\n",
       "      <th>pub-type</th>\n",
       "      <th>publisher</th>\n",
       "      <th>isbn</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>makerspace</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2e033978aa497...</td>\n",
       "      <td>[Cherie Bronkar]</td>\n",
       "      <td>How to start a library makerspace</td>\n",
       "      <td>2017</td>\n",
       "      <td>You may have heard the term makerspace and won...</td>\n",
       "      <td>[greenbib]</td>\n",
       "      <td>incollection</td>\n",
       "      <td>ALA Editions</td>\n",
       "      <td>9780838915042</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>makerspace</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2fb48ac294725...</td>\n",
       "      <td>[Kevin M. Oliver, Jennifer K. Houchins, Robert...</td>\n",
       "      <td>Informing Makerspace Outcomes Through a Lingui...</td>\n",
       "      <td>2020</td>\n",
       "      <td>A growing body of research focuses on what out...</td>\n",
       "      <td>[assessment, outcomes, makerspace, learningana...</td>\n",
       "      <td>article</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://doi.org/10.1007/s10763-020-10060-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>makerspace</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2789d019603b4...</td>\n",
       "      <td>None</td>\n",
       "      <td>The makerspace librarian's sourcebook</td>\n",
       "      <td>2017</td>\n",
       "      <td>The Makerspace Librarian's Sourcebook is an es...</td>\n",
       "      <td>[greenbib]</td>\n",
       "      <td>standard</td>\n",
       "      <td>ALA Editions</td>\n",
       "      <td>9780838915042</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   search_string                                              docno  \\\n",
       "38    makerspace  https://www.bibsonomy.org/bibtex/2e033978aa497...   \n",
       "39    makerspace  https://www.bibsonomy.org/bibtex/2fb48ac294725...   \n",
       "40    makerspace  https://www.bibsonomy.org/bibtex/2789d019603b4...   \n",
       "\n",
       "                                               author  \\\n",
       "38                                   [Cherie Bronkar]   \n",
       "39  [Kevin M. Oliver, Jennifer K. Houchins, Robert...   \n",
       "40                                               None   \n",
       "\n",
       "                                                 text  year  \\\n",
       "38                  How to start a library makerspace  2017   \n",
       "39  Informing Makerspace Outcomes Through a Lingui...  2020   \n",
       "40              The makerspace librarian's sourcebook  2017   \n",
       "\n",
       "                                             abstract  \\\n",
       "38  You may have heard the term makerspace and won...   \n",
       "39  A growing body of research focuses on what out...   \n",
       "40  The Makerspace Librarian's Sourcebook is an es...   \n",
       "\n",
       "                                                 tags      pub-type  \\\n",
       "38                                         [greenbib]  incollection   \n",
       "39  [assessment, outcomes, makerspace, learningana...       article   \n",
       "40                                         [greenbib]      standard   \n",
       "\n",
       "       publisher           isbn                                         url  \n",
       "38  ALA Editions  9780838915042                                              \n",
       "39          None           None  https://doi.org/10.1007/s10763-020-10060-2  \n",
       "40  ALA Editions  9780838915042                                              "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df_filtered.rename(columns={'id': 'docno',\n",
    "                                          'label': 'text'})\n",
    "df_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output location for cleaned json_data\n",
    "json_outfile = './bibsonomy_clean_data/makerspace_vr.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_json(json_outfile, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init **PyTerrier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_string</th>\n",
       "      <th>docno</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>tags</th>\n",
       "      <th>pub-type</th>\n",
       "      <th>publisher</th>\n",
       "      <th>isbn</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>makerspace</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2e033978aa497...</td>\n",
       "      <td>[Cherie Bronkar]</td>\n",
       "      <td>How to start a library makerspace</td>\n",
       "      <td>2017</td>\n",
       "      <td>You may have heard the term makerspace and won...</td>\n",
       "      <td>[greenbib]</td>\n",
       "      <td>incollection</td>\n",
       "      <td>ALA Editions</td>\n",
       "      <td>9.780839e+12</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>makerspace</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2fb48ac294725...</td>\n",
       "      <td>[Kevin M. Oliver, Jennifer K. Houchins, Robert...</td>\n",
       "      <td>Informing Makerspace Outcomes Through a Lingui...</td>\n",
       "      <td>2020</td>\n",
       "      <td>A growing body of research focuses on what out...</td>\n",
       "      <td>[assessment, outcomes, makerspace, learningana...</td>\n",
       "      <td>article</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://doi.org/10.1007/s10763-020-10060-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  search_string                                              docno  \\\n",
       "0    makerspace  https://www.bibsonomy.org/bibtex/2e033978aa497...   \n",
       "1    makerspace  https://www.bibsonomy.org/bibtex/2fb48ac294725...   \n",
       "\n",
       "                                              author  \\\n",
       "0                                   [Cherie Bronkar]   \n",
       "1  [Kevin M. Oliver, Jennifer K. Houchins, Robert...   \n",
       "\n",
       "                                                text  year  \\\n",
       "0                  How to start a library makerspace  2017   \n",
       "1  Informing Makerspace Outcomes Through a Lingui...  2020   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  You may have heard the term makerspace and won...   \n",
       "1  A growing body of research focuses on what out...   \n",
       "\n",
       "                                                tags      pub-type  \\\n",
       "0                                         [greenbib]  incollection   \n",
       "1  [assessment, outcomes, makerspace, learningana...       article   \n",
       "\n",
       "      publisher          isbn                                         url  \n",
       "0  ALA Editions  9.780839e+12                                              \n",
       "1          None           NaN  https://doi.org/10.1007/s10763-020-10060-2  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = pd.read_json(json_outfile)\n",
    "df_filtered.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `pt.IterDictIndexer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'search_string': 'makerspace',\n",
       "  'docno': 'https://www.bibsonomy.org/bibtex/2e033978aa4971fb861cc33beb8c3bb7c/bibgreen',\n",
       "  'author': ['Cherie Bronkar'],\n",
       "  'text': 'How to start a library makerspace',\n",
       "  'year': 2017,\n",
       "  'abstract': \"You may have heard the term makerspace and wondered what it meant. Makerspaces are, simply put, places where people gather to make things. Although that may sound like a simplistic definition, the things that can be created in a makerspace vary a great deal. Makerspaces can be high tech, low tech, and everything in between. A makerspace's offerings revolve around the needs of the community it serves, but the one thing all have in common is that they bring people together to share ideas. Typically, the first thing that comes to mind when thinking about mak- erspaces is 3D printing, but when it comes to what’s going on in makerspaces around the world, that’s just the tip of the iceberg. Makers create things, ideas, and concepts. Makers work in metal, wood, plastic, fabric, paper, and digital forms. From robotics to crocheting, there are no limits to your makerspace. Let your imagination run wild. In this chapter, we’ll provide the information and ideas to get your maker-spaces up and running based on your unique populations and budgets. You’ll find a myriad of ways to create your makerspace. You’ll also discover ways to ensure your makerspace is fun and functional.\",\n",
       "  'tags': ['greenbib'],\n",
       "  'pub-type': 'incollection',\n",
       "  'publisher': 'ALA Editions',\n",
       "  'isbn': 9780838915042.0,\n",
       "  'url': ''}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = df_filtered.to_dict(orient='records')\n",
    "data_dict[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3149"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to store index\n",
    "index_folder_mult = './makerspace_index_mult'\n",
    "\n",
    "# Dict fields for index\n",
    "fields = ['docno', 'text', 'abstract', 'tags']\n",
    "\n",
    "# Create indexer object for dictionary == IterDictIndexer\n",
    "indexer_mult = pt.IterDictIndexer(index_folder_mult,\n",
    "                                  meta={'docno': 200, 'text': 4096},\n",
    "                                  overwrite=True,\n",
    "                                  stemmer='porter')\n",
    "\n",
    "# Create index by passing data -> Dict and fields\n",
    "index_ref_mult = indexer_mult.index(data_dict, fields=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mult = pt.IndexFactory.of(index_ref_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get index stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3149\n",
      "Number of terms: 6871\n",
      "Number of postings: 78810\n",
      "Number of fields: 4\n",
      "Number of tokens: 103351\n",
      "Field names: [docno, text, abstract, tags]\n",
      "Positions:   false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(index_mult.getCollectionStatistics().toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dict = {}\n",
    "\n",
    "for x in index_mult.getLexicon():\n",
    "    tf_dict[x.getKey()] = x.getValue().frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in index_mult.getLexicon():\n",
    "#     print(x.getKey(), x.getValue())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fragen**:\n",
    "- andere NLP pipeline für deutsche Texte: \n",
    "    - https://pyterrier.readthedocs.io/en/latest/terrier-indexing.html#pyterrier.index.TerrierStemmer\n",
    "- mehrere Stemmer verwenden (Deutsch + Englisch)?\n",
    "- wie Index manuell eingrenzen (um bspw. Einträge wie '00', '0000' auszuschließen)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 71\n",
      "00 3\n",
      "000 1\n",
      "001 15\n",
      "002 2\n",
      "003 1\n",
      "005 2\n",
      "006 1\n",
      "008 1\n",
      "0098 1\n",
      "01 6\n",
      "018 1\n",
      "024 1\n",
      "026 1\n",
      "029 1\n",
      "03 1\n",
      "035 1\n",
      "039 6\n",
      "04 2\n",
      "045 2\n",
      "05 3\n",
      "06 1\n",
      "0i 1\n",
      "1 78\n",
      "10 19\n",
      "101 3\n",
      "102 2\n",
      "103 9\n",
      "105 3\n",
      "10th 2\n",
      "11 5\n",
      "1136 1\n",
      "115 1\n",
      "119 4\n",
      "12 20\n",
      "120 1\n",
      "125 2\n",
      "128 3\n",
      "13 6\n",
      "130 4\n",
      "14 2\n",
      "142 1\n",
      "142p 1\n",
      "14th 4\n",
      "15 11\n",
      "150 1\n",
      "16 3\n",
      "160 1\n",
      "165 2\n",
      "17 2\n",
      "170 1\n",
      "18 1\n",
      "183 1\n",
      "19 2\n",
      "1909 1\n",
      "191 2\n",
      "195 2\n",
      "1960 1\n",
      "1984 1\n",
      "1988 1\n",
      "1990 1\n",
      "1991 1\n",
      "1993 2\n",
      "1995 2\n",
      "1996 2\n",
      "1997 2\n",
      "1pp 1\n",
      "1st 1\n",
      "2 76\n",
      "20 9\n",
      "200 3\n",
      "2001 3\n",
      "2002 2\n",
      "2004 1\n",
      "2005 2\n",
      "2006 17\n",
      "2007 10\n",
      "2008 8\n",
      "2009 1\n",
      "2011 1\n",
      "2012 2\n",
      "2013 4\n",
      "2015 8\n",
      "2016 5\n",
      "2017 8\n",
      "2018 6\n",
      "2019 8\n",
      "2020 5\n",
      "2021 2\n",
      "2025 2\n",
      "21 4\n",
      "210 1\n",
      "22 3\n",
      "23 11\n",
      "230v 1\n",
      "233 1\n",
      "24 9\n",
      "2456 8\n",
      "25 6\n",
      "26 6\n",
      "267 1\n",
      "27 4\n",
      "28 4\n",
      "29 3\n",
      "293 3\n",
      "2d 35\n",
      "2x2 1\n",
      "3 76\n",
      "30 5\n",
      "3000 2\n",
      "306 1\n",
      "31 1\n",
      "313 1\n",
      "32 6\n",
      "328 1\n",
      "33 4\n",
      "34 3\n",
      "35 2\n",
      "36 10\n",
      "360 10\n",
      "360proto 2\n",
      "37 2\n",
      "376 1\n",
      "38 2\n",
      "39 3\n",
      "3c 1\n",
      "3d 220\n",
      "3dtv 1\n",
      "3dui 1\n",
      "3pp 2\n",
      "3rd 1\n",
      "3visual 3\n",
      "4 39\n",
      "40 6\n",
      "41 1\n",
      "42 8\n",
      "43 1\n",
      "44 1\n",
      "440 1\n",
      "4422 1\n",
      "45 4\n",
      "453 2\n",
      "46 3\n",
      "47 1\n",
      "48 2\n",
      "49 2\n",
      "4d 4\n",
      "5 29\n",
      "50 11\n",
      "51 2\n",
      "53 2\n",
      "5300 1\n",
      "54 8\n",
      "55 2\n",
      "56 5\n",
      "57 3\n",
      "594 2\n",
      "5a 1\n",
      "5g 1\n",
      "5th 1\n",
      "6 15\n",
      "60 4\n",
      "6071 2\n",
      "61 2\n",
      "612 1\n",
      "619 1\n",
      "627 1\n",
      "64 3\n",
      "6470 8\n",
      "654 1\n",
      "66 1\n",
      "67 1\n",
      "68 2\n",
      "69 3\n",
      "694 3\n",
      "6dof 2\n",
      "6th 2\n",
      "7 8\n",
      "70 4\n",
      "71 1\n",
      "72 1\n",
      "75 4\n",
      "76 2\n",
      "77 2\n",
      "78 4\n",
      "79 4\n",
      "7th 1\n",
      "8 12\n",
      "80 3\n",
      "814 2\n",
      "82 3\n",
      "835 1\n",
      "85 1\n",
      "87 1\n",
      "88 2\n",
      "89 2\n",
      "8th 1\n",
      "9 3\n",
      "90 3\n",
      "91 1\n",
      "93 1\n",
      "94 1\n",
      "95 1\n",
      "96 1\n",
      "97 3\n",
      "978 1\n",
      "98 4\n",
      "99 1\n",
      "9th 1\n",
      "a04 1\n",
      "a1969 1\n",
      "aar 1\n",
      "aat 1\n",
      "abandon 2\n",
      "abartl 2\n",
      "abbilden 1\n",
      "aber 4\n",
      "abernstett 3\n",
      "abgedeckt 1\n",
      "abgelehnt 1\n",
      "abh 1\n",
      "abil 23\n",
      "abkehr 1\n",
      "abl 13\n",
      "aboukh 1\n",
      "abrupt 2\n",
      "abschli 1\n",
      "absenc 8\n",
      "absicherung 1\n",
      "absolut 3\n",
      "abstimmung 1\n",
      "abstract 32\n",
      "abzuleiten 1\n",
      "ac 1\n",
      "academ 23\n",
      "academia 1\n",
      "academiclibrari 1\n",
      "acceler 14\n",
      "acceleromet 3\n",
      "accept 36\n",
      "access 66\n",
      "accid 2\n",
      "accommod 2\n",
      "accompani 10\n",
      "accomplish 6\n",
      "accord 25\n",
      "account 7\n",
      "accumul 1\n",
      "accur 21\n",
      "accuraci 32\n",
      "achakraborti 1\n",
      "achiev 32\n",
      "acht 1\n",
      "acknowledg 1\n",
      "acm 12\n",
      "acoust 3\n",
      "acquir 16\n",
      "acquisit 10\n",
      "acrophob 6\n",
      "acrophobia 6\n",
      "across 15\n",
      "act 11\n",
      "action 21\n",
      "activ 69\n",
      "activist 1\n",
      "activivti 1\n",
      "actor 7\n",
      "actual 20\n",
      "actuat 5\n",
      "acuiti 2\n",
      "acupoint 1\n",
      "acut 6\n",
      "ad 17\n",
      "ada 1\n",
      "adapt 93\n",
      "adaptiv 2\n",
      "add 3\n",
      "addit 63\n",
      "addition 27\n",
      "address 55\n",
      "ademoor 1\n",
      "adequ 3\n",
      "adjunct 1\n",
      "adjust 6\n",
      "administ 2\n",
      "administr 2\n",
      "adolesc 2\n",
      "adopt 10\n",
      "adult 12\n",
      "advanc 55\n",
      "advantag 35\n",
      "advent 2\n",
      "adventur 5\n",
      "advers 1\n",
      "adversari 1\n",
      "advertis 4\n",
      "advic 4\n",
      "advisori 1\n",
      "advocaci 1\n",
      "ae 1\n",
      "aec 1\n",
      "aemr 1\n",
      "aerial 6\n",
      "aerospac 3\n",
      "aesthet 2\n",
      "aeu 1\n",
      "affect 55\n",
      "affili 1\n",
      "affin 8\n",
      "afford 21\n",
      "afragop 1\n",
      "african 1\n",
      "ag 17\n",
      "agenc 4\n",
      "agenda 3\n",
      "agent 67\n",
      "aggreg 2\n",
      "agieren 1\n",
      "agil 2\n",
      "agnost 1\n",
      "ago 1\n",
      "agre 3\n",
      "agreement 2\n",
      "agricultur 1\n",
      "agross 1\n",
      "ahead 6\n",
      "ahm 1\n",
      "aho 1\n",
      "ai 30\n",
      "aid 6\n",
      "aim 54\n",
      "air 7\n",
      "aircraft 1\n",
      "airflow 1\n",
      "airplan 2\n",
      "airr 4\n",
      "airtabl 1\n",
      "aist 1\n",
      "akash 2\n",
      "akteur 1\n",
      "aktiv 1\n",
      "aktiven 1\n",
      "aktivit 1\n",
      "aktuel 4\n",
      "aktuellen 7\n",
      "akustisch 1\n",
      "akut 1\n",
      "akzeptabel 1\n",
      "akzeptanz 15\n",
      "al 56\n",
      "alex 3\n",
      "alexa 1\n",
      "alexarj 1\n",
      "algebra 3\n",
      "algorithm 56\n",
      "alias 6\n",
      "alic 1\n",
      "align 15\n",
      "alis 1\n",
      "alist 1\n",
      "alit 13\n",
      "aliv 1\n",
      "all 4\n",
      "allem 4\n",
      "allen 1\n",
      "allerd 1\n",
      "allevi 2\n",
      "allgemein 6\n",
      "allocentr 2\n",
      "allow 114\n",
      "allt 1\n",
      "alltag 4\n",
      "alltagshelf 1\n",
      "alon 8\n",
      "aloud 1\n",
      "alpha 2\n",
      "alter 28\n",
      "altern 24\n",
      "alterna 2\n",
      "alternativ 1\n",
      "alternativen 1\n",
      "altmann 2\n",
      "altogeth 1\n",
      "alucchi 1\n",
      "alzheim 2\n",
      "am 3\n",
      "amateur 2\n",
      "amazon 1\n",
      "ambient 5\n",
      "ambigu 3\n",
      "amblyopia 1\n",
      "ambros 1\n",
      "ambul 2\n",
      "american 1\n",
      "amir 3\n",
      "amount 9\n",
      "ampl 1\n",
      "amsterdam 1\n",
      "anaesthet 5\n",
      "analges 1\n",
      "analgesia 3\n",
      "analog 8\n",
      "analogon 1\n",
      "analys 15\n",
      "analysephas 2\n",
      "analysi 101\n",
      "analyst 2\n",
      "analyt 16\n",
      "analyz 31\n",
      "anatom 8\n",
      "anatomi 13\n",
      "anaylsi 1\n",
      "anbietet 1\n",
      "anchor 2\n",
      "ancient 3\n",
      "andedfi629i 1\n",
      "ander 2\n",
      "anderem 1\n",
      "anderen 7\n",
      "andrea 1\n",
      "andrefi63 1\n",
      "android 1\n",
      "anfang 1\n",
      "anforderungen 8\n",
      "angehenden 6\n",
      "angel 1\n",
      "angela 1\n",
      "angelehnt 2\n",
      "angereichert 2\n",
      "angesehen 1\n",
      "angesprochen 1\n",
      "angewendet 1\n",
      "angl 17\n",
      "anhand 3\n",
      "ani 1\n",
      "anim 20\n",
      "ank 3\n",
      "ankl 4\n",
      "anlagen 2\n",
      "anlagenbau 2\n",
      "anlagenplanung 2\n",
      "anlehnung 1\n",
      "anleitung 1\n",
      "anlernen 3\n",
      "anlernmethoden 1\n",
      "anlernprozessen 1\n",
      "ann 3\n",
      "annahm 3\n",
      "annevett 1\n",
      "annife 2\n",
      "annot 27\n",
      "annual 2\n",
      "anonym 1\n",
      "anova 1\n",
      "anregungen 2\n",
      "anreicherung 2\n",
      "ansatz 6\n",
      "anschaulich 2\n",
      "anschli 3\n",
      "anschluss 6\n",
      "anspruchsvol 1\n",
      "anstatt 1\n",
      "anstel 1\n",
      "ansto 1\n",
      "anstrengung 1\n",
      "answer 5\n",
      "anthropomorph 2\n",
      "anticip 1\n",
      "antipathet 1\n",
      "antiqu 1\n",
      "antj 4\n",
      "antroposeeni 1\n",
      "antwort 1\n",
      "antwortenden 1\n",
      "antwortet 1\n",
      "anweisungen 3\n",
      "anwend 3\n",
      "anwendern 1\n",
      "anwendung 3\n",
      "anwendungen 24\n",
      "anwendungsbeispielen 1\n",
      "anwendungsfal 1\n",
      "anwendungsfeld 3\n",
      "anwendungsfeldern 4\n",
      "anwendungsgetrieben 2\n",
      "anxieti 28\n",
      "anxiou 2\n",
      "anymor 1\n",
      "anytim 1\n",
      "anzeigen 4\n",
      "anzeigesystem 1\n",
      "anzunehmen 1\n",
      "anzupassen 2\n",
      "anzuzeigen 1\n",
      "ap 11\n",
      "api 3\n",
      "apolog 2\n",
      "app 32\n",
      "appar 9\n",
      "apparatu 2\n",
      "appeal 2\n",
      "appear 36\n",
      "appl 1\n",
      "appli 53\n",
      "applianc 4\n",
      "applic 548\n",
      "applik 1\n",
      "applikationen 7\n",
      "appliqu 1\n",
      "apprais 3\n",
      "appreci 3\n",
      "apprentic 2\n",
      "apprenticeship 4\n",
      "approach 205\n",
      "appropri 2\n",
      "approxim 3\n",
      "april 5\n",
      "aquarium 2\n",
      "ar 440\n",
      "ar0bert 6\n",
      "ar4ar 1\n",
      "arbeit 13\n",
      "arbeiten 3\n",
      "arbeitenden 1\n",
      "arbeitsbelastung 2\n",
      "arbeitsfeld 2\n",
      "arbeitsgestaltung 1\n",
      "arbeitskontexten 3\n",
      "arbeitskraft 1\n",
      "arbeitsleistung 1\n",
      "arbeitsorganis 3\n",
      "arbeitspersonen 1\n",
      "arbeitspl 2\n",
      "arbeitsplatz 3\n",
      "arbeitsprozess 3\n",
      "arbeitsprozessen 4\n",
      "arbeitst 1\n",
      "arbeitsumfeld 1\n",
      "arbeitsumgebungen 1\n",
      "arbeitswerkzeug 1\n",
      "arbitrari 4\n",
      "arc 1\n",
      "arcar 1\n",
      "arch 1\n",
      "archaeolog 9\n",
      "archaeologist 2\n",
      "archeolog 2\n",
      "architec 2\n",
      "architectur 102\n",
      "archiv 1\n",
      "area 61\n",
      "arena 1\n",
      "aret 16\n",
      "argu 10\n",
      "argument 6\n",
      "ari 1\n",
      "aria 1\n",
      "aris 7\n",
      "ark 1\n",
      "arlean 1\n",
      "arlearn 1\n",
      "arm 14\n",
      "armicromachin 1\n",
      "arous 5\n",
      "arpp 1\n",
      "arrang 8\n",
      "arrow 1\n",
      "arsc 1\n",
      "arsembl 3\n",
      "arsset 1\n",
      "art 80\n",
      "artab 1\n",
      "artefact 1\n",
      "artemi 1\n",
      "arten 4\n",
      "articl 69\n",
      "articul 1\n",
      "articular 1\n",
      "artierchen 1\n",
      "artifact 13\n",
      "artifici 26\n",
      "artikel 6\n",
      "artist 4\n",
      "artistri 1\n",
      "artoolkit 3\n",
      "artoolkitplu 1\n",
      "artwork 2\n",
      "arv 1\n",
      "arvika 27\n",
      "arweath 1\n",
      "asap 1\n",
      "ascertain 1\n",
      "asch 1\n",
      "aschmidt 1\n",
      "asd 2\n",
      "asia 1\n",
      "ask 15\n",
      "asme0t2t 1\n",
      "aspect 57\n",
      "aspekt 1\n",
      "asr 2\n",
      "assembl 34\n",
      "assess 98\n",
      "assessor 3\n",
      "asset 4\n",
      "assign 2\n",
      "assist 45\n",
      "assistenz 1\n",
      "assistenzsystem 4\n",
      "associ 32\n",
      "assum 7\n",
      "assumpt 3\n",
      "assur 3\n",
      "astair 1\n",
      "astar0id 1\n",
      "astro 1\n",
      "astronaut 1\n",
      "astronomi 3\n",
      "astrophys 1\n",
      "asymetr 1\n",
      "asymmetr 1\n",
      "asymptot 2\n",
      "asynchron 1\n",
      "at89s52 1\n",
      "athen 2\n",
      "athlet 1\n",
      "ativ 1\n",
      "attach 6\n",
      "attain 1\n",
      "attempt 5\n",
      "attend 3\n",
      "attent 47\n",
      "attenu 1\n",
      "attitud 16\n",
      "attract 12\n",
      "attraktiven 1\n",
      "attribut 9\n",
      "attun 1\n",
      "au 27\n",
      "auch 22\n",
      "audienc 20\n",
      "audio 18\n",
      "audiobook 1\n",
      "audiovisu 1\n",
      "audit 3\n",
      "auditori 12\n",
      "auf 34\n",
      "aufbau 3\n",
      "aufbauend 1\n",
      "aufbereitet 1\n",
      "aufgab 7\n",
      "aufgaben 4\n",
      "aufgebaut 1\n",
      "aufgefasst 1\n",
      "aufgegriffen 1\n",
      "aufgezeigt 1\n",
      "aufgrund 2\n",
      "aufkommen 1\n",
      "aufschl 1\n",
      "aufsetzen 1\n",
      "aufwand 2\n",
      "aufweist 1\n",
      "augementedr 1\n",
      "augenblick 1\n",
      "augendominanz 1\n",
      "augme2 1\n",
      "augment 1721\n",
      "augmentedr 63\n",
      "augmentieren 2\n",
      "augmentierten 1\n",
      "augurscop 1\n",
      "aur 1\n",
      "aural 1\n",
      "ausbalanciert 1\n",
      "ausbildung 3\n",
      "ausblick 2\n",
      "auscult 1\n",
      "ausdruck 1\n",
      "ausgab 2\n",
      "ausgaben 1\n",
      "ausgangspunkt 1\n",
      "ausgestaltung 1\n",
      "auspr 2\n",
      "ausr 1\n",
      "ausreichen 1\n",
      "ausschli 1\n",
      "aussichtsreich 1\n",
      "austausch 1\n",
      "austauschsituationen 3\n",
      "australian 1\n",
      "austria 1\n",
      "auswahl 2\n",
      "auswirken 2\n",
      "auswirkt 1\n",
      "authent 11\n",
      "authentischen 2\n",
      "author 62\n",
      "autism 6\n",
      "autist 1\n",
      "auto 4\n",
      "autobiographi 3\n",
      "autom 10\n",
      "automat 26\n",
      "automati 1\n",
      "automatisiert 1\n",
      "automatisierung 1\n",
      "automobil 3\n",
      "automot 11\n",
      "autonom 18\n",
      "autonomi 2\n",
      "autor 1\n",
      "autorenorientiert 2\n",
      "autostereoscop 4\n",
      "aux 1\n",
      "avail 4\n",
      "avalon 2\n",
      "avancierten 1\n",
      "avatar 232\n",
      "averag 11\n",
      "aviari 1\n",
      "aviat 2\n",
      "avigl 4\n",
      "avion 2\n",
      "avocado 1\n",
      "avoid 25\n",
      "avr 1\n",
      "aw 1\n",
      "awai 1\n",
      "awak 3\n",
      "awar 88\n",
      "award 2\n",
      "ax 1\n",
      "azuma 1\n",
      "azura 1\n",
      "b1802 1\n",
      "b2b 1\n",
      "ba 1\n",
      "back 4\n",
      "backend 1\n",
      "background 16\n",
      "backpack 3\n",
      "backseat 1\n",
      "bacteri 1\n",
      "bad 10\n",
      "baillot 1\n",
      "balanc 10\n",
      "bald 1\n",
      "ballerina 1\n",
      "band 1\n",
      "bandura 1\n",
      "bandwidth 4\n",
      "bang 1\n",
      "bank 2\n",
      "baraba 1\n",
      "bare 2\n",
      "barrier 15\n",
      "base 451\n",
      "baselin 4\n",
      "basi 21\n",
      "basic 14\n",
      "basierend 1\n",
      "basiert 4\n",
      "basistechnologien 2\n",
      "baustein 3\n",
      "baywiss1 1\n",
      "bbb 2\n",
      "bci 1\n",
      "be 44\n",
      "be1 1\n",
      "beach 6\n",
      "beacon 2\n",
      "beam 6\n",
      "beamatron 1\n",
      "beamlit 1\n",
      "beanspruchung 5\n",
      "bear 8\n",
      "bearbeiteten 1\n",
      "beat 1\n",
      "bed 5\n",
      "bedarf 2\n",
      "bedarfsgerecht 3\n",
      "bedeutsam 6\n",
      "bedeutung 1\n",
      "bedingungen 4\n",
      "bedsid 1\n",
      "bee 5\n",
      "beeinflussen 8\n",
      "befassen 1\n",
      "befasst 3\n",
      "befassten 3\n",
      "befragung 2\n",
      "befund 6\n",
      "begin 10\n",
      "beginnend 1\n",
      "beginnt 2\n",
      "begleit 2\n",
      "begleitend 3\n",
      "begleitet 1\n",
      "begleitforschung 3\n",
      "begreifen 1\n",
      "begrenzt 1\n",
      "begrenzten 1\n",
      "begriff 1\n",
      "begriffserweiterung 1\n",
      "begun 2\n",
      "behandelt 1\n",
      "behandlung 1\n",
      "behavefit 14\n",
      "behavio 2\n",
      "behavior 131\n",
      "behaviour 23\n",
      "bei 26\n",
      "beiden 2\n",
      "beigetreten 1\n",
      "beim 4\n",
      "beispiel 4\n",
      "beispielen 1\n",
      "beispielhaft 3\n",
      "beispielsweis 1\n",
      "beitrag 21\n",
      "belastung 1\n",
      "belastungsfaktor 2\n",
      "beleg 1\n",
      "beleuchtet 1\n",
      "beliebt 1\n",
      "belief 4\n",
      "believ 15\n",
      "bell 1\n",
      "belonging 2\n",
      "ben 5\n",
      "benchmark 7\n",
      "bender 1\n",
      "benefici 5\n",
      "benefit 59\n",
      "benin 1\n",
      "benutz 11\n",
      "benutzbarkeit 1\n",
      "benutzerfreundlich 2\n",
      "benutzerfreundlichen 2\n",
      "benutzergerecht 1\n",
      "benutzerinterakt 1\n",
      "benutzern 8\n",
      "benutzerorientiert 2\n",
      "benutzerzentriert 10\n",
      "benutzerzentrierten 4\n",
      "ber 23\n",
      "ber08 1\n",
      "berarbeitet 1\n",
      "berblick 5\n",
      "berechnet 1\n",
      "bereich 7\n",
      "bereichen 3\n",
      "bereichern 2\n",
      "bereit 1\n",
      "bergestellt 1\n",
      "bergreifend 1\n",
      "bergreifenden 1\n",
      "berlagert 4\n",
      "berlagerung 1\n",
      "berlegten 1\n",
      "berlin 3\n",
      "bernahm 3\n",
      "berregional 1\n",
      "bersicht 1\n",
      "bertragen 1\n",
      "berucksichtigung 1\n",
      "berufsausbildung 1\n",
      "beruht 1\n",
      "berwachend 1\n",
      "berwachung 1\n",
      "berwiegend 1\n",
      "beschli 1\n",
      "beschlossen 1\n",
      "beschr 1\n",
      "beschreibt 8\n",
      "beschrieben 3\n",
      "besitzt 1\n",
      "besond 2\n",
      "besonder 1\n",
      "besser 2\n",
      "best 12\n",
      "bestandteil 6\n",
      "bestehen 1\n",
      "bestimmten 1\n",
      "betracht 4\n",
      "betrachten 1\n",
      "betrachteten 1\n",
      "betrag 1\n",
      "betreffenden 1\n",
      "betrieb 1\n",
      "betrieblich 1\n",
      "betrifft 1\n",
      "bett 1\n",
      "bevor 1\n",
      "bewegen 2\n",
      "bewegung 3\n",
      "bewertet 7\n",
      "bewertung 1\n",
      "bez 1\n",
      "bezeichnet 1\n",
      "beziehen 1\n",
      "beziehung 1\n",
      "bezug 1\n",
      "bharath 2\n",
      "bhavya 1\n",
      "bhushan 1\n",
      "bi 5\n",
      "bia 5\n",
      "bias 4\n",
      "bib 7\n",
      "bibgreen 26\n",
      "bibliodoc 3\n",
      "bibliothek 4\n",
      "bibliothekar 2\n",
      "bibliothekarinnen 1\n",
      "bibliothekarisch 2\n",
      "bibliothekarischen 2\n",
      "bibliotheken 12\n",
      "bibliotheksarbeit 1\n",
      "bibliotheksverband 1\n",
      "bibsonomi 3149\n",
      "bibtex 3149\n",
      "bicycl 1\n",
      "bid 2\n",
      "biennial 1\n",
      "bieten 1\n",
      "bietet 3\n",
      "bif 1\n",
      "big 20\n",
      "biggest 1\n",
      "bike 2\n",
      "bilat 1\n",
      "bilater 7\n",
      "bild 3\n",
      "bilden 1\n",
      "bilder 3\n",
      "bildet 1\n",
      "bildschirmgestaltung 2\n",
      "bildungsenhanc 1\n",
      "bildungspraxi 6\n",
      "biliari 1\n",
      "billiard 1\n",
      "billion 1\n",
      "bim 1\n",
      "bimanu 2\n",
      "bimber 2\n",
      "binaur 2\n",
      "binden 1\n",
      "binocular 15\n",
      "binokular 1\n",
      "bio 3\n",
      "biolog 6\n",
      "biologi 6\n",
      "biomanipul 1\n",
      "biomed 2\n",
      "biometr 2\n",
      "biopsi 1\n",
      "biotechnolog 1\n",
      "birgit 3\n",
      "bisher 1\n",
      "bislang 1\n",
      "bisschen 1\n",
      "bladeship 2\n",
      "blank 1\n",
      "blend 19\n",
      "bless 1\n",
      "blick 3\n",
      "blickbewegung 1\n",
      "blickfeld 1\n",
      "blind 32\n",
      "blink 2\n",
      "block 1\n",
      "blow 2\n",
      "blue 1\n",
      "bluetooth 2\n",
      "blur 4\n",
      "bmbf 3\n",
      "bmi 26\n",
      "bmj 1\n",
      "board 3\n",
      "bodi 321\n",
      "bodyimag 2\n",
      "boer 1\n",
      "bond 2\n",
      "bone 76\n",
      "bonu 1\n",
      "book 34\n",
      "bookshelf 1\n",
      "boon 1\n",
      "boost 2\n",
      "boot 1\n",
      "bootstrap 1\n",
      "border 1\n",
      "borrow 1\n",
      "bot 3\n",
      "botan 1\n",
      "botmp 1\n",
      "botsch 3\n",
      "bottom 4\n",
      "botulinum 1\n",
      "bought 8\n",
      "bound 3\n",
      "boundari 9\n",
      "bowl 1\n",
      "box 8\n",
      "brachytherapi 1\n",
      "brain 15\n",
      "branch 1\n",
      "branchen 1\n",
      "brand 2\n",
      "brazovayey 3\n",
      "brdf 1\n",
      "break 12\n",
      "breakthrough 1\n",
      "breast 2\n",
      "breath 6\n",
      "breit 1\n",
      "brettspiel 6\n",
      "brew 1\n",
      "bridg 13\n",
      "brief 4\n",
      "briefli 2\n",
      "bright 1\n",
      "brilliant 1\n",
      "bring 29\n",
      "briviesca 1\n",
      "broad 11\n",
      "broadcast 17\n",
      "broadli 4\n",
      "broken 1\n",
      "brought 4\n",
      "brown 2\n",
      "brows 3\n",
      "browser 10\n",
      "bruinink 1\n",
      "brujic 2\n",
      "brush 2\n",
      "brusilovski 2\n",
      "bscthesi 1\n",
      "bspw 2\n",
      "bt 2\n",
      "bubbl 2\n",
      "buch 4\n",
      "buckeyevr 1\n",
      "budget 4\n",
      "build 57\n",
      "built 10\n",
      "bung 3\n",
      "burst 1\n",
      "busi 14\n",
      "butterfli 1\n",
      "bv 1\n",
      "bypass 1\n",
      "bystand 1\n",
      "bzgl 1\n",
      "bzw 2\n",
      "c02 1\n",
      "c4 1\n",
      "ca 1\n",
      "cabinet 3\n",
      "cabird 1\n",
      "cabl 1\n",
      "cad 3\n",
      "cadav 4\n",
      "cadaver 9\n",
      "caf 2\n",
      "caij 1\n",
      "calcul 12\n",
      "calculu 1\n",
      "calibr 30\n",
      "california 1\n",
      "call 41\n",
      "camera 79\n",
      "cameria 1\n",
      "can 277\n",
      "canada 1\n",
      "canal 2\n",
      "cancel 2\n",
      "cane 4\n",
      "cannon 1\n",
      "canon 2\n",
      "cap 2\n",
      "capabl 31\n",
      "capac 4\n",
      "capit 2\n",
      "capston 1\n",
      "captur 37\n",
      "car 11\n",
      "caracol48 1\n",
      "carbohydr 10\n",
      "card 9\n",
      "cardboard 2\n",
      "cardiac 2\n",
      "cardiolen 1\n",
      "cardiopulmonari 1\n",
      "care 23\n",
      "career 4\n",
      "carefulli 8\n",
      "caren 2\n",
      "carma 1\n",
      "carri 8\n",
      "carv 12\n",
      "case 77\n",
      "casestudi 1\n",
      "castl 3\n",
      "casual 3\n",
      "cat 1\n",
      "catapult 1\n",
      "cataract 1\n",
      "catastroph 1\n",
      "categor 8\n",
      "categori 6\n",
      "cater 2\n",
      "cation 2\n",
      "caus 27\n",
      "causal 5\n",
      "caustic 1\n",
      "cave 18\n",
      "caveut 1\n",
      "cavir 1\n",
      "cbct 2\n",
      "cde 8\n",
      "ceit 1\n",
      "cell 12\n",
      "cellphon 3\n",
      "cellular 1\n",
      "cemeteri 1\n",
      "center 11\n",
      "centr 6\n",
      "central 27\n",
      "centric 6\n",
      "centuri 2\n",
      "cerebr 23\n",
      "ceremoni 1\n",
      "certainli 1\n",
      "ceta 1\n",
      "chain 2\n",
      "chair 4\n",
      "chal 1\n",
      "challeng 105\n",
      "chameleon 1\n",
      "chanc 2\n",
      "chang 107\n",
      "changeblind 1\n",
      "channel 10\n",
      "chapter 4\n",
      "charact 21\n",
      "character 1\n",
      "characterist 24\n",
      "charakteristisch 1\n",
      "charg 3\n",
      "charm 2\n",
      "chase 1\n",
      "check 4\n",
      "checker 1\n",
      "checklist 5\n",
      "chemic 1\n",
      "chemistri 3\n",
      "chen 1\n",
      "chern 1\n",
      "chief 1\n",
      "child 15\n",
      "childhood 2\n",
      "children 67\n",
      "china 7\n",
      "chines 2\n",
      "chirurgi 1\n",
      "chiseldevic 1\n",
      "chlich 1\n",
      "chlichen 1\n",
      "chloe 4\n",
      "chnologi 1\n",
      "chocol 1\n",
      "choic 16\n",
      "cholecystectomi 1\n",
      "choos 2\n",
      "choreographi 1\n",
      "chosen 5\n",
      "chroma 7\n",
      "chromin 1\n",
      "chronic 1\n",
      "chronicl 4\n",
      "chronisch 1\n",
      "chst 9\n",
      "cinema 1\n",
      "cinemat 2\n",
      "cinematographi 2\n",
      "circl 2\n",
      "circu 1\n",
      "circuit 2\n",
      "circul 1\n",
      "circumst 4\n",
      "citadel 1\n",
      "citat 2\n",
      "citi 26\n",
      "citingaren 1\n",
      "citizen 8\n",
      "citli 4\n",
      "civil 1\n",
      "ck 2\n",
      "cklaufquot 1\n",
      "cksichtigen 3\n",
      "cksichtigt 1\n",
      "cksichtigung 1\n",
      "claim 3\n",
      "clarifi 7\n",
      "clariti 2\n",
      "class 14\n",
      "classic 11\n",
      "classif 15\n",
      "classifi 9\n",
      "classroom 51\n",
      "claustrophobia 4\n",
      "clear 4\n",
      "clearli 5\n",
      "clearspac 2\n",
      "clemson 1\n",
      "clever 1\n",
      "click 1\n",
      "client 11\n",
      "cliff 1\n",
      "clifford 1\n",
      "climb 4\n",
      "clinic 22\n",
      "clinicaltri 1\n",
      "clinician 1\n",
      "clip 1\n",
      "clock 3\n",
      "close 24\n",
      "closur 1\n",
      "cloth 2\n",
      "cloud 45\n",
      "cloudlet 1\n",
      "cloudpad 1\n",
      "clout 2\n",
      "clown 1\n",
      "clude 2\n",
      "clue 4\n",
      "cluster 4\n",
      "cmcneil 2\n",
      "coach 3\n",
      "coatepequ 1\n",
      "coauthor 1\n",
      "cobot 1\n",
      "cochran 4\n",
      "coco 1\n",
      "code 18\n",
      "codecub 1\n",
      "codierung 1\n",
      "coeffici 2\n",
      "cofound 1\n",
      "cognit 63\n",
      "cognitiveload 1\n",
      "coher 12\n",
      "cohes 1\n",
      "cohort 2\n",
      "col 1\n",
      "coliseum 1\n",
      "collabor 169\n",
      "colleagu 2\n",
      "collect 29\n",
      "colleg 2\n",
      "collis 7\n",
      "colloc 3\n",
      "color 30\n",
      "colorblind 3\n",
      "colour 1\n",
      "com 13\n",
      "combin 75\n",
      "come 13\n",
      "comfort 9\n",
      "command 11\n",
      "commenc 3\n",
      "comment 3\n",
      "commerc 8\n",
      "commerci 17\n",
      "commiss 1\n",
      "commit 1\n",
      "commod 4\n",
      "common 35\n",
      "commonli 3\n",
      "commonplac 1\n",
      "commun 115\n",
      "commut 2\n",
      "comoth 1\n",
      "compact 1\n",
      "compani 7\n",
      "companion 8\n",
      "compap 1\n",
      "compar 137\n",
      "comparison 21\n",
      "compat 2\n",
      "compel 6\n",
      "compens 11\n",
      "compet 24\n",
      "competit 3\n",
      "compil 2\n",
      "comple 1\n",
      "comple6627 1\n",
      "complement 14\n",
      "complet 28\n",
      "complex 65\n",
      "complianc 4\n",
      "compliant 3\n",
      "complic 4\n",
      "complimentari 1\n",
      "compon 52\n",
      "compos 2\n",
      "composit 12\n",
      "comprehend 2\n",
      "comprehens 14\n",
      "compress 2\n",
      "compris 6\n",
      "compromis 2\n",
      "compulsori 5\n",
      "comput 224\n",
      "computer 6\n",
      "computergeneriert 1\n",
      "computergenerierten 3\n",
      "computergrafik 1\n",
      "computergraph 1\n",
      "computergraphik 1\n",
      "computerunterst 1\n",
      "computervis 1\n",
      "con 1\n",
      "concav 2\n",
      "conceiv 4\n",
      "concentr 8\n",
      "concept 110\n",
      "conceptu 25\n",
      "concern 15\n",
      "concert 1\n",
      "conchuir 1\n",
      "concis 1\n",
      "conclud 9\n",
      "conclus 17\n",
      "concord 1\n",
      "concret 1\n",
      "concurr 8\n",
      "condens 3\n",
      "condit 106\n",
      "conduct 35\n",
      "conduit 2\n",
      "cone 2\n",
      "confenffi62 1\n",
      "confer 9\n",
      "conferenc 2\n",
      "confid 1\n",
      "configur 20\n",
      "confin 4\n",
      "confirm 20\n",
      "conflict 5\n",
      "conform 4\n",
      "confront 1\n",
      "congenit 1\n",
      "congruenc 23\n",
      "congruent 9\n",
      "conjur 2\n",
      "connect 26\n",
      "consecut 1\n",
      "consensu 1\n",
      "consequ 4\n",
      "conserv 6\n",
      "consid 47\n",
      "consider 27\n",
      "consist 43\n",
      "consol 8\n",
      "consolid 1\n",
      "consortium 3\n",
      "constant 1\n",
      "constantli 3\n",
      "constitut 9\n",
      "constrain 3\n",
      "constraint 12\n",
      "construct 51\n",
      "construction 1\n",
      "constructiv 3\n",
      "consult 2\n",
      "consum 30\n",
      "consumpt 2\n",
      "contact 4\n",
      "contain 19\n",
      "content 85\n",
      "contest 1\n",
      "context 87\n",
      "contextu 11\n",
      "continu 17\n",
      "continuum 15\n",
      "contour 3\n",
      "contradict 1\n",
      "contralater 1\n",
      "contrari 1\n",
      "contrast 16\n",
      "contribut 29\n",
      "control 146\n",
      "conv 2\n",
      "convei 3\n",
      "conveni 9\n",
      "convent 14\n",
      "converg 6\n",
      "convers 31\n",
      "convert 1\n",
      "convex 1\n",
      "conveyor 1\n",
      "convinc 6\n",
      "cook 6\n",
      "cooper 15\n",
      "coordin 24\n",
      "cope 2\n",
      "copi 9\n",
      "coral 2\n",
      "cord 2\n",
      "core 11\n",
      "cornucopia 1\n",
      "corp 1\n",
      "corpor 1\n",
      "corpu 1\n",
      "correct 25\n",
      "correctli 6\n",
      "correl 12\n",
      "correspond 10\n",
      "corrobor 2\n",
      "corros 1\n",
      "corsi 1\n",
      "cortana 1\n",
      "cortex 2\n",
      "cortexvr 1\n",
      "cortic 10\n",
      "corvu 1\n",
      "cosmolog 1\n",
      "cost 49\n",
      "costli 2\n",
      "coteach 8\n",
      "count 22\n",
      "counteract 2\n",
      "counterbalanc 1\n",
      "counterpart 5\n",
      "countri 3\n",
      "coupl 7\n",
      "courag 1\n",
      "cours 17\n",
      "cousin 1\n",
      "cover 8\n",
      "coverag 2\n",
      "covid 1\n",
      "covid19 1\n",
      "cp 8\n",
      "cpr 2\n",
      "cpurp 1\n",
      "craft 5\n",
      "crafter 1\n",
      "craftsmen 1\n",
      "craniotomi 3\n",
      "crave 1\n",
      "crc1410 2\n",
      "creat 97\n",
      "creatar 1\n",
      "creation 23\n",
      "creativ 22\n",
      "creator 2\n",
      "creatur 2\n",
      "crisi 2\n",
      "criteria 9\n",
      "critic 40\n",
      "crochet 1\n",
      "cross 11\n",
      "crosser 1\n",
      "crossmod 3\n",
      "crowd 5\n",
      "crucial 15\n",
      "cryoablat 1\n",
      "crystal 4\n",
      "cs7 1\n",
      "cscw 2\n",
      "cse 2\n",
      "ct 1\n",
      "ctb 6\n",
      "ctreff 1\n",
      "cube 7\n",
      "cubic 1\n",
      "cubica 1\n",
      "cubicl 1\n",
      "cue 28\n",
      "cull 1\n",
      "cultiv 1\n",
      "cultur 45\n",
      "cumbersom 2\n",
      "cumulu 1\n",
      "cup 1\n",
      "curat 2\n",
      "current 89\n",
      "curricular 6\n",
      "curriculum 8\n",
      "curso 2\n",
      "cursor 3\n",
      "curv 6\n",
      "custom 16\n",
      "cut 8\n",
      "cyber 8\n",
      "cybercod 2\n",
      "cybercultur 1\n",
      "cybergrasp 1\n",
      "cybersecur 1\n",
      "cybersick 10\n",
      "cyberspac 3\n",
      "cycl 3\n",
      "cyphon 2\n",
      "da 72\n",
      "dabei 11\n",
      "dadurch 2\n",
      "daf 1\n",
      "daher 7\n",
      "dahin 2\n",
      "dai 8\n",
      "daili 13\n",
      "damit 9\n",
      "dan 1\n",
      "dana 1\n",
      "danach 2\n",
      "danc 6\n",
      "danger 3\n",
      "dann 2\n",
      "dar 5\n",
      "daran 3\n",
      "darf 1\n",
      "dargelegt 1\n",
      "dargestellt 7\n",
      "dark 9\n",
      "darnok 1\n",
      "darstellen 2\n",
      "darstellt 2\n",
      "darstellung 6\n",
      "darstellungsformen 2\n",
      "darstellungsm 1\n",
      "dart 1\n",
      "darum 1\n",
      "dass 28\n",
      "data 194\n",
      "databas 24\n",
      "datascap 1\n",
      "dataset 2\n",
      "date 7\n",
      "daten 4\n",
      "datenbril 3\n",
      "datenbrillen 6\n",
      "datenhandschuh 1\n",
      "datenmodellierung 1\n",
      "datensampl 1\n",
      "datensicherheit 1\n",
      "datenverarbeitung 3\n",
      "davemurphi 2\n",
      "davidm 6\n",
      "dazu 2\n",
      "dblp 4811\n",
      "dbv 1\n",
      "dc 2\n",
      "de 72\n",
      "deadlock 1\n",
      "deal 7\n",
      "debat 5\n",
      "debatt 1\n",
      "debrief 3\n",
      "debug 2\n",
      "decad 7\n",
      "decemb 3\n",
      "decentr 4\n",
      "decid 1\n",
      "decis 60\n",
      "decisionmak 1\n",
      "deck 3\n",
      "declin 2\n",
      "decomposit 1\n",
      "decor 3\n",
      "decoupl 11\n",
      "decreas 13\n",
      "dedic 6\n",
      "deep 6\n",
      "deeper 3\n",
      "deepfak 1\n",
      "deeplight 3\n",
      "default 1\n",
      "defeat 1\n",
      "defect 1\n",
      "defenc 1\n",
      "defens 2\n",
      "defici 1\n",
      "deficit 2\n",
      "defin 30\n",
      "definit 20\n",
      "deform 6\n",
      "degrad 2\n",
      "degradieren 1\n",
      "degre 40\n",
      "deictic 15\n",
      "deixi 1\n",
      "delai 1\n",
      "delaunai 2\n",
      "delib 2\n",
      "deliber 4\n",
      "delight 2\n",
      "delin 3\n",
      "deliv 4\n",
      "deliveri 6\n",
      "delta 3\n",
      "delud 1\n",
      "delv 2\n",
      "dem 25\n",
      "demand 14\n",
      "demo 6\n",
      "demograph 2\n",
      "demonstr 86\n",
      "demonstratoren 1\n",
      "den 61\n",
      "denen 1\n",
      "denmark 1\n",
      "dennoch 4\n",
      "denot 1\n",
      "densiti 2\n",
      "dentur 1\n",
      "deolekar 2\n",
      "depart 2\n",
      "depend 16\n",
      "depict 4\n",
      "deplet 1\n",
      "deploi 5\n",
      "deploy 4\n",
      "depress 4\n",
      "depth 36\n",
      "der 269\n",
      "derang 2\n",
      "derart 2\n",
      "deren 3\n",
      "derer 1\n",
      "deriv 30\n",
      "desargu 1\n",
      "describ 3\n",
      "descript 14\n",
      "deshmukh 1\n",
      "desiderata 1\n",
      "desideratum 1\n",
      "design 431\n",
      "desir 2\n",
      "desk 1\n",
      "desktop 46\n",
      "desktopbasiert 1\n",
      "dessen 1\n",
      "dessin 1\n",
      "destin 2\n",
      "detail 29\n",
      "detaillierten 1\n",
      "detaillierter 1\n",
      "dete30fiz 1\n",
      "detect 44\n",
      "detector 4\n",
      "detent 2\n",
      "determin 45\n",
      "determiniert 1\n",
      "detract 1\n",
      "deuten 1\n",
      "deutlich 1\n",
      "deutsch 2\n",
      "deutschen 2\n",
      "deutschland 2\n",
      "develop 312\n",
      "development 1\n",
      "devi 2\n",
      "deviant 2\n",
      "deviat 5\n",
      "devic 121\n",
      "dewei 1\n",
      "dexter 4\n",
      "dfki 4\n",
      "dhjmf 2\n",
      "di 2\n",
      "diabet 5\n",
      "diagnos 8\n",
      "diagnost 2\n",
      "diagram 1\n",
      "dial 2\n",
      "dialog 2\n",
      "dialogu 10\n",
      "diari 1\n",
      "didact 17\n",
      "die 277\n",
      "dieget 2\n",
      "dienen 1\n",
      "dienstleistern 1\n",
      "dienstleistungen 1\n",
      "dient 6\n",
      "dies 25\n",
      "diesem 16\n",
      "diesen 1\n",
      "dieser 31\n",
      "differ 84\n",
      "differenti 14\n",
      "difficult 14\n",
      "difficulti 14\n",
      "digest 2\n",
      "digit 115\n",
      "digital 18\n",
      "digitalen 5\n",
      "digitalis 1\n",
      "digitalisierung 5\n",
      "dii 7\n",
      "dilemma 1\n",
      "dime 1\n",
      "dimens 19\n",
      "dimension 39\n",
      "dimensiona 1\n",
      "dimensionen 1\n",
      "diminish 8\n",
      "din 1\n",
      "dine 1\n",
      "dipp 10\n",
      "direct 53\n",
      "directli 10\n",
      "direkt 1\n",
      "disabl 23\n",
      "disadvantag 18\n",
      "disambigu 5\n",
      "disappear 2\n",
      "disassembl 1\n",
      "disast 2\n",
      "disciplin 3\n",
      "disciplinari 3\n",
      "discomfort 6\n",
      "discours 2\n",
      "discov 9\n",
      "discoveri 5\n",
      "discrep 4\n",
      "discrimin 8\n",
      "discuss 84\n",
      "diseas 16\n",
      "disk 1\n",
      "diskutiert 9\n",
      "disord 30\n",
      "disorient 3\n",
      "dispar 2\n",
      "displac 6\n",
      "displai 253\n",
      "displaysalongsid 1\n",
      "dispositif 1\n",
      "disregard 2\n",
      "disrupt 8\n",
      "dissect 23\n",
      "dissemin 3\n",
      "dissert 1\n",
      "dissimilar 1\n",
      "dissoci 2\n",
      "distanc 34\n",
      "distant 3\n",
      "distanz 1\n",
      "distinct 10\n",
      "distinguish 8\n",
      "distort 15\n",
      "distract 2\n",
      "distractor 3\n",
      "distribu 1\n",
      "distribut 72\n",
      "district 2\n",
      "disturb 12\n",
      "diva 2\n",
      "diverg 1\n",
      "divers 17\n",
      "diversifi 1\n",
      "divid 2\n",
      "dj 1\n",
      "dmal 16\n",
      "dnb 25\n",
      "docent 1\n",
      "doch 1\n",
      "document 8\n",
      "documentari 2\n",
      "doelling 9\n",
      "dof 6\n",
      "dog 2\n",
      "doi 2\n",
      "domain 24\n",
      "dome 2\n",
      "domest 1\n",
      "domi 4\n",
      "domin 10\n",
      "dominik 1\n",
      "don 3\n",
      "dongspac 1\n",
      "donkei 1\n",
      "door 1\n",
      "dort 1\n",
      "dot 1\n",
      "doubl 1\n",
      "doubt 1\n",
      "down 10\n",
      "downsid 1\n",
      "dozen 1\n",
      "dr 5\n",
      "drag 5\n",
      "draw 13\n",
      "drawback 2\n",
      "drawn 5\n",
      "dream 1\n",
      "drei 4\n",
      "dress 1\n",
      "dribbl 1\n",
      "drift 1\n",
      "drill 12\n",
      "drive 7\n",
      "driven 16\n",
      "drone 5\n",
      "drop 10\n",
      "droth 10\n",
      "drum 1\n",
      "drummond 1\n",
      "dscvr 1\n",
      "dt 2\n",
      "dtlz2 1\n",
      "dual 7\n",
      "due 23\n",
      "duerrschnabel 1\n",
      "duira 1\n",
      "duplic 3\n",
      "dura 2\n",
      "dural 1\n",
      "durat 7\n",
      "durch 21\n",
      "durchgef 4\n",
      "dvb 2\n",
      "dvv 1\n",
      "dwarf 4\n",
      "dwell 1\n",
      "dwiebusch 1\n",
      "dyadic 2\n",
      "dynam 56\n",
      "dynamik 1\n",
      "dynamiken 1\n",
      "dynamisch 3\n",
      "ea 1\n",
      "ear 3\n",
      "earli 9\n",
      "earlier 3\n",
      "earth 1\n",
      "earthfar 1\n",
      "earthshak 1\n",
      "eas 6\n",
      "easi 25\n",
      "easier 2\n",
      "easili 7\n",
      "east 1\n",
      "eat 5\n",
      "eatar 2\n",
      "eben 1\n",
      "ebenso 1\n",
      "ebook 1\n",
      "ec 2\n",
      "echten 2\n",
      "echtweltszenario 2\n",
      "eckstein 6\n",
      "eclips 1\n",
      "ecolog 4\n",
      "econom 9\n",
      "economi 5\n",
      "ecosystem 4\n",
      "ect 1\n",
      "ectel09 2\n",
      "ectel2009 2\n",
      "ectop 1\n",
      "ed 2\n",
      "edg 5\n",
      "edit 9\n",
      "editor 6\n",
      "editori 7\n",
      "edu 3\n",
      "educ 220\n",
      "edutain 3\n",
      "eealiti 1\n",
      "eeg 17\n",
      "eeri 7\n",
      "ef 6\n",
      "effcienct 1\n",
      "effect 242\n",
      "effekt 2\n",
      "effektivit 1\n",
      "effer 1\n",
      "efficaci 11\n",
      "effici 49\n",
      "effizient 2\n",
      "effizienz 2\n",
      "effort 11\n",
      "ego 10\n",
      "egocentr 1\n",
      "ehealth 2\n",
      "eichenberg 1\n",
      "eigen 4\n",
      "eigenen 4\n",
      "eigentlich 1\n",
      "eighteen 3\n",
      "eighth 2\n",
      "eighti 2\n",
      "ein 182\n",
      "einbeziehung 2\n",
      "einbezug 1\n",
      "einbindung 7\n",
      "einblendet 1\n",
      "einblendung 3\n",
      "einblick 3\n",
      "eindruck 1\n",
      "einem 21\n",
      "einen 24\n",
      "einer 34\n",
      "einf 1\n",
      "einfach 1\n",
      "einfachen 2\n",
      "einfli 1\n",
      "einfluss 2\n",
      "eingab 9\n",
      "eingabemodalit 1\n",
      "eingang 6\n",
      "eingebettet 1\n",
      "eingebunden 1\n",
      "eingef 3\n",
      "eingegangen 1\n",
      "eingehend 1\n",
      "einheiten 3\n",
      "einhergehen 1\n",
      "einmal 1\n",
      "einsatz 12\n",
      "einsatzf 1\n",
      "einsatzm 3\n",
      "einsetzbarkeit 1\n",
      "einzeln 1\n",
      "einzelnen 1\n",
      "einzublenden 2\n",
      "einzurichten 1\n",
      "eiv 1\n",
      "elabor 4\n",
      "elbow 1\n",
      "eld 2\n",
      "elderli 2\n",
      "elearn 1\n",
      "electr 10\n",
      "electroderm 2\n",
      "electroencephalogram 1\n",
      "electrogoniomet 1\n",
      "electromagnet 2\n",
      "electron 17\n",
      "electrostat 1\n",
      "elektroindustri 1\n",
      "elektrotechnik 1\n",
      "element 27\n",
      "elementari 4\n",
      "elev 4\n",
      "eleven 6\n",
      "elicit 7\n",
      "elig 2\n",
      "elimin 1\n",
      "ellyssa 1\n",
      "elmqvist 1\n",
      "elsevi 1\n",
      "elucid 2\n",
      "em 11\n",
      "eman 1\n",
      "emat 1\n",
      "emb 4\n",
      "embas 1\n",
      "embed 30\n",
      "embodi 229\n",
      "embrac 5\n",
      "emerg 36\n",
      "emerson 1\n",
      "emlab 14\n",
      "emmi 2\n",
      "emordi 1\n",
      "emot 24\n",
      "emotion 1\n",
      "emotional 1\n",
      "empathi 6\n",
      "empfundenen 1\n",
      "emphas 5\n",
      "emphasi 4\n",
      "empir 38\n",
      "empirisch 10\n",
      "emploi 9\n",
      "empow 2\n",
      "empower 2\n",
      "en 17\n",
      "enabl 79\n",
      "enc 2\n",
      "encephalomalacia 1\n",
      "enchant 3\n",
      "encod 6\n",
      "encompass 3\n",
      "encount 39\n",
      "encourag 19\n",
      "encultur 2\n",
      "end 31\n",
      "endeavour 1\n",
      "ender 1\n",
      "endergebni 1\n",
      "endger 7\n",
      "endoscop 6\n",
      "endow 1\n",
      "endrass 2\n",
      "energi 8\n",
      "enforc 1\n",
      "eng 2\n",
      "engag 32\n",
      "engin 84\n",
      "englisch 3\n",
      "englischunterricht 6\n",
      "english 12\n",
      "enhanc 100\n",
      "enitsirhc 4\n",
      "enjoi 3\n",
      "enjoy 8\n",
      "enlarg 2\n",
      "enlighten 2\n",
      "enorm 2\n",
      "enquiri 1\n",
      "enrich 11\n",
      "enrol 3\n",
      "ensur 10\n",
      "enter 2\n",
      "enterpris 5\n",
      "entertain 27\n",
      "enthusiast 4\n",
      "entir 12\n",
      "entiti 9\n",
      "entlang 1\n",
      "entrepreneurship 2\n",
      "entri 3\n",
      "entropi 4\n",
      "entsprechen 2\n",
      "entsprechend 1\n",
      "entstanden 1\n",
      "entstehen 2\n",
      "entsteht 1\n",
      "entwarfen 3\n",
      "entwickeln 6\n",
      "entwickelt 14\n",
      "entwickelten 4\n",
      "entwickl 1\n",
      "entwicklung 22\n",
      "entwicklungen 5\n",
      "entwicklungsprojekt 1\n",
      "entwurfsvorgehen 3\n",
      "enumer 1\n",
      "envelop 1\n",
      "environ 565\n",
      "environment 17\n",
      "environmez9 1\n",
      "envisor 1\n",
      "epipolar 1\n",
      "epm 16\n",
      "epson 2\n",
      "equal 1\n",
      "equat 3\n",
      "equip 21\n",
      "equit 1\n",
      "equival 4\n",
      "er 5\n",
      "era 3\n",
      "erb 1\n",
      "erdem 3\n",
      "ereidt 2\n",
      "eren 1\n",
      "erfahrbar 1\n",
      "erfahren 1\n",
      "erfahrungen 11\n",
      "erfassbar 6\n",
      "erfasst 3\n",
      "erfassungswerkzeug 1\n",
      "erfolgt 1\n",
      "erfolgten 1\n",
      "erforderlich 1\n",
      "erfordert 1\n",
      "erforscht 1\n",
      "erg 15\n",
      "ergeben 1\n",
      "ergebni 1\n",
      "ergebniss 11\n",
      "ergonom 7\n",
      "ergonomi 2\n",
      "ergonomisch 3\n",
      "ergonomischen 2\n",
      "erhalten 4\n",
      "erhoben 7\n",
      "erkennbar 1\n",
      "erkl 3\n",
      "erl 1\n",
      "erlauben 1\n",
      "erlaubt 2\n",
      "erleben 1\n",
      "erm 18\n",
      "erproben 1\n",
      "errat 1\n",
      "erratum 1\n",
      "erreichbar 1\n",
      "erreichbarkeit 1\n",
      "erreichbarkeiten 1\n",
      "erreicht 1\n",
      "erron 2\n",
      "error 37\n",
      "erscheinen 1\n",
      "erscheint 1\n",
      "erspac 1\n",
      "erst 8\n",
      "erstellten 3\n",
      "erstellung 1\n",
      "ersten 4\n",
      "ert 1\n",
      "erwachsen 1\n",
      "erwarten 1\n",
      "erwartungen 1\n",
      "erweitern 3\n",
      "erweitert 9\n",
      "erweiterten 1\n",
      "erweiterung 2\n",
      "erzeugt 1\n",
      "erzielt 1\n",
      "es 29\n",
      "escap 5\n",
      "esd 2\n",
      "especi 26\n",
      "essenc 2\n",
      "essenti 26\n",
      "establish 29\n",
      "esteem 1\n",
      "estim 82\n",
      "etablieren 1\n",
      "etablierung 1\n",
      "etfi6tti0fiz 1\n",
      "ether 1\n",
      "ethic 15\n",
      "ethnograph 2\n",
      "etwa 1\n",
      "etx 2\n",
      "eu 1\n",
      "euclidean 1\n",
      "euroitv2006 2\n",
      "europ 3\n",
      "european 18\n",
      "eurovr 6\n",
      "evalu 240\n",
      "evaluationen 1\n",
      "evaluationsfoku 1\n",
      "evaluationsphas 1\n",
      "evaluieren 1\n",
      "evaluiert 10\n",
      "evaluierung 1\n",
      "evar 2\n",
      "event 25\n",
      "eventu 1\n",
      "everydai 17\n",
      "everydaywork 1\n",
      "evid 24\n",
      "evl 2\n",
      "evok 5\n",
      "evolut 6\n",
      "evolutionari 2\n",
      "evolv 7\n",
      "ewolf 20\n",
      "exact 5\n",
      "exactli 1\n",
      "exam 1\n",
      "examin 46\n",
      "exampl 50\n",
      "excav 3\n",
      "exceed 1\n",
      "exceedingli 2\n",
      "excel 1\n",
      "except 1\n",
      "excess 1\n",
      "exchang 6\n",
      "excit 6\n",
      "exclud 6\n",
      "exclus 1\n",
      "execut 15\n",
      "exemplar 1\n",
      "exemplari 5\n",
      "exemplarili 2\n",
      "exemplarisch 2\n",
      "exemplifi 1\n",
      "exercis 41\n",
      "exergam 3\n",
      "exert 3\n",
      "exhaust 2\n",
      "exhibit 31\n",
      "exhort 1\n",
      "exihibit 1\n",
      "exist 42\n",
      "existierend 1\n",
      "exit 2\n",
      "expand 15\n",
      "expect 21\n",
      "expedit 2\n",
      "expens 5\n",
      "experi 366\n",
      "experienc 29\n",
      "experienti 4\n",
      "experiment 48\n",
      "experimentellen 1\n",
      "expert 28\n",
      "expertis 2\n",
      "explain 18\n",
      "explan 6\n",
      "expli 4\n",
      "explicit 7\n",
      "explicitli 1\n",
      "explizit 1\n",
      "exploit 11\n",
      "explor 161\n",
      "explorativ 3\n",
      "explorativen 3\n",
      "exploratori 15\n",
      "exploratoria 1\n",
      "explos 2\n",
      "expos 6\n",
      "exposur 23\n",
      "express 29\n",
      "extend 57\n",
      "extens 32\n",
      "extent 13\n",
      "extern 8\n",
      "exterocept 1\n",
      "extract 9\n",
      "extranet 1\n",
      "extrem 12\n",
      "extrud 1\n",
      "extrus 2\n",
      "ey 76\n",
      "eyeglass 1\n",
      "eyetoi 1\n",
      "f2 1\n",
      "fa 1\n",
      "fab 4\n",
      "fablab 3\n",
      "fabric 5\n",
      "fabrik 1\n",
      "fabrikplanung 3\n",
      "facad 1\n",
      "face 54\n",
      "facet 1\n",
      "fach 4\n",
      "fachbereichen 1\n",
      "fachcommun 1\n",
      "fachmagazin 1\n",
      "fachspezifisch 1\n",
      "facial 19\n",
      "facil 7\n",
      "facilit 26\n",
      "facto 2\n",
      "factor 62\n",
      "factori 11\n",
      "faculti 3\n",
      "fahranweisungen 1\n",
      "fahrzeug 4\n",
      "fahrzeugnavig 6\n",
      "fail 2\n",
      "failur 3\n",
      "fair 2\n",
      "faith 2\n",
      "fake 1\n",
      "faktor 1\n",
      "faktoren 1\n",
      "fall 15\n",
      "famili 2\n",
      "familiar 10\n",
      "famou 1\n",
      "fan 1\n",
      "fanci 8\n",
      "fantasi 4\n",
      "far 19\n",
      "farfield 1\n",
      "farthest 1\n",
      "fascin 3\n",
      "fashion 5\n",
      "fasst 1\n",
      "fast 8\n",
      "faster 10\n",
      "fastest 1\n",
      "faszinierenden 2\n",
      "favor 8\n",
      "fazit 1\n",
      "fb7 1\n",
      "fbw 3\n",
      "fcod 1\n",
      "fear 18\n",
      "feasibl 20\n",
      "featur 71\n",
      "fect 1\n",
      "fedeart 1\n",
      "feder 1\n",
      "federico 1\n",
      "feed 1\n",
      "feedback 62\n",
      "feel 21\n",
      "feet 2\n",
      "fehlt 1\n",
      "felixl7 7\n",
      "felt 4\n",
      "femal 27\n",
      "femtopro 6\n",
      "fertigung 3\n",
      "fertigungstechnik 1\n",
      "festgestellt 1\n",
      "feststellbar 1\n",
      "feststellen 1\n",
      "ffentlich 7\n",
      "ffentlichen 3\n",
      "ffnen 2\n",
      "fhw 3\n",
      "fi 1\n",
      "fiction 4\n",
      "fidel 17\n",
      "fiduci 1\n",
      "field 121\n",
      "fifteenth 1\n",
      "fifti 2\n",
      "figur 2\n",
      "file 1\n",
      "fill 5\n",
      "film 3\n",
      "filmmak 2\n",
      "filter 14\n",
      "final 36\n",
      "financi 3\n",
      "find 69\n",
      "finden 6\n",
      "fine 3\n",
      "finer 1\n",
      "finger 6\n",
      "fingerprint 1\n",
      "fingertip 1\n",
      "finland 1\n",
      "fire 6\n",
      "firm 1\n",
      "first 75\n",
      "fish 8\n",
      "fishey 1\n",
      "fit 6\n",
      "fix 5\n",
      "fixat 3\n",
      "flag 1\n",
      "flare 1\n",
      "flash 1\n",
      "flavor 3\n",
      "flbs12 3\n",
      "flexibilisiert 1\n",
      "flexibilisierten 1\n",
      "flexibl 28\n",
      "flickr 1\n",
      "flight 4\n",
      "flint63 21\n",
      "flipgrid 1\n",
      "float 2\n",
      "flood 1\n",
      "floor 8\n",
      "florian 5\n",
      "florianheinrich 2\n",
      "flow 20\n",
      "flowvr 2\n",
      "flugzeug 1\n",
      "flugzeugbau 1\n",
      "flugzeugindustri 1\n",
      "fluid 1\n",
      "fluoroscopi 2\n",
      "fly 5\n",
      "fmea 1\n",
      "fmri 4\n",
      "fn 1\n",
      "foam 1\n",
      "focal 1\n",
      "focu 37\n",
      "focus 26\n",
      "focusk3d 2\n",
      "focuss 1\n",
      "fog 6\n",
      "foil 1\n",
      "foku 4\n",
      "fokusgruppen 2\n",
      "folgend 1\n",
      "folgenderma 1\n",
      "follow 32\n",
      "food 6\n",
      "foodshar 3\n",
      "foot 5\n",
      "foothold 1\n",
      "footprint 1\n",
      "forc 19\n",
      "forde93 1\n",
      "forearm 2\n",
      "forecast 1\n",
      "forefront 2\n",
      "foreground 1\n",
      "foreign 9\n",
      "forens 3\n",
      "foreword 1\n",
      "forg 1\n",
      "fork 1\n",
      "form 47\n",
      "formal 5\n",
      "formanc 2\n",
      "format 10\n",
      "formen 3\n",
      "formenbau 1\n",
      "formula 1\n",
      "forschung 6\n",
      "forschungsdaten 1\n",
      "forschungsprojekt 2\n",
      "forschungsprozessen 1\n",
      "forschungssystemen 1\n",
      "fortbewegung 1\n",
      "fortbildung 1\n",
      "fortgeschritten 3\n",
      "forti 1\n",
      "fortschreitend 1\n",
      "forum 7\n",
      "forward 7\n",
      "foss 3\n",
      "foster 18\n",
      "found 53\n",
      "founda 1\n",
      "foundat 16\n",
      "fourteen 1\n",
      "fourth 2\n",
      "foveat 2\n",
      "fp7 1\n",
      "fprager 2\n",
      "fractur 1\n",
      "frage 2\n",
      "frageb 1\n",
      "fragen 3\n",
      "fragestellungen 3\n",
      "fragil 2\n",
      "fragment 3\n",
      "frame 26\n",
      "framework 132\n",
      "frameworkbasiert 1\n",
      "franc 2\n",
      "franco 2\n",
      "free 14\n",
      "free2fail 1\n",
      "freedom 7\n",
      "freeli 7\n",
      "freesoftwar 1\n",
      "freizeit 1\n",
      "fremdk 1\n",
      "frequenc 13\n",
      "frequent 5\n",
      "freshman 2\n",
      "friction 3\n",
      "friedman 1\n",
      "friend 1\n",
      "friendli 7\n",
      "frighten 1\n",
      "fring 1\n",
      "front 21\n",
      "frontal 2\n",
      "frontier 4\n",
      "fruchtbar 1\n",
      "fruit 12\n",
      "fruition 1\n",
      "frustrat 3\n",
      "ft 6\n",
      "ftv 1\n",
      "fu 8\n",
      "fuel 1\n",
      "fulfil 5\n",
      "full 41\n",
      "fulli 10\n",
      "fullpap 1\n",
      "fun 4\n",
      "function 47\n",
      "fund 5\n",
      "fundament 24\n",
      "fundamental 1\n",
      "funktionalit 4\n",
      "funktionen 6\n",
      "funktioniert 1\n",
      "fur 2\n",
      "furnitur 4\n",
      "furth 1\n",
      "fuse 2\n",
      "fusion 6\n",
      "futur 111\n",
      "futurist 2\n",
      "gab 1\n",
      "gadget 3\n",
      "gain 20\n",
      "gaipel 1\n",
      "gait 30\n",
      "gaitat 6\n",
      "gall 1\n",
      "galleri 3\n",
      "galvan 4\n",
      "gambl 48\n",
      "game 284\n",
      "gamelab 1\n",
      "gamelik 1\n",
      "gamepad 2\n",
      "gameplai 7\n",
      "gamif 9\n",
      "gamifi 16\n",
      "ganal 1\n",
      "ganz 2\n",
      "gap 23\n",
      "garden 28\n",
      "gather 4\n",
      "gaug 1\n",
      "gaussian 3\n",
      "gaze 23\n",
      "gbar 2\n",
      "gbent 1\n",
      "gcube 1\n",
      "ge 3\n",
      "gebastelt 1\n",
      "gebiet 2\n",
      "gebracht 1\n",
      "gebrauchstauglich 1\n",
      "gef 2\n",
      "gegeben 1\n",
      "gegen 2\n",
      "gegensatz 5\n",
      "gegenst 2\n",
      "gegenstand 3\n",
      "gegenw 1\n",
      "gehen 6\n",
      "gel 1\n",
      "gelassen 1\n",
      "gem 7\n",
      "gemacht 4\n",
      "gemeinhin 1\n",
      "gemeinsam 8\n",
      "gemeinsamen 1\n",
      "gemeinsamkeiten 3\n",
      "gemeinschaftlichen 1\n",
      "gen 7\n",
      "genau 1\n",
      "genauer 2\n",
      "genauigkeit 1\n",
      "gender 9\n",
      "gene 2\n",
      "genealogi 25\n",
      "gener 157\n",
      "generalsurgeri 1\n",
      "generierung 2\n",
      "genet 10\n",
      "genr 2\n",
      "gensurg 1\n",
      "genuin 3\n",
      "genutzt 6\n",
      "genvirtu 8\n",
      "geo 2\n",
      "geograph 8\n",
      "geolleri 1\n",
      "geoloc 1\n",
      "geolog 1\n",
      "geometr 10\n",
      "geometri 20\n",
      "geomorpholog 1\n",
      "geophys 2\n",
      "georghackenberg 1\n",
      "geoscienc 1\n",
      "geoscop 4\n",
      "geospati 1\n",
      "geotag 1\n",
      "geovisualis 1\n",
      "ger 2\n",
      "geringer 1\n",
      "german 11\n",
      "germani 3\n",
      "gesamten 1\n",
      "gesamtsystem 1\n",
      "geschraubt 1\n",
      "gesehen 3\n",
      "gesellschaft 3\n",
      "gesellschaftlich 1\n",
      "gesellschaftlichen 3\n",
      "gesprochen 2\n",
      "gesprochenen 1\n",
      "gestalt 1\n",
      "gestalten 4\n",
      "gestaltet 4\n",
      "gestaltung 11\n",
      "gestaltungsaspekt 6\n",
      "gestaltungsbereichen 2\n",
      "gestaltungsmerkmal 1\n",
      "gestaltungsmodel 1\n",
      "gestaltungsprozess 1\n",
      "gestattet 4\n",
      "gesteckt 1\n",
      "gestellt 1\n",
      "gesteuert 1\n",
      "gestiegen 1\n",
      "gestur 64\n",
      "gestutzten 1\n",
      "gesundheit 1\n",
      "getestet 3\n",
      "getit 6\n",
      "getrieben 1\n",
      "gew 4\n",
      "gewachsen 1\n",
      "gewerblich 1\n",
      "gewinnbringend 3\n",
      "geworden 1\n",
      "gezeigt 2\n",
      "ghost 1\n",
      "gi 3\n",
      "gibt 3\n",
      "gierdowski 1\n",
      "gif 1\n",
      "giro 1\n",
      "gito 1\n",
      "give 20\n",
      "given 19\n",
      "glass 18\n",
      "gleichen 3\n",
      "glich 10\n",
      "glichen 8\n",
      "glicher 1\n",
      "glichkeiten 21\n",
      "glichst 2\n",
      "glicht 10\n",
      "glichungsdidaktik 1\n",
      "glimps 1\n",
      "global 12\n",
      "globalen 1\n",
      "globe 1\n",
      "glossari 1\n",
      "glossi 1\n",
      "glove 3\n",
      "glycem 1\n",
      "glyfada 1\n",
      "go 3\n",
      "goal 33\n",
      "goalkeep 8\n",
      "goe 5\n",
      "gogh 1\n",
      "gon 5\n",
      "googl 11\n",
      "gopi 1\n",
      "gorilla 1\n",
      "gov 1\n",
      "govern 1\n",
      "gp 1\n",
      "gpu 2\n",
      "gr 7\n",
      "grab 5\n",
      "grace 1\n",
      "grade 9\n",
      "gradual 2\n",
      "graduat 3\n",
      "grafe 9\n",
      "graffiti 1\n",
      "grafik 1\n",
      "grafikkarten 1\n",
      "grain 2\n",
      "gram 1\n",
      "grandstand 2\n",
      "grant 3\n",
      "graph 21\n",
      "graphic 66\n",
      "graphomotor 3\n",
      "graphxml 1\n",
      "grapp2006 4\n",
      "grasp 2\n",
      "graviti 6\n",
      "great 17\n",
      "greater 10\n",
      "greatli 8\n",
      "greec 4\n",
      "green 4\n",
      "greenbib 26\n",
      "gregor 2\n",
      "grei 1\n",
      "greifbaren 6\n",
      "greifen 1\n",
      "grenzen 2\n",
      "grid 4\n",
      "gridinoc 1\n",
      "griesbau 1\n",
      "grimag 1\n",
      "gro 6\n",
      "groov 1\n",
      "grope 1\n",
      "ground 19\n",
      "groundcam 1\n",
      "groundwork 3\n",
      "group 119\n",
      "groupwar 1\n",
      "grow 10\n",
      "growth 4\n",
      "gruen 1\n",
      "grundb 1\n",
      "grundlag 6\n",
      "grundlagen 8\n",
      "grundlegend 1\n",
      "grundlegenden 2\n",
      "grupp 1\n",
      "gs 3\n",
      "gt 1\n",
      "guest 4\n",
      "gui 3\n",
      "guid 68\n",
      "guidanc 15\n",
      "guidar 1\n",
      "guidebook 4\n",
      "guidelin 20\n",
      "guitar 1\n",
      "gulliv 1\n",
      "gung 7\n",
      "gungen 1\n",
      "gut 1\n",
      "guten 1\n",
      "guter 1\n",
      "gv 4\n",
      "gymnasialen 6\n",
      "gymvr 1\n",
      "gynaecolog 2\n",
      "gynaecologist 3\n",
      "gyru 1\n",
      "h2020 3\n",
      "haben 13\n",
      "hack 1\n",
      "hacker 2\n",
      "hackerspac 12\n",
      "hackspac 1\n",
      "haesler 2\n",
      "haifa 2\n",
      "halbautomatisch 2\n",
      "half 7\n",
      "hall 1\n",
      "hallucin 6\n",
      "halv 1\n",
      "hanapp 2\n",
      "hand 90\n",
      "handbal 5\n",
      "handbook 4\n",
      "handbuch 2\n",
      "handel 1\n",
      "handelt 2\n",
      "handgestenerkennung 1\n",
      "handheld 45\n",
      "handicraft 1\n",
      "handl 14\n",
      "handlungsfeld 1\n",
      "handlungspraktisch 6\n",
      "handrail 2\n",
      "handset 1\n",
      "handwerklich 1\n",
      "handwrit 18\n",
      "hannov 3\n",
      "hannoverschen 1\n",
      "happen 1\n",
      "haptic 40\n",
      "hapticsnak 1\n",
      "haptiqu 1\n",
      "haptisch 8\n",
      "hapto 1\n",
      "har 1\n",
      "hard 5\n",
      "hardand 1\n",
      "hardcov 1\n",
      "hardwar 33\n",
      "harevir 1\n",
      "harm 5\n",
      "harmon 3\n",
      "hart 1\n",
      "hasn 1\n",
      "hat 3\n",
      "haunert 1\n",
      "haupt 1\n",
      "hauptelement 1\n",
      "hauptgrund 1\n",
      "havior 2\n",
      "hci 147\n",
      "hdr 3\n",
      "hdrc 1\n",
      "head 121\n",
      "headlight 1\n",
      "headphon 4\n",
      "headset 12\n",
      "heal 1\n",
      "health 36\n",
      "healthcar 18\n",
      "healthi 15\n",
      "hear 5\n",
      "heard 1\n",
      "heart 6\n",
      "heartbeat 10\n",
      "heavi 2\n",
      "heavili 6\n",
      "height 8\n",
      "hein 10\n",
      "heinrich 2\n",
      "heinrichk 1\n",
      "hel 1\n",
      "held 13\n",
      "helden 1\n",
      "helfer 1\n",
      "helicoptor 1\n",
      "hellen 2\n",
      "helmet 11\n",
      "help 52\n",
      "helper 1\n",
      "hemiparet 4\n",
      "hemiplegia 1\n",
      "hepato 1\n",
      "herausforderungen 2\n",
      "herbert 1\n",
      "here 17\n",
      "heritag 27\n",
      "herk 4\n",
      "hero 1\n",
      "herstellung 1\n",
      "herzschlagfrequenz 4\n",
      "heterogen 10\n",
      "heurist 4\n",
      "heut 1\n",
      "heutig 3\n",
      "heutzutag 1\n",
      "hidden 4\n",
      "hide 2\n",
      "hier 8\n",
      "hierarch 7\n",
      "hierarchi 1\n",
      "hierbei 4\n",
      "hierf 2\n",
      "higer 1\n",
      "high 121\n",
      "higher 44\n",
      "highest 4\n",
      "highli 24\n",
      "highlight 21\n",
      "highwirepr 1\n",
      "higkeit 4\n",
      "hikeyb 1\n",
      "hin 2\n",
      "hinau 2\n",
      "hinder 1\n",
      "hing 1\n",
      "hinreichend 2\n",
      "hinsichtlich 8\n",
      "hint 2\n",
      "hinter 2\n",
      "hintergr 1\n",
      "hintergrund 8\n",
      "histor 10\n",
      "histori 10\n",
      "hit 4\n",
      "hitl 1\n",
      "hlga 1\n",
      "hlt 1\n",
      "hlten 1\n",
      "hmd 57\n",
      "hoc 1\n",
      "hoch 1\n",
      "hochschullehr 12\n",
      "hochwertig 1\n",
      "hohe 2\n",
      "hohem 1\n",
      "hoher 3\n",
      "hold 10\n",
      "holist 2\n",
      "holler 1\n",
      "holo 1\n",
      "holoart 2\n",
      "holobench 1\n",
      "holocaust 1\n",
      "holocpr 3\n",
      "hologram 8\n",
      "holograph 7\n",
      "hololearn 2\n",
      "hololen 11\n",
      "hololensplan 1\n",
      "hololenstm 1\n",
      "holomol 1\n",
      "holosim 2\n",
      "holst 1\n",
      "home 19\n",
      "homecom 2\n",
      "homewindow 1\n",
      "homographi 1\n",
      "homuncular 1\n",
      "honeybe 3\n",
      "hong 1\n",
      "hope 4\n",
      "horizont 2\n",
      "horror 1\n",
      "hospit 7\n",
      "host 4\n",
      "hot 1\n",
      "hotspot 1\n",
      "hour 9\n",
      "hous 3\n",
      "howtext 1\n",
      "href 1\n",
      "hrend 2\n",
      "hri 3\n",
      "hrleisten 2\n",
      "hrt 3\n",
      "hrten 5\n",
      "hrtf 2\n",
      "hrung 10\n",
      "hrv 12\n",
      "ht09 1\n",
      "ht2009 1\n",
      "htc 2\n",
      "html5 6\n",
      "http 3173\n",
      "huge 4\n",
      "hull 1\n",
      "hum 2\n",
      "human 204\n",
      "humanitarian 1\n",
      "humanoid 6\n",
      "hundr 2\n",
      "hunt 2\n",
      "hurdl 1\n",
      "hurtienn 2\n",
      "hybrid 32\n",
      "hybridsocieti 2\n",
      "hydrocephalu 2\n",
      "hydror 1\n",
      "hygien 1\n",
      "hype 1\n",
      "hyper 2\n",
      "hypermedia 6\n",
      "hyperr 1\n",
      "hyperrealist 1\n",
      "hypersurfac 1\n",
      "hypothes 6\n",
      "hypothesi 4\n",
      "hz 7\n",
      "i40 1\n",
      "iceberg 1\n",
      "ich 1\n",
      "icon 6\n",
      "ict 2\n",
      "ictic 2\n",
      "id 2\n",
      "ide 1\n",
      "idea 25\n",
      "ideal 4\n",
      "ideat 3\n",
      "ideen 2\n",
      "ident 18\n",
      "identif 9\n",
      "identifi 56\n",
      "identifizieren 3\n",
      "ideseditor 2\n",
      "ieee 14\n",
      "ieee2019 2\n",
      "ieeexplor 1\n",
      "ifla 3\n",
      "igen 1\n",
      "ignor 4\n",
      "igstk 1\n",
      "igt 18\n",
      "ihnen 1\n",
      "ihr 4\n",
      "ihrem 6\n",
      "ihrer 3\n",
      "ii 7\n",
      "iii 5\n",
      "iim 2\n",
      "ijact 1\n",
      "iji 3\n",
      "ijtsrd 31\n",
      "ilin 1\n",
      "ill 4\n",
      "illinoi 1\n",
      "illumin 22\n",
      "illus 25\n",
      "illusion 1\n",
      "illustr 18\n",
      "ilmxlab 1\n",
      "im 77\n",
      "imag 137\n",
      "imageri 4\n",
      "imagin 5\n",
      "imbal 1\n",
      "imit 2\n",
      "immediaci 1\n",
      "immer 6\n",
      "immers 309\n",
      "immobil 2\n",
      "impact 72\n",
      "impair 33\n",
      "impart 2\n",
      "imper 1\n",
      "impercept 3\n",
      "imperfect 2\n",
      "imperi 1\n",
      "implement 129\n",
      "impli 3\n",
      "implic 21\n",
      "implicit 7\n",
      "import 117\n",
      "importantli 4\n",
      "imposs 1\n",
      "impostor 36\n",
      "impract 1\n",
      "imprecis 1\n",
      "impress 8\n",
      "improv 115\n",
      "imu 1\n",
      "imwesentlichen 2\n",
      "inabl 6\n",
      "inaccur 1\n",
      "inadequ 1\n",
      "inar 1\n",
      "inbar 1\n",
      "inborn 2\n",
      "incept 1\n",
      "incid 3\n",
      "incis 2\n",
      "inclin 2\n",
      "includ 108\n",
      "inclus 18\n",
      "incongru 22\n",
      "inconsist 2\n",
      "inconveni 2\n",
      "incorpor 23\n",
      "increa 1\n",
      "increas 93\n",
      "increasingli 12\n",
      "increment 1\n",
      "incret 2\n",
      "independ 17\n",
      "index 13\n",
      "india 1\n",
      "indic 17\n",
      "indiffer 3\n",
      "indikationen 1\n",
      "indirect 4\n",
      "individu 44\n",
      "individualis 1\n",
      "individualisiert 7\n",
      "individuel 2\n",
      "indoor 14\n",
      "indoorlokalisierung 1\n",
      "indoornavig 4\n",
      "indroductori 1\n",
      "induc 31\n",
      "induct 4\n",
      "industri 66\n",
      "industriel 6\n",
      "industriellen 2\n",
      "industrieumfeld 1\n",
      "industry4 1\n",
      "inevit 2\n",
      "inexpens 11\n",
      "infanc 1\n",
      "infer 6\n",
      "influenc 90\n",
      "influenti 2\n",
      "infop 1\n",
      "inform 28\n",
      "informat 2\n",
      "informatik 24\n",
      "informationen 23\n",
      "informationretriec 1\n",
      "informationsdienst 1\n",
      "informationsentnahm 1\n",
      "informationstechnik 1\n",
      "informatorisch 2\n",
      "infrar 1\n",
      "infrastructur 16\n",
      "infrastrukturen 1\n",
      "infring 2\n",
      "ing 10\n",
      "ingredi 1\n",
      "ingress 3\n",
      "inhalt 2\n",
      "inhaltlichen 1\n",
      "inherit 1\n",
      "inhibit 2\n",
      "initi 36\n",
      "initiativen 1\n",
      "inject 1\n",
      "injectx 7\n",
      "injur 10\n",
      "injuri 11\n",
      "ink 6\n",
      "inner 1\n",
      "innerhalb 4\n",
      "innocu 1\n",
      "innov 34\n",
      "innovativ 2\n",
      "inpati 5\n",
      "input 51\n",
      "inquiri 10\n",
      "insbesonder 4\n",
      "insect 1\n",
      "insert 1\n",
      "insid 7\n",
      "insight 25\n",
      "inspect 4\n",
      "inspektionsrobotern 1\n",
      "inspir 9\n",
      "instal 5\n",
      "instanc 8\n",
      "instandhaltung 2\n",
      "instant 4\n",
      "institut 10\n",
      "instruct 19\n",
      "instructionaldesign 1\n",
      "instructiv 2\n",
      "instructor 15\n",
      "instrument 9\n",
      "insuffici 2\n",
      "insync 11\n",
      "int 2\n",
      "intak 1\n",
      "integr 98\n",
      "integriert 1\n",
      "integrierten 3\n",
      "intellectu 3\n",
      "intellig 54\n",
      "intelligent 1\n",
      "intend 9\n",
      "intens 2\n",
      "intent 9\n",
      "inter 25\n",
      "interact 698\n",
      "interactionsuitcas 12\n",
      "interactivo 1\n",
      "interagieren 2\n",
      "interakt 34\n",
      "interaktionen 7\n",
      "interaktionsart 2\n",
      "interaktionsarten 4\n",
      "interaktionsbereich 6\n",
      "interaktionskonzept 4\n",
      "interaktionsm 6\n",
      "interaktiv 2\n",
      "interconnect 2\n",
      "intercultur 6\n",
      "interd 1\n",
      "interdepend 1\n",
      "interdisciplinari 7\n",
      "interdisziplin 18\n",
      "interdisziplinarit 1\n",
      "interessant 1\n",
      "interest 34\n",
      "interestingli 2\n",
      "interf 1\n",
      "interfac 266\n",
      "interfaceconcept 1\n",
      "interfer 2\n",
      "intergener 1\n",
      "interior 3\n",
      "interkulturel 3\n",
      "interkulturellen 3\n",
      "interleav 4\n",
      "interlocutor 2\n",
      "intermedi 10\n",
      "intermediari 1\n",
      "intern 28\n",
      "internet 19\n",
      "interoper 2\n",
      "interperson 3\n",
      "interpol 2\n",
      "interpret 13\n",
      "interpretiert 1\n",
      "interprocess 1\n",
      "interquartil 2\n",
      "interrat 2\n",
      "interrel 2\n",
      "interscop 3\n",
      "intersect 3\n",
      "interv 1\n",
      "interven 1\n",
      "intervent 74\n",
      "interventionen 3\n",
      "interview 17\n",
      "intml 2\n",
      "intra 3\n",
      "intraclass 2\n",
      "intraop 2\n",
      "intric 1\n",
      "intrins 7\n",
      "intro 3\n",
      "introduc 71\n",
      "introduct 16\n",
      "introductori 1\n",
      "intrus 4\n",
      "intuit 34\n",
      "intuitiv 4\n",
      "invad 3\n",
      "invalid 2\n",
      "invalu 1\n",
      "invari 2\n",
      "invas 10\n",
      "invent 5\n",
      "inventor 1\n",
      "invers 4\n",
      "invest 4\n",
      "investig 120\n",
      "invis 3\n",
      "invit 5\n",
      "invok 1\n",
      "involv 34\n",
      "inwiefern 1\n",
      "iot 4\n",
      "iowa 9\n",
      "ipsilater 1\n",
      "iption 1\n",
      "ir 1\n",
      "iron 1\n",
      "irvo 1\n",
      "isa 3\n",
      "isbn 1\n",
      "isch 1\n",
      "ishioma 1\n",
      "island 2\n",
      "ismar 1\n",
      "isn 1\n",
      "iso 3\n",
      "isol 2\n",
      "issenberg 1\n",
      "issn 8\n",
      "issu 77\n",
      "ist 33\n",
      "istanbul 1\n",
      "istophan 1\n",
      "iswc08 1\n",
      "itc 2\n",
      "itc30 1\n",
      "itegpub 13\n",
      "item 8\n",
      "iter 6\n",
      "iti 1\n",
      "itt 1\n",
      "itv 2\n",
      "iv 2\n",
      "iva 6\n",
      "ivbo 1\n",
      "ivr 3\n",
      "iwdr2015 1\n",
      "jabreftest 2\n",
      "jacket 1\n",
      "jacob 1\n",
      "jahr 2\n",
      "jahren 4\n",
      "jai 1\n",
      "jam 3\n",
      "januar 1\n",
      "japan 2\n",
      "japanes 2\n",
      "java 2\n",
      "je 1\n",
      "jection 1\n",
      "jedem 1\n",
      "jederzeit 1\n",
      "jedoch 5\n",
      "jellyfish 1\n",
      "jenniferhaefn 4\n",
      "jennifertied 3\n",
      "jeroen 1\n",
      "jeweil 1\n",
      "jeweiligen 3\n",
      "jha 2\n",
      "jinghuailin 1\n",
      "jitter 2\n",
      "jmu 16\n",
      "jnd 1\n",
      "jockei 1\n",
      "jofo 2\n",
      "jog 1\n",
      "joi 2\n",
      "join 2\n",
      "joint 4\n",
      "jointli 1\n",
      "journal 24\n",
      "journalismu 1\n",
      "joy 1\n",
      "joystick 2\n",
      "jpkobler 2\n",
      "jpmor 2\n",
      "jpstauffert 3\n",
      "judg 8\n",
      "judgment 7\n",
      "jugendlich 1\n",
      "jugendlichen 1\n",
      "julier 1\n",
      "jump 1\n",
      "jumpar 1\n",
      "june 2\n",
      "jung 1\n",
      "just 12\n",
      "juz 1\n",
      "jvrb 20\n",
      "jvrc 2\n",
      "kabl 1\n",
      "kaidan 1\n",
      "kanji 1\n",
      "kann 10\n",
      "kapitel 10\n",
      "kappa 1\n",
      "kart 3\n",
      "karten 7\n",
      "katastrophenschutz 1\n",
      "kaufen 1\n",
      "kaum 3\n",
      "kb 1\n",
      "keep 3\n",
      "kei 34\n",
      "keil 1\n",
      "kein 3\n",
      "kenntni 1\n",
      "kenntniss 1\n",
      "kept 3\n",
      "keyboard 20\n",
      "keynot 2\n",
      "keynvis 1\n",
      "keyword 1\n",
      "kfz 1\n",
      "kh 2\n",
      "kick 2\n",
      "kid 1\n",
      "killer 1\n",
      "kilobot 1\n",
      "kind 9\n",
      "kinder 1\n",
      "kindern 1\n",
      "kinect 17\n",
      "kinemat 8\n",
      "king 1\n",
      "kingdom 1\n",
      "kingslei 1\n",
      "kinofilmen 2\n",
      "kit 4\n",
      "klinisch 1\n",
      "kmd 4\n",
      "knapsack 1\n",
      "knob 1\n",
      "knot 2\n",
      "know 7\n",
      "knowledg 76\n",
      "known 14\n",
      "kochm 1\n",
      "kognitiv 1\n",
      "kolb 1\n",
      "kollabor 2\n",
      "kologisch 1\n",
      "kombin 1\n",
      "kommenden 2\n",
      "kommerziellen 3\n",
      "kommunik 5\n",
      "kommunikationsort 1\n",
      "kompensiert 1\n",
      "kompetenz 3\n",
      "kompetenzentwicklung 2\n",
      "kompetenzerwerb 6\n",
      "komplex 5\n",
      "komplexen 1\n",
      "komplexeren 1\n",
      "komponenten 1\n",
      "kong 1\n",
      "konkret 3\n",
      "konkreten 1\n",
      "konkretisiert 2\n",
      "konnt 4\n",
      "konnten 3\n",
      "konomischen 3\n",
      "kontaktanalog 1\n",
      "kontext 7\n",
      "kontextbezogen 3\n",
      "kontinuierlich 1\n",
      "kontraindikationen 1\n",
      "kontrolliert 1\n",
      "konventionel 1\n",
      "konventionellen 3\n",
      "konzentr 1\n",
      "konzept 33\n",
      "kooperationspartn 1\n",
      "kooperativen 2\n",
      "kopfbewegung 1\n",
      "korwisi 1\n",
      "kosten 2\n",
      "kostenersparni 1\n",
      "kosteng 1\n",
      "kreativen 1\n",
      "kreativr 1\n",
      "kreuzungen 3\n",
      "krevelen 3\n",
      "krl 12\n",
      "kroski 1\n",
      "kruskal 1\n",
      "ksva07 1\n",
      "kulturel 3\n",
      "kulturellen 1\n",
      "kundenwunsch 1\n",
      "kvogelei 1\n",
      "kw 54\n",
      "l3 1\n",
      "la 6\n",
      "lab 27\n",
      "labor 1\n",
      "laboratori 29\n",
      "laborstudi 1\n",
      "labyrinth 1\n",
      "lack 29\n",
      "lag 6\n",
      "lage 2\n",
      "lagebestimmung 2\n",
      "lagerprozess 1\n",
      "lai 5\n",
      "laimbe 1\n",
      "lamp 1\n",
      "lan 1\n",
      "land 2\n",
      "landau 1\n",
      "landeck 1\n",
      "landmark 4\n",
      "landscap 5\n",
      "lang 1\n",
      "langfristig 1\n",
      "langsam 1\n",
      "languag 19\n",
      "lanham 1\n",
      "lanzagorta 1\n",
      "lap 1\n",
      "laparoscop 29\n",
      "laparoscopi 3\n",
      "laparoscopist 1\n",
      "laptop 4\n",
      "lar 1\n",
      "larg 58\n",
      "larger 18\n",
      "largest 2\n",
      "laser 10\n",
      "last 5\n",
      "late 4\n",
      "latenc 37\n",
      "later 6\n",
      "latest 4\n",
      "latoschik 5\n",
      "launch 1\n",
      "laval 2\n",
      "lave 1\n",
      "law 4\n",
      "layer 33\n",
      "layout 4\n",
      "layoutgestaltung 1\n",
      "layoutplanung 3\n",
      "layup 2\n",
      "lbp06 2\n",
      "lc 4\n",
      "lcd 10\n",
      "le 2\n",
      "lead 51\n",
      "leader 1\n",
      "leagu 1\n",
      "leakag 1\n",
      "lean 1\n",
      "learn 335\n",
      "learner 13\n",
      "learninganalyt 2\n",
      "learnt 2\n",
      "leas 1\n",
      "least 5\n",
      "leav 2\n",
      "leben 3\n",
      "leccear 1\n",
      "lectur 9\n",
      "lectura 1\n",
      "led 5\n",
      "ledood 1\n",
      "left 4\n",
      "legaci 2\n",
      "legal 2\n",
      "legen 1\n",
      "legend 1\n",
      "legibl 7\n",
      "lehr 16\n",
      "lehrb 1\n",
      "lehrbuch 5\n",
      "lehren 6\n",
      "lehrerbildung 6\n",
      "lehrpersonen 6\n",
      "lehrveranstaltungen 2\n",
      "leicht 1\n",
      "leiden 1\n",
      "leisbi 1\n",
      "leistet 1\n",
      "leistung 1\n",
      "leistungsf 1\n",
      "leitfaden 1\n",
      "leitprojekt 5\n",
      "len 4\n",
      "leng 1\n",
      "length 2\n",
      "lengthen 1\n",
      "lengthi 1\n",
      "leninsha 2\n",
      "lens 1\n",
      "lepski 2\n",
      "lernen 19\n",
      "lernend 3\n",
      "lernenden 1\n",
      "lernerfolg 1\n",
      "lernkontext 1\n",
      "lernkurv 1\n",
      "lernleistung 1\n",
      "lernort 1\n",
      "lernprozess 13\n",
      "lernr 1\n",
      "lernszenario 1\n",
      "lernwelt 1\n",
      "leser 2\n",
      "lesion 6\n",
      "lesser 1\n",
      "lesson 8\n",
      "let 2\n",
      "lethal 1\n",
      "letter 1\n",
      "letzten 4\n",
      "letztlich 1\n",
      "level 83\n",
      "lever 1\n",
      "leverag 16\n",
      "lexic 8\n",
      "lfte 1\n",
      "li 5\n",
      "liaison 1\n",
      "liarokapi 2\n",
      "librari 117\n",
      "librarian 13\n",
      "libro 1\n",
      "licens 6\n",
      "lich 2\n",
      "lidar 5\n",
      "liefert 1\n",
      "liegt 2\n",
      "lieu 1\n",
      "life 39\n",
      "lifestyl 1\n",
      "light 37\n",
      "lightmap 1\n",
      "lightn 2\n",
      "lightprob 1\n",
      "lightskin 1\n",
      "lightweight 7\n",
      "likabl 1\n",
      "like 43\n",
      "likert 2\n",
      "limb 2\n",
      "limit 52\n",
      "line 11\n",
      "linear 5\n",
      "lineup 1\n",
      "lingual 2\n",
      "linguist 3\n",
      "linguisticanalysi 1\n",
      "link 17\n",
      "linux 4\n",
      "liquid 5\n",
      "lis3d 1\n",
      "list 6\n",
      "listen 1\n",
      "liter 1\n",
      "literaci 7\n",
      "literari 1\n",
      "literatur 24\n",
      "literaturversorgung 1\n",
      "lithuania 2\n",
      "littl 12\n",
      "littlefield 1\n",
      "live 39\n",
      "liver 2\n",
      "liwc 1\n",
      "lizenzierung 1\n",
      "ll 3\n",
      "lle 2\n",
      "llinger 4\n",
      "llnwd 1\n",
      "lnteract 1\n",
      "lnterakt 1\n",
      "lo 1\n",
      "load 20\n",
      "loaner 1\n",
      "lobe 1\n",
      "local 22\n",
      "locat 59\n",
      "lock 3\n",
      "locomot 9\n",
      "locomotor 2\n",
      "locui 5\n",
      "log 2\n",
      "logic 9\n",
      "logist 2\n",
      "lokalisierung 1\n",
      "long 2\n",
      "longer 10\n",
      "look 23\n",
      "loop 4\n",
      "loos 4\n",
      "lose 1\n",
      "loss 4\n",
      "lost 2\n",
      "low 64\n",
      "lower 23\n",
      "lowest 4\n",
      "lrw 1\n",
      "lt 1\n",
      "ltni 1\n",
      "ltomlin21 1\n",
      "lucasp 1\n",
      "ludolog 1\n",
      "lugrin 7\n",
      "lugrinj 6\n",
      "lumin 2\n",
      "luminos 2\n",
      "lumipen 1\n",
      "m100 1\n",
      "ma2ra 1\n",
      "macao 1\n",
      "machen 3\n",
      "machin 37\n",
      "machineri 3\n",
      "macht 2\n",
      "maco 4\n",
      "macromedia 1\n",
      "mad 5\n",
      "made 23\n",
      "mag 1\n",
      "magazin 3\n",
      "maggie2008 2\n",
      "magia 1\n",
      "magic 2\n",
      "magicmeet 2\n",
      "magnet 8\n",
      "magnetomet 2\n",
      "magnetosensit 2\n",
      "magnifi 1\n",
      "magnitud 2\n",
      "mai 44\n",
      "mainland 1\n",
      "mainli 6\n",
      "mainstream 1\n",
      "maintain 20\n",
      "mainteintfi 1\n",
      "mainten 17\n",
      "major 16\n",
      "mak 1\n",
      "make 104\n",
      "makeabl 1\n",
      "makeawar 1\n",
      "maker 32\n",
      "makerbu 2\n",
      "makermov 1\n",
      "makerspac 193\n",
      "maketec 1\n",
      "makeup 1\n",
      "mal 3\n",
      "male 7\n",
      "mall 1\n",
      "malposit 2\n",
      "mamul 1\n",
      "mamulengo 1\n",
      "man 12\n",
      "manag 75\n",
      "manageri 1\n",
      "mandala 1\n",
      "mandatori 2\n",
      "mandibl 4\n",
      "maneuver 1\n",
      "manifest 4\n",
      "manifesto 1\n",
      "manifold 8\n",
      "manikin 1\n",
      "manipul 57\n",
      "mann 4\n",
      "mannequin 1\n",
      "manner 13\n",
      "manoeuvr 1\n",
      "mansisharma 2\n",
      "manual 8\n",
      "manuel 2\n",
      "manuellen 3\n",
      "manufactur 15\n",
      "map 36\n",
      "mapp 1\n",
      "mar 5\n",
      "marbl 1\n",
      "marc 4\n",
      "marcerich 13\n",
      "march 11\n",
      "mare 1\n",
      "margin 2\n",
      "maria 1\n",
      "marin 2\n",
      "maritim 3\n",
      "mark 1\n",
      "marker 31\n",
      "markerbasiert 2\n",
      "markerbasierten 1\n",
      "markerless 11\n",
      "market 14\n",
      "marketpac 1\n",
      "marketplac 3\n",
      "marketspac 17\n",
      "mart 1\n",
      "martinverduzco 1\n",
      "maschin 3\n",
      "maschinen 3\n",
      "maschinenbau 4\n",
      "mask 5\n",
      "mass 12\n",
      "massiv 3\n",
      "master 2\n",
      "mastoid 4\n",
      "mastoidectomi 8\n",
      "mat 1\n",
      "match 14\n",
      "matchless 1\n",
      "materi 18\n",
      "materiel 2\n",
      "materiellen 1\n",
      "math 2\n",
      "mathemat 8\n",
      "mathland 1\n",
      "matrix 4\n",
      "matter 8\n",
      "matthew 1\n",
      "matur 3\n",
      "maxillofaci 1\n",
      "maxim 9\n",
      "maximis 2\n",
      "maximum 1\n",
      "mayb 3\n",
      "maze 6\n",
      "mcginiti 1\n",
      "mcm 22\n",
      "meal 2\n",
      "mean 40\n",
      "meaning 9\n",
      "meant 4\n",
      "measur 134\n",
      "mechan 31\n",
      "mechatron 2\n",
      "medar 1\n",
      "media 28\n",
      "median 8\n",
      "mediat 24\n",
      "medic 63\n",
      "medicin 15\n",
      "medien 3\n",
      "medienbeitr 6\n",
      "medienphilosophi 1\n",
      "medienphilosophisch 1\n",
      "medienwissenschaft 1\n",
      "medika 1\n",
      "medio 1\n",
      "mediolater 2\n",
      "medit 9\n",
      "medium 21\n",
      "medizin 1\n",
      "medizintechnik 1\n",
      "medlin 2\n",
      "medpaeddokuwiki 1\n",
      "meet 29\n",
      "mehr 5\n",
      "mehrwert 1\n",
      "meist 3\n",
      "meisten 2\n",
      "melbourn 4\n",
      "mellifera 1\n",
      "mellitu 1\n",
      "membran 3\n",
      "memholo 1\n",
      "memor 1\n",
      "memori 11\n",
      "men 2\n",
      "meneteqel 1\n",
      "mensch 11\n",
      "menschen 2\n",
      "menschtechnik 1\n",
      "ment 1\n",
      "mental 30\n",
      "mention 6\n",
      "mentor 2\n",
      "menu 7\n",
      "merdan 1\n",
      "mere 3\n",
      "merg 6\n",
      "merlo 4\n",
      "mesh 16\n",
      "mesmer 1\n",
      "mess 1\n",
      "messag 2\n",
      "messung 3\n",
      "met 3\n",
      "meta 1\n",
      "metacognit 3\n",
      "metadata 2\n",
      "metadaten 1\n",
      "metal 1\n",
      "metamodel 1\n",
      "metaphor 20\n",
      "metaphys 1\n",
      "metasurfac 1\n",
      "metavers 1\n",
      "meter 7\n",
      "method 152\n",
      "methoden 6\n",
      "methodisch 1\n",
      "methodolog 24\n",
      "methodsrandom 1\n",
      "metric 14\n",
      "mfiedler 2\n",
      "mfischbach 2\n",
      "mh4ller 18\n",
      "mhealth 1\n",
      "mhp 4\n",
      "mi 8\n",
      "mich 1\n",
      "michael 1\n",
      "micosoft 1\n",
      "micro 2\n",
      "microopt 1\n",
      "microscop 8\n",
      "microsoft 13\n",
      "mid 7\n",
      "middl 1\n",
      "middlewar 4\n",
      "midi 1\n",
      "mig 1\n",
      "migrat 2\n",
      "mild 1\n",
      "mileston 2\n",
      "milgram 2\n",
      "militari 4\n",
      "million 1\n",
      "millisecond 1\n",
      "mimick 2\n",
      "min 1\n",
      "mind 50\n",
      "mine 5\n",
      "mini 1\n",
      "miniatur 7\n",
      "minim 23\n",
      "minimis 1\n",
      "minimum 1\n",
      "minitrack 1\n",
      "minut 16\n",
      "mira 1\n",
      "mirag 1\n",
      "mirar 1\n",
      "mirart 1\n",
      "mirror 55\n",
      "mirrorworld 1\n",
      "misalign 1\n",
      "miscellan 1\n",
      "misestim 4\n",
      "mismatch 9\n",
      "mispercept 2\n",
      "miss 9\n",
      "mission 2\n",
      "mist 1\n",
      "mistak 1\n",
      "mit 115\n",
      "mitarbeitend 2\n",
      "mitarbeitern 2\n",
      "mitbegr 1\n",
      "miteinand 1\n",
      "miteinzubeziehen 1\n",
      "mitgestaltung 1\n",
      "mitig 4\n",
      "mitt 1\n",
      "mittel 5\n",
      "mittelfristigen 1\n",
      "mitteln 2\n",
      "mittelpunkt 2\n",
      "mittelst 1\n",
      "mix 1205\n",
      "mixfab 1\n",
      "mixin3d 1\n",
      "mixt 3\n",
      "mize 1\n",
      "mlearn 1\n",
      "mm 13\n",
      "mmi 17\n",
      "mmisiak 4\n",
      "mmlich 1\n",
      "mmlichen 2\n",
      "mmlicher 1\n",
      "mmog 2\n",
      "mmrpet 1\n",
      "mmspub 38\n",
      "mmvr 1\n",
      "mnemon 1\n",
      "mobil 286\n",
      "mobilecomput 3\n",
      "mobilemak 1\n",
      "mobilen 1\n",
      "mobilit 5\n",
      "mock 1\n",
      "mockup 1\n",
      "modal 23\n",
      "mode 28\n",
      "model 170\n",
      "moder 12\n",
      "modern 22\n",
      "modernen 5\n",
      "modif 16\n",
      "modifi 22\n",
      "modul 53\n",
      "modular 9\n",
      "modulat 5\n",
      "mohana 1\n",
      "mohit 1\n",
      "molecular 2\n",
      "moment 3\n",
      "mon 1\n",
      "monei 1\n",
      "monitor 28\n",
      "mono 2\n",
      "monocular 7\n",
      "monographi 1\n",
      "monolith 2\n",
      "monoscop 7\n",
      "monotonen 1\n",
      "montag 6\n",
      "montageablaufplanung 1\n",
      "montagearbeitspl 3\n",
      "montageprozess 3\n",
      "montageprozessen 1\n",
      "montageszenarien 2\n",
      "montaget 2\n",
      "montagezeiten 1\n",
      "month 1\n",
      "moon 1\n",
      "morbid 1\n",
      "morgan 2\n",
      "morgen 1\n",
      "mortal 1\n",
      "mosaic 2\n",
      "motek 1\n",
      "mother 1\n",
      "motion 69\n",
      "motiv 65\n",
      "motivational 1\n",
      "motor 34\n",
      "motorischem 1\n",
      "mount 93\n",
      "mountain 1\n",
      "mous 2\n",
      "move 22\n",
      "movement 77\n",
      "moverio 2\n",
      "movi 4\n",
      "mozaik 1\n",
      "mpard 1\n",
      "mpeg 1\n",
      "mqtt 1\n",
      "mr 100\n",
      "mr2 2\n",
      "mr360 1\n",
      "mrat 1\n",
      "mri 2\n",
      "mrpipelin 1\n",
      "mrt 1\n",
      "mrtouch 1\n",
      "mrv 1\n",
      "mrx 1\n",
      "ms 16\n",
      "mshop 2\n",
      "mt 14\n",
      "muav 3\n",
      "muc 1\n",
      "muddlewar 1\n",
      "mueller 1\n",
      "multi 75\n",
      "multiag 2\n",
      "multicor 1\n",
      "multidimension 5\n",
      "multidisciplinari 5\n",
      "multiform 1\n",
      "multilay 1\n",
      "multimedi 1\n",
      "multimedia 17\n",
      "multimedialen 6\n",
      "multimod 89\n",
      "multipass 1\n",
      "multipl 57\n",
      "multiplay 4\n",
      "multiprocessor 1\n",
      "multiscal 1\n",
      "multisensor 1\n",
      "multisensori 2\n",
      "multitask 2\n",
      "multithread 1\n",
      "multitouch 10\n",
      "multitud 2\n",
      "multius 1\n",
      "mundraub 1\n",
      "municip 1\n",
      "murmur 1\n",
      "muscl 6\n",
      "muscular 1\n",
      "musculoskelet 1\n",
      "muse 1\n",
      "museum 37\n",
      "music 15\n",
      "muss 1\n",
      "mutual 1\n",
      "mxed 1\n",
      "mybsc 46\n",
      "mybsccandid 8\n",
      "myown 257\n",
      "mypubl 1\n",
      "myriad 1\n",
      "myringotomi 8\n",
      "mysteri 6\n",
      "myth 3\n",
      "n95 1\n",
      "nach 2\n",
      "nachbearbeitet 1\n",
      "nachfolgend 1\n",
      "nachfolgenden 1\n",
      "nachgewiesen 1\n",
      "nachhaltigen 1\n",
      "nachhaltigkeit 1\n",
      "nachschlaglekt 2\n",
      "naegl 1\n",
      "nagyapp 1\n",
      "naheelaulrich 1\n",
      "naissanc 3\n",
      "naiv 2\n",
      "name 4\n",
      "nano 1\n",
      "napkin 1\n",
      "narr 14\n",
      "narrow 2\n",
      "narrowcast 1\n",
      "nasa 1\n",
      "nat 2\n",
      "nation 12\n",
      "nativ 4\n",
      "natur 60\n",
      "naturalist 1\n",
      "nausea 4\n",
      "naval 1\n",
      "navig 67\n",
      "navigationssoftwar 1\n",
      "navinvr 1\n",
      "nda 2\n",
      "ndd 1\n",
      "nde 2\n",
      "ndefrei 1\n",
      "nden 1\n",
      "ndern 1\n",
      "ndernder 1\n",
      "ndert 3\n",
      "ndet 1\n",
      "ndig 2\n",
      "ndige 1\n",
      "ndisch 1\n",
      "ndlich 2\n",
      "ndni 8\n",
      "ndoel 10\n",
      "ne 4\n",
      "ne0i 1\n",
      "near 8\n",
      "nearli 3\n",
      "neben 12\n",
      "necess 3\n",
      "necessarili 2\n",
      "neck 3\n",
      "need 91\n",
      "needl 2\n",
      "neg 19\n",
      "neglect 5\n",
      "neglig 1\n",
      "negot 1\n",
      "negoti 14\n",
      "nehaparikar396 1\n",
      "nehmen 4\n",
      "neighborhood 1\n",
      "neigt 1\n",
      "nen 1\n",
      "neoplast 2\n",
      "nephrectomi 1\n",
      "nerdmannsd 1\n",
      "nerv 3\n",
      "nervou 1\n",
      "nest 3\n",
      "nestor 2\n",
      "net 4\n",
      "netcollabor 1\n",
      "netherland 1\n",
      "network 43\n",
      "netz 1\n",
      "netzwerkgest 1\n",
      "netzwerktechnik 1\n",
      "neu 2\n",
      "neuartigen 1\n",
      "neue 16\n",
      "neuen 10\n",
      "neuentwicklung 1\n",
      "neuer 10\n",
      "neural 13\n",
      "neuro 1\n",
      "neurodevelop 1\n",
      "neurodevelopment 1\n",
      "neuroergonom 1\n",
      "neurofeedback 2\n",
      "neurogoggl 1\n",
      "neurolog 5\n",
      "neuron 2\n",
      "neuronavig 4\n",
      "neuroplast 2\n",
      "neuropsycholog 1\n",
      "neuropsychologist 1\n",
      "neurorehabilit 1\n",
      "neurosci 2\n",
      "neurosom 1\n",
      "neurosurg 9\n",
      "neurosurgeri 5\n",
      "neurovascular 3\n",
      "neutral 1\n",
      "nevada 2\n",
      "nevermind 1\n",
      "new 160\n",
      "newli 2\n",
      "newman 1\n",
      "newton 1\n",
      "next 9\n",
      "nextm 1\n",
      "nfc 1\n",
      "nftige 1\n",
      "nftigen 4\n",
      "nftiger 1\n",
      "nge 1\n",
      "ngenden 2\n",
      "ngernavig 2\n",
      "ngig 1\n",
      "ngigen 1\n",
      "ngigkeit 1\n",
      "nglich 1\n",
      "ngsten 2\n",
      "nicht 16\n",
      "nicotin 1\n",
      "niebl 6\n",
      "niedrig 1\n",
      "nigerian 3\n",
      "nineteen 1\n",
      "nintendo 1\n",
      "niveau 1\n",
      "nken 1\n",
      "nlich 1\n",
      "nline 1\n",
      "nnen 12\n",
      "noch 3\n",
      "node 15\n",
      "nois 3\n",
      "nokia 1\n",
      "nomad 2\n",
      "nomen 1\n",
      "non 34\n",
      "nonetheless 1\n",
      "nonlinear 3\n",
      "nonus 1\n",
      "nonverb 8\n",
      "nonweld 1\n",
      "nordstadt 1\n",
      "norm 3\n",
      "normal 16\n",
      "notabl 1\n",
      "notat 1\n",
      "note 3\n",
      "notebook 1\n",
      "notic 3\n",
      "notif 11\n",
      "notifi 1\n",
      "notion 1\n",
      "nottingham 2\n",
      "notwendig 2\n",
      "notwendigen 2\n",
      "novel 69\n",
      "novelti 1\n",
      "novemb 1\n",
      "novic 21\n",
      "novr 1\n",
      "now 11\n",
      "nowadai 3\n",
      "nowfre0 1\n",
      "nschten 1\n",
      "nsga 1\n",
      "nstiger 1\n",
      "nu 1\n",
      "nuechter76 2\n",
      "nugget 1\n",
      "null 2618\n",
      "number 38\n",
      "numer 14\n",
      "nur 5\n",
      "nurtur 1\n",
      "nutrit 8\n",
      "nutzbar 1\n",
      "nutzbarkeit 2\n",
      "nutzen 1\n",
      "nutzenden 1\n",
      "nutzer 2\n",
      "nutzerbeteiligung 1\n",
      "nutzererlebni 2\n",
      "nutzerevalu 1\n",
      "nutzergerecht 3\n",
      "nutzern 2\n",
      "nutzerpraferenz 1\n",
      "nutzt 1\n",
      "nutzung 9\n",
      "nzen 2\n",
      "nzend 1\n",
      "nzt 7\n",
      "nzung 4\n",
      "nzungen 1\n",
      "o21vr 1\n",
      "oakland 1\n",
      "ob 6\n",
      "oberdoerf 43\n",
      "oberstuf 3\n",
      "obes 8\n",
      "object 261\n",
      "objekt 17\n",
      "objekten 16\n",
      "objekterkennung 6\n",
      "obremski 1\n",
      "obscur 4\n",
      "observ 46\n",
      "obstacl 11\n",
      "obstetr 1\n",
      "obtain 21\n",
      "obtrus 1\n",
      "obviou 2\n",
      "occas 1\n",
      "occasion 1\n",
      "occlud 19\n",
      "occlus 20\n",
      "occup 3\n",
      "occupi 2\n",
      "occur 15\n",
      "oclc 1\n",
      "oct 1\n",
      "octob 3\n",
      "octopu 1\n",
      "octre 4\n",
      "ocular 1\n",
      "oculock 1\n",
      "oculomotor 1\n",
      "oculu 10\n",
      "oder 15\n",
      "odour 1\n",
      "oer 1\n",
      "ofcontrol 1\n",
      "offen 6\n",
      "offenen 6\n",
      "offer 44\n",
      "offic 5\n",
      "offlin 1\n",
      "offload 2\n",
      "ofn 1\n",
      "ofsimul 1\n",
      "oft 1\n",
      "oftentim 2\n",
      "ographiqu 1\n",
      "ohio 1\n",
      "oil 3\n",
      "oiv 2\n",
      "okonjo 1\n",
      "okret 2\n",
      "old 4\n",
      "older 4\n",
      "oldeslo 5\n",
      "olfact 1\n",
      "olocalis 1\n",
      "ologi 1\n",
      "olwal 1\n",
      "olymp 3\n",
      "olympia 1\n",
      "omar 3\n",
      "omit 1\n",
      "omk06 2\n",
      "omni 2\n",
      "omnidirect 3\n",
      "omnidirektional 1\n",
      "on 4\n",
      "onarbe09i 1\n",
      "onaugmei 1\n",
      "onc 6\n",
      "ongo 5\n",
      "onli 53\n",
      "onlin 37\n",
      "ontolog 5\n",
      "opac 1\n",
      "open 79\n",
      "opengl 2\n",
      "openli 1\n",
      "openmask 2\n",
      "oper 51\n",
      "opera 3\n",
      "operationssa 1\n",
      "ophthalmoscop 1\n",
      "opinion 8\n",
      "opportun 37\n",
      "oppos 1\n",
      "optic 29\n",
      "optim 17\n",
      "optimis 3\n",
      "option 14\n",
      "oral 2\n",
      "orbit 1\n",
      "orchestr 4\n",
      "order 49\n",
      "ordinari 4\n",
      "org 3152\n",
      "organ 15\n",
      "organis 1\n",
      "organisiert 1\n",
      "organiz 3\n",
      "ori 2\n",
      "orient 31\n",
      "orientiert 2\n",
      "origami 1\n",
      "origin 20\n",
      "ort 4\n",
      "orthogon 2\n",
      "orthopaed 1\n",
      "orthoped 2\n",
      "orthot 1\n",
      "ortung 3\n",
      "osat 3\n",
      "osawaru 1\n",
      "oscarswelt 1\n",
      "oseretski 1\n",
      "osgartoolkit 1\n",
      "ost 16\n",
      "ostens 1\n",
      "osteotomi 8\n",
      "osu 1\n",
      "otolaryngolog 11\n",
      "otolaryngologist 1\n",
      "otolog 4\n",
      "otologist 1\n",
      "otorhinolaryngolog 1\n",
      "otss 15\n",
      "oup 2\n",
      "outag 1\n",
      "outcom 43\n",
      "outdoor 32\n",
      "outlier 2\n",
      "outlin 16\n",
      "outlook 2\n",
      "outpac 2\n",
      "outperform 5\n",
      "output 11\n",
      "outreach 3\n",
      "outsid 2\n",
      "outweigh 1\n",
      "overarch 3\n",
      "overcom 13\n",
      "overflow 1\n",
      "overhead 1\n",
      "overlai 20\n",
      "overlaid 5\n",
      "overlap 1\n",
      "overload 1\n",
      "overlook 1\n",
      "overshadow 1\n",
      "overt 2\n",
      "overview 28\n",
      "overwhelm 1\n",
      "ovgu 2\n",
      "owl 2\n",
      "owner 1\n",
      "ownership 16\n",
      "oxygen 1\n",
      "oz 2\n",
      "p2p 1\n",
      "pac 1\n",
      "pacif 1\n",
      "pack 2\n",
      "packag 7\n",
      "pad 3\n",
      "page 4\n",
      "paid 2\n",
      "pain 14\n",
      "paint 5\n",
      "pair 1\n",
      "palm 1\n",
      "palmtop 1\n",
      "palsi 21\n",
      "pamphlet 1\n",
      "panacea 1\n",
      "pancreat 1\n",
      "pandora 1\n",
      "panel 13\n",
      "panic 1\n",
      "panoram 5\n",
      "panorama 4\n",
      "papakonstantin 2\n",
      "papar 1\n",
      "paper 291\n",
      "paperdud 1\n",
      "paperspac 1\n",
      "papieranleitung 1\n",
      "papierform 1\n",
      "par 2\n",
      "paradebeispiel 1\n",
      "paradigm 41\n",
      "paradigmen 1\n",
      "paradox 1\n",
      "parahippocamp 1\n",
      "parallax 14\n",
      "parallel 3\n",
      "parallelen 1\n",
      "paramed 3\n",
      "paramet 12\n",
      "parameter 1\n",
      "parameteriz 2\n",
      "parametr 2\n",
      "paranas 1\n",
      "paraprofession 1\n",
      "parasit 2\n",
      "parat 1\n",
      "paravr 1\n",
      "parent 1\n",
      "pareto 4\n",
      "pariet 1\n",
      "parieto 1\n",
      "park 1\n",
      "parkinson 2\n",
      "parmar 2\n",
      "part 48\n",
      "parti 6\n",
      "partial 10\n",
      "particip 230\n",
      "participatori 2\n",
      "particulari 2\n",
      "partizip 1\n",
      "partizipativ 2\n",
      "partizipativen 1\n",
      "partli 9\n",
      "partner 3\n",
      "partnership 1\n",
      "pass 1\n",
      "passag 16\n",
      "passeng 2\n",
      "passiv 11\n",
      "password 1\n",
      "past 14\n",
      "pat 1\n",
      "patch 1\n",
      "patent 4\n",
      "path 15\n",
      "pathfind 3\n",
      "patholog 7\n",
      "patient 94\n",
      "patron 3\n",
      "pattern 25\n",
      "pbi 2\n",
      "pc 12\n",
      "pda 3\n",
      "pdf 9\n",
      "pdfcopi 1\n",
      "peak 1\n",
      "peca 1\n",
      "pedagog 22\n",
      "pedagogi 5\n",
      "pedal 1\n",
      "pedestrian 1\n",
      "pediatr 3\n",
      "peephol 1\n",
      "peer 25\n",
      "peg 1\n",
      "pegboard 1\n",
      "pen 5\n",
      "pendent 1\n",
      "peopl 39\n",
      "per 1\n",
      "perceiv 70\n",
      "perceivedstress 1\n",
      "percent 6\n",
      "percentag 1\n",
      "percept 193\n",
      "perceptu 24\n",
      "perfect 3\n",
      "perfekt 1\n",
      "perform 210\n",
      "pergames2006 2\n",
      "perienc 2\n",
      "period 9\n",
      "periodont 1\n",
      "periperson 1\n",
      "peripher 11\n",
      "perman 2\n",
      "permanenten 1\n",
      "permeat 2\n",
      "permiss 1\n",
      "permit 1\n",
      "persist 10\n",
      "person 91\n",
      "personalfluktu 1\n",
      "personen 2\n",
      "personnel 4\n",
      "perspect 55\n",
      "perspekt 5\n",
      "perspektiv 3\n",
      "perspektiven 2\n",
      "perspektivisch 1\n",
      "persuas 3\n",
      "perturb 2\n",
      "pervas 14\n",
      "pet 4\n",
      "peterkullmann 1\n",
      "pfeil 3\n",
      "pft 2\n",
      "pfung 1\n",
      "pfungskett 1\n",
      "pgy 2\n",
      "ph 1\n",
      "phacoemulsif 1\n",
      "phantom 2\n",
      "pharmacolog 4\n",
      "phase 10\n",
      "phasen 2\n",
      "phenomena 22\n",
      "phenomenon 8\n",
      "philosoph 1\n",
      "philosophi 3\n",
      "phobia 4\n",
      "phone 35\n",
      "photo 5\n",
      "photogeist 1\n",
      "photogrammetr 1\n",
      "photogrammetri 6\n",
      "photograph 5\n",
      "photographi 4\n",
      "photometr 1\n",
      "photon 3\n",
      "photonav 2\n",
      "photorealist 37\n",
      "php 2\n",
      "physic 170\n",
      "physician 1\n",
      "physio 1\n",
      "physiolog 17\n",
      "physiotherapi 2\n",
      "physiotherapist 1\n",
      "physiqu 1\n",
      "physolog 1\n",
      "pi3 2\n",
      "piano 3\n",
      "pick 2\n",
      "pico 1\n",
      "pictori 2\n",
      "pictur 3\n",
      "piktogrammen 6\n",
      "pile 2\n",
      "pileu 1\n",
      "pillar 1\n",
      "pillowvr 1\n",
      "pilot 36\n",
      "pilotstudi 12\n",
      "pilotuntersuchung 6\n",
      "pinch 1\n",
      "pinpoint 3\n",
      "pintar 2\n",
      "pioneer 1\n",
      "pipelin 7\n",
      "pivot 1\n",
      "pixel 5\n",
      "pkullmann 1\n",
      "pl 1\n",
      "plabst 1\n",
      "place 39\n",
      "placement 8\n",
      "plai 74\n",
      "plan 61\n",
      "planar 1\n",
      "plane 8\n",
      "planen 1\n",
      "planet 2\n",
      "plann 1\n",
      "planner 6\n",
      "plant 6\n",
      "planung 3\n",
      "planungsprozess 1\n",
      "planungsstand 1\n",
      "planungsszenario 1\n",
      "plastic 5\n",
      "plasticit 1\n",
      "plastisch 1\n",
      "plateau 2\n",
      "platform 79\n",
      "plato 1\n",
      "platon 3\n",
      "platoon 1\n",
      "plattformen 2\n",
      "platziert 1\n",
      "plausibl 40\n",
      "play 18\n",
      "playabl 1\n",
      "player 28\n",
      "playground 10\n",
      "playlist 1\n",
      "playth 2\n",
      "ple 2\n",
      "pleasur 3\n",
      "plotter 1\n",
      "plu 6\n",
      "plug 2\n",
      "pmem 1\n",
      "pneumat 1\n",
      "pocket 1\n",
      "point 79\n",
      "pois 4\n",
      "pok 1\n",
      "polar 1\n",
      "poli 6\n",
      "polit 3\n",
      "polygon 6\n",
      "polyhedron 2\n",
      "polyimid 1\n",
      "polynomi 2\n",
      "pond 1\n",
      "pons 1\n",
      "poor 2\n",
      "poorli 5\n",
      "pop 3\n",
      "popul 13\n",
      "popular 11\n",
      "porta 2\n",
      "portabl 8\n",
      "portal 1\n",
      "portfolio 2\n",
      "portion 7\n",
      "pose 23\n",
      "posemmr 1\n",
      "posit 78\n",
      "positionieren 1\n",
      "positionierung 1\n",
      "positiv 7\n",
      "possess 2\n",
      "possibl 73\n",
      "possit 1\n",
      "post 24\n",
      "poster 15\n",
      "postgradu 1\n",
      "postop 1\n",
      "postsurg 1\n",
      "posttest 1\n",
      "posttraumat 1\n",
      "postul 4\n",
      "postur 7\n",
      "posturographi 1\n",
      "potanti 1\n",
      "pote6te2 1\n",
      "potent 1\n",
      "potenti 136\n",
      "potential 9\n",
      "potentiel 1\n",
      "potentiellen 1\n",
      "potenzi 10\n",
      "potenzial 1\n",
      "potenziel 2\n",
      "potenziellen 1\n",
      "pound 1\n",
      "pour 4\n",
      "power 22\n",
      "powerbal 1\n",
      "pp138 1\n",
      "pr 11\n",
      "practic 86\n",
      "practis 1\n",
      "practition 5\n",
      "praktiken 1\n",
      "praktisch 2\n",
      "praktischen 4\n",
      "pratibha 3\n",
      "praxi 2\n",
      "praxisbezug 1\n",
      "praxishandbuch 1\n",
      "praxisnah 2\n",
      "praxistauglichkeit 1\n",
      "prayer 1\n",
      "pre 13\n",
      "pre9667fiz 1\n",
      "preach 1\n",
      "precis 22\n",
      "precomput 3\n",
      "predefin 3\n",
      "predict 25\n",
      "predomin 1\n",
      "predominantli 5\n",
      "prefabr 2\n",
      "prefer 21\n",
      "pregnanc 1\n",
      "prei 1\n",
      "prejudic 1\n",
      "preliminari 11\n",
      "preliminarili 1\n",
      "premier 2\n",
      "preoper 1\n",
      "prepar 10\n",
      "prepared 1\n",
      "preprint 1\n",
      "preschool 1\n",
      "presenc 114\n",
      "present 320\n",
      "preserv 7\n",
      "press 2\n",
      "pressur 2\n",
      "presum 1\n",
      "pretend 1\n",
      "pretest 1\n",
      "preval 1\n",
      "prevent 10\n",
      "preview 4\n",
      "previou 32\n",
      "previous 11\n",
      "price 2\n",
      "pride 1\n",
      "prima 3\n",
      "primari 16\n",
      "primarili 9\n",
      "primer 2\n",
      "princip 3\n",
      "principl 21\n",
      "print 16\n",
      "printer 2\n",
      "prinzipien 1\n",
      "prior 6\n",
      "privaci 24\n",
      "privat 13\n",
      "privileg 2\n",
      "priya 2\n",
      "proactiv 1\n",
      "probanden 3\n",
      "probe 1\n",
      "problem 48\n",
      "problemat 4\n",
      "problemen 1\n",
      "problemlagen 6\n",
      "procedur 44\n",
      "proceed 8\n",
      "process 170\n",
      "produc 14\n",
      "product 66\n",
      "productdesign 1\n",
      "produkt 26\n",
      "produktentstehung 2\n",
      "produktentwicklung 1\n",
      "produktionsanlag 1\n",
      "produktionsbereich 2\n",
      "produktionskett 1\n",
      "produktionsmaschinen 1\n",
      "produktionsprozess 1\n",
      "produktlebenszyklu 1\n",
      "produktmanag 1\n",
      "profession 23\n",
      "profici 3\n",
      "profil 6\n",
      "prog 3\n",
      "program 68\n",
      "programm 6\n",
      "progress 22\n",
      "prohibit 1\n",
      "project 116\n",
      "projector 11\n",
      "projekt 6\n",
      "projekten 1\n",
      "projektideen 1\n",
      "projektieren 1\n",
      "projektpartnern 1\n",
      "projektphas 1\n",
      "promin 4\n",
      "promis 46\n",
      "promot 12\n",
      "prone 2\n",
      "proof 5\n",
      "prop 4\n",
      "propag 4\n",
      "proper 2\n",
      "properi 1\n",
      "properti 23\n",
      "proport 3\n",
      "propos 92\n",
      "propriocept 4\n",
      "prosa 2\n",
      "prosoci 1\n",
      "prospect 15\n",
      "prosthes 2\n",
      "prosthesi 1\n",
      "prosthet 3\n",
      "protecitv 1\n",
      "protect 10\n",
      "proteu 24\n",
      "protocol 11\n",
      "prototyp 99\n",
      "prototypen 1\n",
      "prototypenbasiert 3\n",
      "prototypentwicklung 1\n",
      "prototypisch 3\n",
      "prototypischen 2\n",
      "prove 12\n",
      "proven 3\n",
      "provid 1\n",
      "provok 5\n",
      "proxem 1\n",
      "proxi 1\n",
      "proxim 1\n",
      "prozess 7\n",
      "prozessen 1\n",
      "prozessinformationen 2\n",
      "prozessoren 1\n",
      "pseudo 1\n",
      "psychiatr 2\n",
      "psychiatri 1\n",
      "psycho 3\n",
      "psycholog 32\n",
      "psychometr 6\n",
      "psychomotor 2\n",
      "psychopatholog 1\n",
      "psychophys 4\n",
      "psychosi 1\n",
      "psychosoci 1\n",
      "psychotherapeut 2\n",
      "psychotherapi 3\n",
      "psycinfo 1\n",
      "ptsd 4\n",
      "pub 1\n",
      "public 70\n",
      "publicli 2\n",
      "publish 30\n",
      "pubm 5\n",
      "punctuat 2\n",
      "punctur 1\n",
      "punithakumar 1\n",
      "purchas 3\n",
      "pure 5\n",
      "purpos 25\n",
      "pursu 2\n",
      "push 4\n",
      "put 4\n",
      "pvq 3\n",
      "pwm 2\n",
      "qoe 1\n",
      "qr 3\n",
      "quadrocopt 1\n",
      "quakerunn 1\n",
      "qual 1\n",
      "qualifi 1\n",
      "qualifik 1\n",
      "qualit 23\n",
      "qualitativ 2\n",
      "qualitativen 1\n",
      "qualiti 55\n",
      "quantifi 1\n",
      "quantit 12\n",
      "quantitativen 2\n",
      "quantum 1\n",
      "quasi 3\n",
      "quaternion 2\n",
      "quellen 1\n",
      "queri 1\n",
      "querschnitt 1\n",
      "quest 5\n",
      "question 27\n",
      "questionnair 17\n",
      "quick 2\n",
      "quickli 3\n",
      "quiet 1\n",
      "quilt 1\n",
      "quir 1\n",
      "quiz 1\n",
      "race 3\n",
      "racial 4\n",
      "rad 1\n",
      "radar 4\n",
      "radial 1\n",
      "radianc 2\n",
      "radiat 1\n",
      "radic 2\n",
      "radio 1\n",
      "radiofrequ 1\n",
      "radiograph 1\n",
      "radiolog 1\n",
      "radios 2\n",
      "radiotherapi 2\n",
      "rahmen 10\n",
      "rahmenbedingungen 1\n",
      "rahmensystem 2\n",
      "rai 25\n",
      "rais 6\n",
      "ral 3\n",
      "ran 1\n",
      "random 23\n",
      "randomis 3\n",
      "randomli 3\n",
      "rang 47\n",
      "rank 2\n",
      "rapid 15\n",
      "rapidli 13\n",
      "rapidprototyp 1\n",
      "rare 1\n",
      "raskar 2\n",
      "raspberrykid 3\n",
      "raster 1\n",
      "rat 1\n",
      "rate 68\n",
      "rater 2\n",
      "rational 2\n",
      "raum 2\n",
      "raw 2\n",
      "raycast 1\n",
      "rct 5\n",
      "rdern 1\n",
      "rderten 2\n",
      "rderung 1\n",
      "re06 1\n",
      "re0939 1\n",
      "re1t 1\n",
      "re6i93 1\n",
      "re9667fiz 1\n",
      "re9797 1\n",
      "reach 11\n",
      "react 2\n",
      "reaction 5\n",
      "reactiv 5\n",
      "read 17\n",
      "reader 5\n",
      "readi 10\n",
      "readili 1\n",
      "real 346\n",
      "realen 12\n",
      "realia 1\n",
      "realidad 1\n",
      "realign 1\n",
      "realisieren 3\n",
      "realisiert 1\n",
      "realisierung 1\n",
      "realisierungen 1\n",
      "realism 23\n",
      "realist 44\n",
      "realisticwat 1\n",
      "realistisch 1\n",
      "realistischen 1\n",
      "realit 36\n",
      "realiti 4213\n",
      "realityappl 2\n",
      "realiz 34\n",
      "reallifeengin 1\n",
      "realm 6\n",
      "realtim 26\n",
      "realworld 3\n",
      "reason 11\n",
      "rebecca 9\n",
      "rebeccahein 3\n",
      "recabarren 1\n",
      "recal 1\n",
      "receiv 23\n",
      "recent 76\n",
      "recept 1\n",
      "receptor 2\n",
      "rechentechnik 1\n",
      "rechner 2\n",
      "rechnergenerierten 1\n",
      "recip 1\n",
      "reciproc 1\n",
      "recogn 3\n",
      "recognit 16\n",
      "recommend 25\n",
      "reconcil 2\n",
      "reconcili 2\n",
      "reconfigur 2\n",
      "reconnaiss 1\n",
      "reconstruct 25\n",
      "record 20\n",
      "recov 2\n",
      "recoveri 1\n",
      "recreat 13\n",
      "recruit 7\n",
      "recsys2020 1\n",
      "rectangular 2\n",
      "rectifi 1\n",
      "recurs 1\n",
      "recycl 9\n",
      "red 4\n",
      "redesign 4\n",
      "redgraph 3\n",
      "redirect 1\n",
      "redraw 1\n",
      "reduc 26\n",
      "reduct 10\n",
      "redukt 1\n",
      "redund 2\n",
      "redux 1\n",
      "refer 49\n",
      "refere 2\n",
      "referenc 1\n",
      "referencia 1\n",
      "referenti 2\n",
      "refin 2\n",
      "refineri 3\n",
      "reflect 63\n",
      "reflectedr 4\n",
      "reflectometri 1\n",
      "reflektiert 6\n",
      "refocus 1\n",
      "reform 1\n",
      "refract 1\n",
      "refresh 2\n",
      "refus 1\n",
      "reg 1\n",
      "regard 22\n",
      "regardless 5\n",
      "regelm 1\n",
      "regen 1\n",
      "region 7\n",
      "regist 6\n",
      "registr 26\n",
      "registrar 1\n",
      "regress 2\n",
      "regul 3\n",
      "regularli 2\n",
      "regulatori 1\n",
      "regulski 10\n",
      "rehabilit 52\n",
      "rehavr 1\n",
      "rehears 3\n",
      "rei 1\n",
      "reichen 1\n",
      "reichweit 1\n",
      "reihenfolg 2\n",
      "reinen 1\n",
      "reinforc 1\n",
      "reinhardt 2\n",
      "reisch 1\n",
      "reiscop 1\n",
      "reiseassistenzsystem 2\n",
      "reitmayr 1\n",
      "rekha 1\n",
      "rekonstruiert 1\n",
      "rel 22\n",
      "relaps 2\n",
      "relat 16\n",
      "related 2\n",
      "relationship 45\n",
      "relativ 2\n",
      "relativist 1\n",
      "relax 1\n",
      "releas 1\n",
      "relev 31\n",
      "relevant 1\n",
      "relevanten 1\n",
      "relevanz 1\n",
      "reli 12\n",
      "reliabl 21\n",
      "relief 1\n",
      "relight 1\n",
      "remain 26\n",
      "remark 1\n",
      "remedi 1\n",
      "remix 1\n",
      "remlabnet 2\n",
      "remodel 1\n",
      "remot 52\n",
      "remov 10\n",
      "ren 7\n",
      "renaiss 2\n",
      "renal 1\n",
      "render 92\n",
      "renderloop 2\n",
      "renderman 1\n",
      "renew 3\n",
      "reno 2\n",
      "renukasrinidhi 1\n",
      "reorgan 2\n",
      "repair 12\n",
      "reparaturprozeduren 1\n",
      "repariert 1\n",
      "repeat 5\n",
      "repeatedli 5\n",
      "repetit 7\n",
      "repetitiven 1\n",
      "replac 10\n",
      "replic 10\n",
      "replica 4\n",
      "report 55\n",
      "reportedli 1\n",
      "repositori 2\n",
      "repres 25\n",
      "represeant 1\n",
      "represent 71\n",
      "reproduc 3\n",
      "reproduct 6\n",
      "reproject 9\n",
      "republ 1\n",
      "repurpos 2\n",
      "request 1\n",
      "requir 104\n",
      "rescu 1\n",
      "research 267\n",
      "researchpark 7\n",
      "resembl 5\n",
      "reservoir 1\n",
      "reshap 2\n",
      "resid 15\n",
      "resist 4\n",
      "resiz 5\n",
      "resolut 11\n",
      "resolv 3\n",
      "reson 4\n",
      "resourc 8\n",
      "respect 26\n",
      "respiratori 1\n",
      "respond 5\n",
      "respons 26\n",
      "ressourcen 1\n",
      "rest 7\n",
      "restrain 1\n",
      "restrict 9\n",
      "result 206\n",
      "resultiert 1\n",
      "resultsof 2\n",
      "resuscit 4\n",
      "resynthesi 1\n",
      "retail 5\n",
      "retarget 1\n",
      "retent 2\n",
      "rethink 3\n",
      "retin 12\n",
      "retina 4\n",
      "retriev 7\n",
      "return 1\n",
      "reus 7\n",
      "reusabl 10\n",
      "reveal 52\n",
      "reveri 1\n",
      "revert 3\n",
      "review 86\n",
      "revisit 2\n",
      "reviv 3\n",
      "revman 1\n",
      "revolut 2\n",
      "revolution 2\n",
      "revolv 1\n",
      "rfe 3\n",
      "rfer 1\n",
      "rfid 5\n",
      "rfni 1\n",
      "rgb 4\n",
      "rgbd 3\n",
      "rhein 9\n",
      "rhinoplasti 1\n",
      "ri 14\n",
      "rich 8\n",
      "richer 3\n",
      "richli 2\n",
      "richterek 1\n",
      "richteten 1\n",
      "richtig 1\n",
      "riedel 1\n",
      "riedmann 2\n",
      "rift 8\n",
      "rigen 1\n",
      "right 9\n",
      "rigidli 1\n",
      "ring 2\n",
      "ripe 1\n",
      "rise 5\n",
      "risk 32\n",
      "ritu 1\n",
      "rium 1\n",
      "rken 1\n",
      "rkten 1\n",
      "rkung 1\n",
      "rlichen 2\n",
      "rnesselrath 1\n",
      "road 3\n",
      "roadmap 1\n",
      "robocup 1\n",
      "robot 107\n",
      "roboterunterst 1\n",
      "robust 6\n",
      "rock 1\n",
      "rockstar 1\n",
      "rodent 4\n",
      "roh07 2\n",
      "rohitg 1\n",
      "role 51\n",
      "roll 2\n",
      "rollenbasiert 1\n",
      "rollenbrettspiel 1\n",
      "rollenspiel 8\n",
      "rollenspielbasierten 6\n",
      "room 22\n",
      "root 1\n",
      "rotat 2\n",
      "roth 1\n",
      "rothnrol 6\n",
      "rough 1\n",
      "roughli 2\n",
      "round 5\n",
      "rout 8\n",
      "routenplanung 3\n",
      "routin 9\n",
      "routinen 1\n",
      "row 2\n",
      "rowman 1\n",
      "royal 2\n",
      "rper 1\n",
      "rst 2\n",
      "rster 1\n",
      "rt 8\n",
      "rtechnik 1\n",
      "rterung 1\n",
      "rtig 1\n",
      "rtlicher 1\n",
      "rtlx 1\n",
      "rtmp 1\n",
      "rub 1\n",
      "rubber 1\n",
      "rubik 1\n",
      "rudimentarili 1\n",
      "rule 8\n",
      "run 28\n",
      "rund 1\n",
      "rv 2\n",
      "rvika 1\n",
      "rw 9\n",
      "rwoz 3\n",
      "rwth 1\n",
      "rzburg 2\n",
      "saber 1\n",
      "saccad 2\n",
      "sach 1\n",
      "safe 6\n",
      "safeguard 2\n",
      "safemr 1\n",
      "safeti 14\n",
      "sagen 1\n",
      "sagitt 5\n",
      "sailboat 1\n",
      "sale 1\n",
      "salienc 2\n",
      "salpingectomi 4\n",
      "salpingotomi 3\n",
      "sammelband 1\n",
      "sammelbegriff 1\n",
      "sammlung 3\n",
      "sampl 26\n",
      "samueltruman 2\n",
      "sannwald 1\n",
      "sapna 2\n",
      "sar 1\n",
      "sarc 1\n",
      "sascha 1\n",
      "satellit 2\n",
      "satisfact 6\n",
      "satisfi 1\n",
      "sau 1\n",
      "save 2\n",
      "savvi 4\n",
      "saw 9\n",
      "scaffold 7\n",
      "scala 1\n",
      "scalabl 18\n",
      "scale 47\n",
      "scan 16\n",
      "scanner 2\n",
      "scarc 2\n",
      "scariest 2\n",
      "scatter 1\n",
      "scenario 67\n",
      "scenarioa 1\n",
      "scene 65\n",
      "scenecontext 2\n",
      "scenectrl 1\n",
      "scenegraph 2\n",
      "sceneri 1\n",
      "sceve 1\n",
      "schaffen 3\n",
      "schaper 2\n",
      "schedul 2\n",
      "scheint 1\n",
      "schell 1\n",
      "schema 1\n",
      "scheme 5\n",
      "schiffsbau 1\n",
      "schizophrenia 4\n",
      "schl 1\n",
      "schlagwort 1\n",
      "schlie 2\n",
      "schliesslich 1\n",
      "schmerzbehandlung 1\n",
      "schmerzen 1\n",
      "schmerzmedikamenten 1\n",
      "schmerztherapi 1\n",
      "schmiegen 1\n",
      "schnell 1\n",
      "schneller 1\n",
      "schnittstel 6\n",
      "scholar 8\n",
      "schon 2\n",
      "school 32\n",
      "schooler 1\n",
      "schr 2\n",
      "schreiben 1\n",
      "schritt 3\n",
      "schrittweis 1\n",
      "schrittzahl 1\n",
      "schule 2\n",
      "schultheiss 1\n",
      "schulunterricht 1\n",
      "schw 1\n",
      "schwei 5\n",
      "schwerpunkt 1\n",
      "scienc 35\n",
      "scientif 42\n",
      "scientist 6\n",
      "scive 1\n",
      "sclerosi 5\n",
      "scope 8\n",
      "scopu 1\n",
      "score 35\n",
      "scorpiodrom 1\n",
      "scratch 1\n",
      "screen 23\n",
      "screenshot 2\n",
      "scroll 4\n",
      "sculpt 1\n",
      "sculptar 1\n",
      "sculptur 3\n",
      "sd 13\n",
      "sd06 2\n",
      "sdar 1\n",
      "sdt 37\n",
      "se 3\n",
      "se9t 1\n",
      "sea 2\n",
      "seamless 10\n",
      "seamlessli 9\n",
      "search 13\n",
      "seari 19\n",
      "season 1\n",
      "seat 5\n",
      "second 36\n",
      "secondari 9\n",
      "secret 2\n",
      "section 21\n",
      "sector 8\n",
      "secur 12\n",
      "see 54\n",
      "seek 13\n",
      "segment 19\n",
      "sehen 1\n",
      "sehr 2\n",
      "sein 5\n",
      "seinem 1\n",
      "seismic 1\n",
      "seit 1\n",
      "seiten 1\n",
      "sektion 1\n",
      "sektor 1\n",
      "selbst 4\n",
      "selbstbeschreibungsf 1\n",
      "selbststudium 2\n",
      "selbstverst 2\n",
      "select 58\n",
      "self 65\n",
      "selfiewal 1\n",
      "selfmad 1\n",
      "selv 4\n",
      "semant 47\n",
      "semanticweb 1\n",
      "semi 26\n",
      "semiimmers 1\n",
      "seminar 15\n",
      "seminarkonzept 3\n",
      "semiot 2\n",
      "semitranspar 3\n",
      "sen 2\n",
      "send 1\n",
      "senior 2\n",
      "sens 74\n",
      "sensat 11\n",
      "sensit 8\n",
      "sensor 54\n",
      "sensori 20\n",
      "sensorimotor 1\n",
      "sensornetwork 1\n",
      "sentenc 2\n",
      "sentient 1\n",
      "sentiert 4\n",
      "sepa 2\n",
      "separ 19\n",
      "septemb 2\n",
      "sequenc 4\n",
      "sequenti 1\n",
      "ser 1\n",
      "serendipit 1\n",
      "seri 3\n",
      "serv 16\n",
      "server 7\n",
      "servic 113\n",
      "servicebereich 1\n",
      "servicef 1\n",
      "servicio 1\n",
      "servo 8\n",
      "session 34\n",
      "set 72\n",
      "setup 38\n",
      "seu 2\n",
      "seventh 2\n",
      "seventi 2\n",
      "sever 4\n",
      "sew 2\n",
      "sex 1\n",
      "sexual 2\n",
      "sfm 4\n",
      "sgrafe 1\n",
      "shade 8\n",
      "shader 1\n",
      "shadow 24\n",
      "shape 30\n",
      "share 46\n",
      "sharetti0fizt3 1\n",
      "sharpen 2\n",
      "shed 2\n",
      "sheep 1\n",
      "shelf 15\n",
      "shift 11\n",
      "shini 1\n",
      "ship 2\n",
      "shivastava 1\n",
      "shoe 1\n",
      "shoot 1\n",
      "shooter 1\n",
      "shop 9\n",
      "shopper 3\n",
      "short 26\n",
      "shortcom 1\n",
      "shorter 2\n",
      "shortest 1\n",
      "shot 2\n",
      "shoulder 2\n",
      "show 97\n",
      "showcas 4\n",
      "shown 22\n",
      "shvil 1\n",
      "sia 11\n",
      "sic 1\n",
      "sich 30\n",
      "sichergestellt 1\n",
      "sichert 1\n",
      "sicherung 1\n",
      "sicherzustellen 1\n",
      "sicht 3\n",
      "sichtbar 1\n",
      "sichtfeld 4\n",
      "sichtweis 1\n",
      "sick 20\n",
      "side 18\n",
      "sidebar 1\n",
      "sie 7\n",
      "siebdruckwerkst 1\n",
      "sig 1\n",
      "siggraph 3\n",
      "sight 2\n",
      "siginific 1\n",
      "sigma 2\n",
      "sigmoid 11\n",
      "sign 1\n",
      "signal 16\n",
      "signatur 1\n",
      "signific 93\n",
      "significantli 73\n",
      "signifik 1\n",
      "signifikant 2\n",
      "signifikanten 1\n",
      "silico 1\n",
      "sim 12\n",
      "similar 18\n",
      "similarli 1\n",
      "simpl 10\n",
      "simpli 10\n",
      "simplic 6\n",
      "simplifi 4\n",
      "simplist 1\n",
      "simul 305\n",
      "simultan 11\n",
      "simx 6\n",
      "sind 23\n",
      "singh 1\n",
      "singl 23\n",
      "sinitha 2\n",
      "sinken 1\n",
      "sinn 1\n",
      "sino 1\n",
      "sinodur 1\n",
      "sinu 8\n",
      "sinus 1\n",
      "siri 1\n",
      "site 10\n",
      "situ 1\n",
      "situat 31\n",
      "situationsabh 1\n",
      "situationsgerecht 2\n",
      "sixteen 1\n",
      "sixth 3\n",
      "sixti 2\n",
      "size 31\n",
      "sjtuelearninglab 1\n",
      "skalierbar 2\n",
      "skasei 1\n",
      "skbulu 1\n",
      "skelet 1\n",
      "sketch 21\n",
      "sketchup 1\n",
      "skill 82\n",
      "skin 7\n",
      "skirmish 1\n",
      "skyfar 2\n",
      "slam 4\n",
      "slegroux 2\n",
      "slice 1\n",
      "slide 1\n",
      "slightli 3\n",
      "slot 10\n",
      "slow 3\n",
      "slower 1\n",
      "sma 1\n",
      "small 16\n",
      "smallab 1\n",
      "smaller 2\n",
      "smallest 2\n",
      "smarpro 1\n",
      "smart 42\n",
      "smartbox 6\n",
      "smarter 1\n",
      "smartfactori 1\n",
      "smarthom 1\n",
      "smartphon 17\n",
      "smartwatch 1\n",
      "smc 2\n",
      "smell 1\n",
      "smoke 1\n",
      "snap 1\n",
      "snap2plai 3\n",
      "snapshot 4\n",
      "snauth 1\n",
      "sne 4\n",
      "soap 1\n",
      "soccer 3\n",
      "social 165\n",
      "societ 2\n",
      "societi 8\n",
      "socio 10\n",
      "socioeconom 1\n",
      "sociomateri 1\n",
      "sod 5\n",
      "sofi 1\n",
      "soft 6\n",
      "softwar 113\n",
      "softwaretechnischen 1\n",
      "sol 1\n",
      "solch 1\n",
      "solid 11\n",
      "soll 5\n",
      "sollen 2\n",
      "sollt 2\n",
      "sollten 1\n",
      "solut 56\n",
      "solutionsfor 1\n",
      "solv 5\n",
      "somatosensori 1\n",
      "somit 3\n",
      "sommer 1\n",
      "sondern 4\n",
      "songbook 1\n",
      "soni 2\n",
      "sonycsl 1\n",
      "soon 2\n",
      "soorajkbabu 1\n",
      "sophist 1\n",
      "sophrologist 1\n",
      "sou 1\n",
      "sound 18\n",
      "soundscap 1\n",
      "sourc 40\n",
      "sourcebook 3\n",
      "soweit 1\n",
      "sowi 24\n",
      "sowohl 10\n",
      "sozial 9\n",
      "sozialen 5\n",
      "sp 18\n",
      "space 160\n",
      "spacedesign 1\n",
      "span 6\n",
      "spars 3\n",
      "spastic 1\n",
      "spatial 97\n",
      "spatio 3\n",
      "spatiotempor 2\n",
      "speak 8\n",
      "speaker 6\n",
      "spearman 1\n",
      "speci 4\n",
      "special 50\n",
      "specialis 2\n",
      "specialist 1\n",
      "specif 77\n",
      "specifi 3\n",
      "spectat 8\n",
      "spectral 1\n",
      "spectrum 4\n",
      "specul 1\n",
      "specular 6\n",
      "speech 12\n",
      "speed 21\n",
      "spektrum 1\n",
      "spent 4\n",
      "spezifik 1\n",
      "spezifisch 2\n",
      "spheric 3\n",
      "sphero 1\n",
      "spiel 6\n",
      "spielelement 6\n",
      "spielfiguren 6\n",
      "spielgeschehen 6\n",
      "spielt 1\n",
      "spielumgebung 6\n",
      "spielzeug 1\n",
      "spike 6\n",
      "spin 1\n",
      "spinal 2\n",
      "spite 1\n",
      "splice 1\n",
      "spline 1\n",
      "split 11\n",
      "splitter 1\n",
      "sponsor 1\n",
      "sporad 1\n",
      "sport 13\n",
      "sprachinterakt 1\n",
      "spread 1\n",
      "springen 1\n",
      "springer 11\n",
      "sprinkl 1\n",
      "squar 1\n",
      "sr 6\n",
      "srg 2\n",
      "sseltechnologi 1\n",
      "sselung 1\n",
      "ssen 3\n",
      "ssr 9\n",
      "ssten 1\n",
      "ssvep 1\n",
      "st 8\n",
      "sta 1\n",
      "staat 2\n",
      "stabl 3\n",
      "stack 3\n",
      "stadium 1\n",
      "stadt 1\n",
      "stadtbibliothek 4\n",
      "stadtentwicklung 3\n",
      "stadtmodel 1\n",
      "stadtmodellen 7\n",
      "staf 1\n",
      "staff 7\n",
      "stage 19\n",
      "stai 3\n",
      "stake 4\n",
      "stakehold 1\n",
      "stakeout 1\n",
      "stand 7\n",
      "standard 41\n",
      "standardis 1\n",
      "star 2\n",
      "stark 1\n",
      "start 21\n",
      "state 54\n",
      "static 6\n",
      "station 1\n",
      "stationari 7\n",
      "statisch 1\n",
      "statist 5\n",
      "statistisch 2\n",
      "stattfand 1\n",
      "statu 17\n",
      "stauffer 1\n",
      "stauffert 1\n",
      "steadi 1\n",
      "steadili 1\n",
      "steer 4\n",
      "steerabl 1\n",
      "stehen 4\n",
      "stehenden 1\n",
      "steht 3\n",
      "steigend 1\n",
      "steinhaeuss 2\n",
      "stellen 5\n",
      "stellenwert 1\n",
      "stellt 7\n",
      "stellten 1\n",
      "stem 20\n",
      "step 39\n",
      "stephi 1\n",
      "stereo 6\n",
      "stereochemistri 1\n",
      "stereoscop 49\n",
      "stereotyp 7\n",
      "stet 1\n",
      "steuernder 1\n",
      "steuerungen 1\n",
      "stevenarild 15\n",
      "sthesisten 1\n",
      "sticki 1\n",
      "stigmat 5\n",
      "still 40\n",
      "stimul 28\n",
      "stimuli 20\n",
      "stimulu 4\n",
      "stitch 1\n",
      "stock 1\n",
      "stoff 1\n",
      "stofflichen 1\n",
      "storag 3\n",
      "store 11\n",
      "stori 5\n",
      "storyt 1\n",
      "storytel 19\n",
      "stow 1\n",
      "strai 1\n",
      "straightforward 2\n",
      "strain 16\n",
      "strang 1\n",
      "stranger 1\n",
      "strateg 1\n",
      "strategi 15\n",
      "stream 18\n",
      "street 2\n",
      "strength 3\n",
      "strengthen 9\n",
      "stress 16\n",
      "stretch 1\n",
      "stride 4\n",
      "strive 1\n",
      "stroke 28\n",
      "stroll 5\n",
      "strong 14\n",
      "stronger 3\n",
      "strongest 2\n",
      "strongli 7\n",
      "structur 58\n",
      "strukturen 1\n",
      "strukturiert 1\n",
      "stuck 3\n",
      "student 97\n",
      "studi 360\n",
      "studierend 3\n",
      "studierenden 11\n",
      "studierstub 8\n",
      "studiert 1\n",
      "studio 13\n",
      "studium 2\n",
      "stung 1\n",
      "style 9\n",
      "styliz 3\n",
      "stylu 22\n",
      "stylus 6\n",
      "su 1\n",
      "sub 1\n",
      "subash 1\n",
      "subject 73\n",
      "subjektiv 1\n",
      "submiss 2\n",
      "suboptim 1\n",
      "subordin 2\n",
      "subscal 2\n",
      "subsequ 5\n",
      "subskalen 1\n",
      "subspecialti 2\n",
      "substanc 2\n",
      "substanti 4\n",
      "substitut 17\n",
      "subtask 1\n",
      "subtest 1\n",
      "subtl 2\n",
      "subtop 4\n",
      "success 41\n",
      "successfulli 13\n",
      "such 1\n",
      "sue 2\n",
      "suffer 12\n",
      "suffici 7\n",
      "suggest 42\n",
      "suit 5\n",
      "suitabl 13\n",
      "sum 7\n",
      "summ 2\n",
      "summar 7\n",
      "summari 10\n",
      "summer 1\n",
      "sung 1\n",
      "sungsan 1\n",
      "sunlight 3\n",
      "super 1\n",
      "superhuman 1\n",
      "superimpos 6\n",
      "superman 1\n",
      "supermarket 1\n",
      "superposit 1\n",
      "superquadr 1\n",
      "supervis 12\n",
      "supplement 6\n",
      "supplementari 2\n",
      "suppli 5\n",
      "supplier 1\n",
      "support 216\n",
      "suppos 2\n",
      "suppress 6\n",
      "sur 2\n",
      "sure 1\n",
      "surfac 74\n",
      "surgeon 15\n",
      "surgeri 55\n",
      "surgeryfrom 1\n",
      "surgic 60\n",
      "surprisingli 1\n",
      "surround 20\n",
      "surv 3\n",
      "survei 59\n",
      "surveil 2\n",
      "survivor 1\n",
      "suscept 3\n",
      "suspens 2\n",
      "sustain 14\n",
      "sutur 1\n",
      "svr 10\n",
      "swarm 2\n",
      "swarnamaalika 1\n",
      "swipe 6\n",
      "swiss 1\n",
      "switch 8\n",
      "sy 3\n",
      "sydewynd 1\n",
      "syllabu 4\n",
      "symbol 1\n",
      "sympathet 1\n",
      "symposium 2\n",
      "symposiumon 1\n",
      "symptom 5\n",
      "synchron 11\n",
      "synchronis 2\n",
      "syncret 1\n",
      "synergi 2\n",
      "synergien 1\n",
      "synergist 1\n",
      "synesthet 1\n",
      "synthes 4\n",
      "synthesi 2\n",
      "synthet 3\n",
      "sysrelevantforl3 2\n",
      "syst 2\n",
      "systefi 1\n",
      "system 747\n",
      "systemat 46\n",
      "systematisch 1\n",
      "systemdesign 1\n",
      "systemen 13\n",
      "systementwicklung 1\n",
      "systemergonomisch 3\n",
      "systemgestaltung 4\n",
      "systemplanung 1\n",
      "systemplattform 1\n",
      "szenarienbasiert 2\n",
      "szenengraphen 1\n",
      "szykman 3\n",
      "tabl 6\n",
      "tablet 13\n",
      "tabletop 26\n",
      "tackl 3\n",
      "tactil 3\n",
      "tactual 1\n",
      "tadit 1\n",
      "tag 12\n",
      "tailor 4\n",
      "take 23\n",
      "taken 16\n",
      "talk 2\n",
      "tam 2\n",
      "tangibl 54\n",
      "tangl 1\n",
      "tank 10\n",
      "tanz 1\n",
      "tap 10\n",
      "tape 1\n",
      "target 44\n",
      "tarpaulin 1\n",
      "task 163\n",
      "taspel 1\n",
      "tast 4\n",
      "tat 1\n",
      "tatigkeiten 1\n",
      "tattoo 5\n",
      "tattooar 2\n",
      "tauglichkeit 1\n",
      "tax 1\n",
      "taxonomi 8\n",
      "taxonomien 1\n",
      "tb 8\n",
      "tbi 1\n",
      "te 10\n",
      "teach 82\n",
      "teacher 60\n",
      "teaching1 2\n",
      "team 16\n",
      "teamroom 2\n",
      "teamwork 1\n",
      "tech 6\n",
      "technic 61\n",
      "technician 7\n",
      "technik 12\n",
      "techniken 8\n",
      "technikentwicklung 1\n",
      "techniqu 138\n",
      "technisch 5\n",
      "technischen 2\n",
      "technisierung 1\n",
      "technolog 350\n",
      "technologi 8\n",
      "technologien 8\n",
      "technologienutzung 1\n",
      "technologisch 1\n",
      "technozentriert 1\n",
      "tediou 1\n",
      "teen 2\n",
      "teesid 5\n",
      "tefl 3\n",
      "tegmen 5\n",
      "teil 4\n",
      "teilen 1\n",
      "teilhab 2\n",
      "teilweis 2\n",
      "tel 1\n",
      "tele 4\n",
      "telecollabor 1\n",
      "telecommun 2\n",
      "teleconferenc 2\n",
      "telehealth 1\n",
      "telemainten 1\n",
      "telementor 1\n",
      "telemetri 1\n",
      "teleoper 9\n",
      "telephon 2\n",
      "telepres 6\n",
      "telerobot 2\n",
      "telescop 1\n",
      "teleservic 2\n",
      "televis 14\n",
      "telexist 1\n",
      "tell 1\n",
      "tem 3\n",
      "temperatur 1\n",
      "templat 1\n",
      "tempor 85\n",
      "temporari 1\n",
      "temposurg 5\n",
      "ten 1\n",
      "tend 7\n",
      "tendenc 5\n",
      "tendenzen 1\n",
      "tenni 1\n",
      "tens 3\n",
      "tension 1\n",
      "tent 3\n",
      "tenteil 1\n",
      "ter 2\n",
      "tereb 1\n",
      "teren 1\n",
      "terer 1\n",
      "term 31\n",
      "termin 2\n",
      "terminologi 1\n",
      "terrif 1\n",
      "territori 2\n",
      "terror 1\n",
      "tertiari 1\n",
      "test 96\n",
      "testb 6\n",
      "testgebiet 3\n",
      "testicular 1\n",
      "testimoni 1\n",
      "tet 4\n",
      "tetc 6\n",
      "tether 2\n",
      "text 28\n",
      "textual 2\n",
      "textur 12\n",
      "th 3\n",
      "thanrefi0t31 1\n",
      "thau 1\n",
      "theapplianc 2\n",
      "theater 3\n",
      "theatr 8\n",
      "theatric 1\n",
      "theft 2\n",
      "thegreatemu 16\n",
      "thema 2\n",
      "thematisieren 2\n",
      "theme 5\n",
      "themen 2\n",
      "theorem 1\n",
      "theoret 18\n",
      "theoretischen 2\n",
      "theori 32\n",
      "therapeut 16\n",
      "therapi 80\n",
      "therapist 18\n",
      "therealist 1\n",
      "theresa 1\n",
      "therewith 2\n",
      "thermal 3\n",
      "thermostat 1\n",
      "thesaiorg 3\n",
      "thesi 13\n",
      "thick 1\n",
      "thiespfeiff 5\n",
      "thin 3\n",
      "think 26\n",
      "third 13\n",
      "thirteen 2\n",
      "thirti 1\n",
      "thisfie0 1\n",
      "thoma 2\n",
      "thought 10\n",
      "thousand 1\n",
      "threat 2\n",
      "threedimension 1\n",
      "threshold 2\n",
      "throw 4\n",
      "thrown 2\n",
      "ti 2\n",
      "tied 5\n",
      "tief 2\n",
      "tiefen 2\n",
      "tier 1\n",
      "tiger 1\n",
      "tigerey 1\n",
      "tighter 1\n",
      "tightli 2\n",
      "tigkeit 1\n",
      "tigkeiten 4\n",
      "tigt 4\n",
      "tile 9\n",
      "time 309\n",
      "timeless 1\n",
      "timeli 2\n",
      "timewarp 2\n",
      "tinker 1\n",
      "tinmith 1\n",
      "tinnitu 1\n",
      "tion 1\n",
      "tip 6\n",
      "tisch 6\n",
      "tissu 4\n",
      "titl 1\n",
      "tive 2\n",
      "tl 1\n",
      "tng 1\n",
      "tobi 1\n",
      "todai 22\n",
      "toenni 7\n",
      "tof 1\n",
      "toguid 1\n",
      "toi 3\n",
      "toler 3\n",
      "tomar 1\n",
      "tomographi 1\n",
      "tomorrow 2\n",
      "tone 2\n",
      "toni 1\n",
      "took 11\n",
      "tool 131\n",
      "tooldevic 1\n",
      "toolkit 22\n",
      "toolset 1\n",
      "top 19\n",
      "topic 26\n",
      "topograph 1\n",
      "topographi 1\n",
      "topolog 1\n",
      "toread 2\n",
      "tornado 1\n",
      "toronto 1\n",
      "torrenc 1\n",
      "toru 1\n",
      "toss 1\n",
      "tot 1\n",
      "total 27\n",
      "touch 48\n",
      "touchless 1\n",
      "toupsz 3\n",
      "tour 11\n",
      "tourism 3\n",
      "tourist 1\n",
      "touristenverkehr 1\n",
      "tower 1\n",
      "toxin 1\n",
      "toyra 1\n",
      "trace 9\n",
      "traceabl 1\n",
      "tracer 1\n",
      "traceread 1\n",
      "track 146\n",
      "tracker 16\n",
      "trackingsystem 1\n",
      "trackingsystemen 1\n",
      "trade 2\n",
      "trademark 1\n",
      "tradit 50\n",
      "tradition 3\n",
      "traditionellen 1\n",
      "traffic 1\n",
      "tragbar 6\n",
      "trai 1\n",
      "train 268\n",
      "traine 40\n",
      "trainer 13\n",
      "trainingsmal 1\n",
      "trainingsverfahren 1\n",
      "trait 16\n",
      "trajectori 9\n",
      "trampolin 1\n",
      "tran 1\n",
      "trane 1\n",
      "transcrani 1\n",
      "transcript 1\n",
      "transcultur 9\n",
      "transfer 14\n",
      "transform 41\n",
      "transformativ 1\n",
      "transformo 1\n",
      "transit 12\n",
      "transkulturel 6\n",
      "transkulturellen 3\n",
      "translat 5\n",
      "translatar 1\n",
      "transmedia 1\n",
      "transmiss 3\n",
      "transmit 3\n",
      "transpar 9\n",
      "transport 4\n",
      "trap 1\n",
      "traumat 4\n",
      "travel 23\n",
      "travers 8\n",
      "traviata 1\n",
      "treadmil 14\n",
      "treasa 1\n",
      "treasur 1\n",
      "treat 10\n",
      "treatment 28\n",
      "tree 5\n",
      "treffen 2\n",
      "trek 1\n",
      "trembl 1\n",
      "tremend 2\n",
      "trend 39\n",
      "tri 2\n",
      "trial 45\n",
      "tricord 1\n",
      "trigger 1\n",
      "trip 2\n",
      "trojan 1\n",
      "troll 1\n",
      "trope 1\n",
      "trotz 1\n",
      "troubl 2\n",
      "troubleshoot 1\n",
      "trough 1\n",
      "true 8\n",
      "truli 4\n",
      "trunk 2\n",
      "trust 5\n",
      "trustworthi 2\n",
      "try 8\n",
      "tsagenten 1\n",
      "tsseminar 3\n",
      "tsukuba 1\n",
      "tt 1\n",
      "tten 12\n",
      "tual 2\n",
      "tubal 2\n",
      "tubsicg 4\n",
      "tui 1\n",
      "tum 1\n",
      "tumor 2\n",
      "tunnel 3\n",
      "tural 1\n",
      "turbid 1\n",
      "ture 1\n",
      "turn 14\n",
      "tutor 8\n",
      "tutori 15\n",
      "tv 18\n",
      "tweek 1\n",
      "twelv 1\n",
      "twenti 11\n",
      "twice 4\n",
      "twohand 1\n",
      "txt 1\n",
      "ty 1\n",
      "tympan 3\n",
      "type 56\n",
      "typic 31\n",
      "tyson 1\n",
      "tze 2\n",
      "tzen 5\n",
      "tzlich 4\n",
      "tzlichen 1\n",
      "tzt 3\n",
      "tzte 2\n",
      "tzten 2\n",
      "tzter 1\n",
      "tzung 15\n",
      "u0027 14\n",
      "u0027industri 1\n",
      "uar 2\n",
      "ubigi 4\n",
      "ubiquit 64\n",
      "ubiquitouscomput 5\n",
      "ubiscop 2\n",
      "ubitrack 4\n",
      "ueq 1\n",
      "ufig 6\n",
      "ugment 2\n",
      "ui 4\n",
      "uipm 1\n",
      "ultim 2\n",
      "ultra 5\n",
      "ultrafast 6\n",
      "ultrasound 2\n",
      "um 30\n",
      "umap2023 1\n",
      "umar 1\n",
      "umbrella 1\n",
      "umen 2\n",
      "umfassend 3\n",
      "umfasst 3\n",
      "umgang 1\n",
      "umgeben 1\n",
      "umgebung 3\n",
      "umgebungen 2\n",
      "umgesetzt 1\n",
      "umi 3\n",
      "umlich 3\n",
      "umlichen 1\n",
      "umpc 1\n",
      "umsetzung 7\n",
      "umsetzungen 2\n",
      "umstieg 1\n",
      "umwelt 3\n",
      "umzusetzen 1\n",
      "un 6\n",
      "unabl 1\n",
      "unaffect 2\n",
      "unattain 1\n",
      "unawar 3\n",
      "unbegrenzt 1\n",
      "unblind 2\n",
      "uncalibr 2\n",
      "uncanni 2\n",
      "uncertainti 2\n",
      "unclear 8\n",
      "unconstrain 4\n",
      "unconvent 1\n",
      "und 322\n",
      "undecid 1\n",
      "underdevelop 1\n",
      "underestim 7\n",
      "undergon 1\n",
      "undergradu 3\n",
      "underground 4\n",
      "underli 10\n",
      "understand 62\n",
      "undertak 1\n",
      "undertaken 1\n",
      "underwat 8\n",
      "undetermin 2\n",
      "unencumb 1\n",
      "unerwartet 1\n",
      "uneth 1\n",
      "unexpect 1\n",
      "unfamiliar 1\n",
      "unfit 1\n",
      "unfold 1\n",
      "unforeseen 5\n",
      "unfortun 4\n",
      "ungenauigkeiten 1\n",
      "unifi 7\n",
      "uniform 4\n",
      "unimod 4\n",
      "union 2\n",
      "uniqu 7\n",
      "unit 7\n",
      "uniti 5\n",
      "unity3d 2\n",
      "univers 38\n",
      "universit 3\n",
      "uniwu 4\n",
      "unknown 8\n",
      "unlik 1\n",
      "unlimit 2\n",
      "unman 3\n",
      "unmet 2\n",
      "unnatur 2\n",
      "unnecessari 2\n",
      "unobstruct 1\n",
      "unobtrus 4\n",
      "unoccupi 2\n",
      "unoffici 2\n",
      "unpack 1\n",
      "unpleas 2\n",
      "unpredict 2\n",
      "unrealrox 1\n",
      "unrel 2\n",
      "unser 1\n",
      "unskil 1\n",
      "unsupervis 2\n",
      "unter 12\n",
      "unternehmen 4\n",
      "unternehmensteilen 1\n",
      "unterrichtseinheit 2\n",
      "unterrichtsentw 3\n",
      "unterrichtskonzept 3\n",
      "unterrichtskonzepten 3\n",
      "unterrichtsstunden 3\n",
      "unterscheiden 1\n",
      "unterschied 5\n",
      "unterschiedlich 8\n",
      "unterschiedlichen 3\n",
      "unterst 18\n",
      "untersucht 5\n",
      "untersuchung 4\n",
      "untersuchungen 2\n",
      "unterthemen 1\n",
      "unterzogen 1\n",
      "unteth 2\n",
      "untrain 1\n",
      "unveil 3\n",
      "unwant 8\n",
      "unzureichend 1\n",
      "updat 6\n",
      "upfront 1\n",
      "upper 8\n",
      "upright 2\n",
      "ur 2\n",
      "urban 46\n",
      "urg 2\n",
      "urgent 2\n",
      "url 14\n",
      "us 464\n",
      "usa 2\n",
      "usabl 64\n",
      "usag 16\n",
      "usc 1\n",
      "user 623\n",
      "usergener 1\n",
      "userinterfac 1\n",
      "userstudi 5\n",
      "usher 2\n",
      "utert 1\n",
      "util 39\n",
      "utilis 4\n",
      "utilisateur 1\n",
      "utter 2\n",
      "uvmod 1\n",
      "uwb 128\n",
      "ux 18\n",
      "v1205 6\n",
      "v1500 1\n",
      "v2 2\n",
      "v3i10 1\n",
      "va 10\n",
      "vaishnavi 2\n",
      "valenc 1\n",
      "valid 50\n",
      "validiert 1\n",
      "vallei 2\n",
      "valu 35\n",
      "valuabl 12\n",
      "valv 1\n",
      "van 1\n",
      "vantag 1\n",
      "vari 16\n",
      "variabl 23\n",
      "variat 2\n",
      "varieti 22\n",
      "varriertm 1\n",
      "varyfast 2\n",
      "vast 1\n",
      "vbo 4\n",
      "ve 59\n",
      "vection 1\n",
      "veenu 2\n",
      "vega 1\n",
      "veget 4\n",
      "vehicl 8\n",
      "vehicular 1\n",
      "veloc 2\n",
      "velop 1\n",
      "venga 1\n",
      "vengateshkumar 1\n",
      "venic 1\n",
      "venou 1\n",
      "vent 2\n",
      "ventriloquist 1\n",
      "venu 1\n",
      "ver 4\n",
      "verankert 1\n",
      "veranstaltungsreih 2\n",
      "verantwortung 2\n",
      "verbal 8\n",
      "verbessern 3\n",
      "verbessert 2\n",
      "verbesserten 1\n",
      "verbesserungen 2\n",
      "verbesserungsvorschl 1\n",
      "verbindet 1\n",
      "verborgen 2\n",
      "verbreitet 3\n",
      "verbreitung 1\n",
      "verbunden 8\n",
      "vereint 7\n",
      "verf 8\n",
      "verfahren 1\n",
      "verfolgt 3\n",
      "verg 1\n",
      "vergleich 3\n",
      "vergleichbar 1\n",
      "vergleichend 4\n",
      "vergleichenden 1\n",
      "verglichen 2\n",
      "verh 1\n",
      "verif 4\n",
      "verifi 2\n",
      "verifik 1\n",
      "verkehr 2\n",
      "verkn 2\n",
      "vermeiden 1\n",
      "vermittelt 3\n",
      "vermittlung 1\n",
      "veronicasu 5\n",
      "versatil 5\n",
      "verschieden 6\n",
      "verschiedenen 5\n",
      "verschli 1\n",
      "verschmelzung 1\n",
      "version 33\n",
      "versorgungsmobilit 1\n",
      "verst 8\n",
      "versuchspersonen 1\n",
      "versuchsumgebung 2\n",
      "verteilten 1\n",
      "vertic 2\n",
      "vertieft 3\n",
      "vertreten 1\n",
      "verwenden 4\n",
      "verwendet 2\n",
      "verwendeten 1\n",
      "verwendung 4\n",
      "vesp 1\n",
      "vestibular 6\n",
      "vi 1\n",
      "viabil 1\n",
      "viabl 4\n",
      "vibro 1\n",
      "vibrotactil 1\n",
      "vice 1\n",
      "victim 1\n",
      "victori 1\n",
      "victorian 2\n",
      "vide 2\n",
      "video 117\n",
      "videoanleitung 1\n",
      "videoconferenc 4\n",
      "videophon 1\n",
      "videotap 3\n",
      "viel 1\n",
      "vielen 2\n",
      "vieler 1\n",
      "vielmehr 2\n",
      "vielschichtig 1\n",
      "vielschichtigen 1\n",
      "vienna 1\n",
      "vierten 1\n",
      "view 111\n",
      "viewer 5\n",
      "viewpoint 6\n",
      "viewport 1\n",
      "vignesh 1\n",
      "vilearn 4\n",
      "villain 1\n",
      "vinci 1\n",
      "violet 1\n",
      "vir 2\n",
      "virolog 2\n",
      "virtu 2\n",
      "virtual 2252\n",
      "virtualhau 1\n",
      "virtualqwerti 1\n",
      "virtualr 11\n",
      "virtualt 1\n",
      "virtuatx 1\n",
      "virtuel 33\n",
      "virtuellen 17\n",
      "virtueron2 1\n",
      "virtuou 1\n",
      "visag 1\n",
      "vishap 1\n",
      "visibl 9\n",
      "vision 31\n",
      "visit 4\n",
      "visitor 12\n",
      "vista 3\n",
      "visual 334\n",
      "visualexpresso 1\n",
      "visuali 1\n",
      "visualis 17\n",
      "visualisierung 3\n",
      "visuel 2\n",
      "visuellen 1\n",
      "visuo 6\n",
      "visuomotor 4\n",
      "vital 10\n",
      "vitaz 1\n",
      "vitra 45\n",
      "vitrectomi 1\n",
      "vitro 1\n",
      "vive 3\n",
      "vivo 4\n",
      "vjing 1\n",
      "vocabulari 1\n",
      "vocal 2\n",
      "voic 7\n",
      "void 1\n",
      "volit 6\n",
      "vollst 3\n",
      "voltag 1\n",
      "volum 18\n",
      "volumetr 3\n",
      "volunt 1\n",
      "voluntari 1\n",
      "volvo 1\n",
      "vom 2\n",
      "von 131\n",
      "vonmammen 1\n",
      "vonolfen 1\n",
      "vor 17\n",
      "vorangetrieben 1\n",
      "vorausgesetzt 1\n",
      "voraussetzungen 1\n",
      "voraussichtlichen 1\n",
      "vordergrund 1\n",
      "vorgehen 5\n",
      "vorgehensweis 1\n",
      "vorgehensweisen 1\n",
      "vorgelagert 1\n",
      "vorgenommen 2\n",
      "vorgeschlagen 1\n",
      "vorgestellt 16\n",
      "vorgestellten 3\n",
      "vorhergehend 1\n",
      "vorherzusehen 1\n",
      "vorlag 1\n",
      "vorliegend 1\n",
      "vorstellung 2\n",
      "vorteil 2\n",
      "vortrag 2\n",
      "vorzusto 1\n",
      "voucher 1\n",
      "voxel 9\n",
      "voxelman 7\n",
      "vr 659\n",
      "vr2 2\n",
      "vr4vrt 1\n",
      "vraudi 2\n",
      "vrd 16\n",
      "vrecko 1\n",
      "vret 11\n",
      "vrgait 1\n",
      "vrgi 2\n",
      "vrise 1\n",
      "vrml 4\n",
      "vrsim 1\n",
      "vrst19 4\n",
      "vsrt 1\n",
      "vst 16\n",
      "vt 4\n",
      "vtime 5\n",
      "vuforia 1\n",
      "vulner 2\n",
      "vuzix 1\n",
      "vwedu 1\n",
      "wadvancez 1\n",
      "wag 1\n",
      "wagner 1\n",
      "wahrgenommenen 2\n",
      "wahrnehmung 1\n",
      "wai 64\n",
      "wait 11\n",
      "walk 60\n",
      "walkabl 1\n",
      "walkthrough 3\n",
      "wall 6\n",
      "walli 1\n",
      "walsh 2\n",
      "wand 1\n",
      "wandel 1\n",
      "want 10\n",
      "war 1\n",
      "warehous 1\n",
      "waren 1\n",
      "warfar 1\n",
      "warrant 2\n",
      "wartungskompetenz 1\n",
      "washington 2\n",
      "wast 11\n",
      "water 2\n",
      "wave 1\n",
      "wayfind 2\n",
      "wde 1\n",
      "wdee 1\n",
      "we61 1\n",
      "we719ifiz9t 1\n",
      "weak 5\n",
      "weakli 1\n",
      "wear 8\n",
      "wearabl 41\n",
      "weather 3\n",
      "weav 1\n",
      "weaver 1\n",
      "web 39\n",
      "webcam 4\n",
      "webiz 2\n",
      "webqual 1\n",
      "websit 3\n",
      "wechseln 1\n",
      "week 5\n",
      "weg 1\n",
      "wege 3\n",
      "weibel 8\n",
      "weight 124\n",
      "weit 1\n",
      "weiter 4\n",
      "weiteren 3\n",
      "weiterentwickelnd 1\n",
      "weiterf 1\n",
      "weiterhin 1\n",
      "weiterverarbeitet 2\n",
      "welch 7\n",
      "welcher 1\n",
      "welcom 2\n",
      "weld 19\n",
      "welder 2\n",
      "well 145\n",
      "welt 2\n",
      "welten 3\n",
      "wem 1\n",
      "wenger 1\n",
      "wenig 4\n",
      "wenn 1\n",
      "wer 1\n",
      "werd 1\n",
      "werden 76\n",
      "werfen 1\n",
      "werkst 11\n",
      "werkstaetten 1\n",
      "werkstatt 2\n",
      "werkstoffbearbeitung 1\n",
      "werkzeug 2\n",
      "werkzeugbau 1\n",
      "werkzeugen 1\n",
      "werkzeugmaschinen 3\n",
      "wertsch 2\n",
      "wertvollen 2\n",
      "wesentlich 2\n",
      "wesentlichen 2\n",
      "wettbewerb 1\n",
      "whale 1\n",
      "what 2\n",
      "wheatston 1\n",
      "wheelchair 3\n",
      "whist 1\n",
      "white 1\n",
      "whitnei 4\n",
      "who 28\n",
      "whole 5\n",
      "wichtig 5\n",
      "wichtigen 1\n",
      "wide 27\n",
      "wider 5\n",
      "widespread 1\n",
      "widget 2\n",
      "wie 18\n",
      "wiedenmai 1\n",
      "wieder 1\n",
      "wiedergegeben 1\n",
      "wienrich 30\n",
      "wifi 1\n",
      "wii 5\n",
      "wikipedia 1\n",
      "wilcoxon 1\n",
      "wild 2\n",
      "wiljami74 1\n",
      "will 107\n",
      "willingham 1\n",
      "willwad 4\n",
      "wim 19\n",
      "window 15\n",
      "windtunnel 1\n",
      "wir 14\n",
      "wird 33\n",
      "wire 2\n",
      "wirear 1\n",
      "wiredraw 1\n",
      "wireless 9\n",
      "wirklich 1\n",
      "wirkungsprozess 1\n",
      "wirtschaft 3\n",
      "wirtschaftlichen 2\n",
      "wirtschaftlichkeit 1\n",
      "wish 2\n",
      "wissen 1\n",
      "wissenschaft 3\n",
      "wissenschaftlich 4\n",
      "wissenschaftlichen 1\n",
      "wissensmanag 1\n",
      "wissensvermittlung 1\n",
      "witha 1\n",
      "withteeth 1\n",
      "wizard 2\n",
      "wlic 3\n",
      "wo 1\n",
      "wobei 2\n",
      "wof 1\n",
      "wohl 2\n",
      "wolf 2\n",
      "women 6\n",
      "wonder 2\n",
      "wonderland 1\n",
      "wood 4\n",
      "wooden 2\n",
      "word 13\n",
      "wordgestur 2\n",
      "work 161\n",
      "workbench 2\n",
      "worker 8\n",
      "workflow 7\n",
      "workforc 1\n",
      "workgroup 1\n",
      "workload 10\n",
      "workplac 4\n",
      "workshop 54\n",
      "workspac 15\n",
      "world 173\n",
      "worn 2\n",
      "wors 2\n",
      "worst 4\n",
      "wortec 1\n",
      "worten 1\n",
      "worth 1\n",
      "wrist 1\n",
      "write 4\n",
      "written 6\n",
      "wrt 4\n",
      "wurd 28\n",
      "wurdeim 1\n",
      "wurden 16\n",
      "www 3165\n",
      "x00b0 1\n",
      "x2013 1\n",
      "x2019 1\n",
      "x201c 1\n",
      "x201d 1\n",
      "x3d 3\n",
      "x3dom 1\n",
      "ximul 1\n",
      "xml 2\n",
      "xr 71\n",
      "xrealiti 1\n",
      "xrhub 1\n",
      "xroad 5\n",
      "xtreme 1\n",
      "yadav 2\n",
      "yarmi 1\n",
      "year 34\n",
      "yellow 1\n",
      "yield 4\n",
      "yish 6\n",
      "yogesh 1\n",
      "young 5\n",
      "younger 1\n",
      "youth 1\n",
      "youtub 1\n",
      "ystem 1\n",
      "zahl 1\n",
      "zation 1\n",
      "zeeland 1\n",
      "zeigen 5\n",
      "zeigt 4\n",
      "zeit 3\n",
      "zeitalt 1\n",
      "zeitgeb 2\n",
      "zeitkalibrierung 2\n",
      "zeitlich 1\n",
      "zeng 1\n",
      "zentral 4\n",
      "zentralen 2\n",
      "zentrum 1\n",
      "ziel 4\n",
      "zielerreichung 6\n",
      "zielf 9\n",
      "zielort 1\n",
      "zielpfad 1\n",
      "zigbe 3\n",
      "zombi 1\n",
      "zone 2\n",
      "zoologieii 1\n",
      "zoom 1\n",
      "zu 77\n",
      "zudem 1\n",
      "zuerst 2\n",
      "zufriedenheitsgrad 6\n",
      "zug 1\n",
      "zuge 1\n",
      "zugeh 1\n",
      "zuk 6\n",
      "zukunft 1\n",
      "zukunftsvisionen 1\n",
      "zum 35\n",
      "zun 9\n",
      "zunehmend 11\n",
      "zur 53\n",
      "zusammen 1\n",
      "zusammenarbeit 8\n",
      "zusammenarbeiten 1\n",
      "zusammenfassung 1\n",
      "zusammengefasst 2\n",
      "zusammenh 3\n",
      "zusammenhang 1\n",
      "zusatzinform 1\n",
      "zuschauer 1\n",
      "zuschauerkontexten 1\n",
      "zwar 1\n",
      "zwei 6\n",
      "zweidimensional 3\n",
      "zweidimensionalen 1\n",
      "zweit 1\n",
      "zweiten 5\n",
      "zwischen 5\n",
      "zzz 11\n"
     ]
    }
   ],
   "source": [
    "for key, value in tf_dict.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dblp', 4811),\n",
       " ('realiti', 4213),\n",
       " ('http', 3173),\n",
       " ('www', 3165),\n",
       " ('org', 3152),\n",
       " ('bibsonomi', 3149),\n",
       " ('bibtex', 3149),\n",
       " ('null', 2618),\n",
       " ('virtual', 2252),\n",
       " ('augment', 1721),\n",
       " ('mix', 1205),\n",
       " ('system', 747),\n",
       " ('interact', 698),\n",
       " ('vr', 659),\n",
       " ('user', 623),\n",
       " ('environ', 565),\n",
       " ('applic', 548),\n",
       " ('us', 464),\n",
       " ('base', 451),\n",
       " ('ar', 440)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dict_sorted = sorted(tf_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "tf_dict_sorted[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create search engine with `tf` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = pt.BatchRetrieve(index_mult, wmodel='Tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/21d2cad7623d1...</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/21b40218abc28...</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2e033978aa497...</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2b80b466b358b...</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2c1f2498fcee3...</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                                              docno  rank  score  \\\n",
       "0   1     20  https://www.bibsonomy.org/bibtex/21d2cad7623d1...     0   13.0   \n",
       "1   1     11  https://www.bibsonomy.org/bibtex/21b40218abc28...     1   12.0   \n",
       "2   1      0  https://www.bibsonomy.org/bibtex/2e033978aa497...     2   10.0   \n",
       "3   1      5  https://www.bibsonomy.org/bibtex/2b80b466b358b...     3    9.0   \n",
       "4   1     15  https://www.bibsonomy.org/bibtex/2c1f2498fcee3...     4    9.0   \n",
       "\n",
       "        query  \n",
       "0  makerspace  \n",
       "1  makerspace  \n",
       "2  makerspace  \n",
       "3  makerspace  \n",
       "4  makerspace  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.search('makerspace').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create search engine with `tf_idf` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pt.BatchRetrieve(index_mult, wmodel='TF_IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2b80b466b358b...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.491605</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2e033978aa497...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.328137</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2a34c65255fec...</td>\n",
       "      <td>2</td>\n",
       "      <td>4.294879</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/28a2b925ea75f...</td>\n",
       "      <td>3</td>\n",
       "      <td>4.167149</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/26307bb287f16...</td>\n",
       "      <td>4</td>\n",
       "      <td>4.166325</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                                              docno  rank  \\\n",
       "0   1      5  https://www.bibsonomy.org/bibtex/2b80b466b358b...     0   \n",
       "1   1      0  https://www.bibsonomy.org/bibtex/2e033978aa497...     1   \n",
       "2   1     19  https://www.bibsonomy.org/bibtex/2a34c65255fec...     2   \n",
       "3   1     68  https://www.bibsonomy.org/bibtex/28a2b925ea75f...     3   \n",
       "4   1      8  https://www.bibsonomy.org/bibtex/26307bb287f16...     4   \n",
       "\n",
       "      score       query  \n",
       "0  4.491605  makerspace  \n",
       "1  4.328137  makerspace  \n",
       "2  4.294879  makerspace  \n",
       "3  4.167149  makerspace  \n",
       "4  4.166325  makerspace  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.search('makerspace').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare `tf_idf` and `tf` results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/28463afafe48b...</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>virtual reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2fcb9d250115e...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>virtual reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2155</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2fcb9d250115e...</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>virtual reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>229</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/23911dd5794fe...</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>virtual reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2f9e2a782e3ab...</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>virtual reality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                                              docno  rank  score  \\\n",
       "0   1    159  https://www.bibsonomy.org/bibtex/28463afafe48b...     0   28.0   \n",
       "1   1    153  https://www.bibsonomy.org/bibtex/2fcb9d250115e...     1   20.0   \n",
       "2   1   2155  https://www.bibsonomy.org/bibtex/2fcb9d250115e...     2   20.0   \n",
       "3   1    229  https://www.bibsonomy.org/bibtex/23911dd5794fe...     3   19.0   \n",
       "4   1    155  https://www.bibsonomy.org/bibtex/2f9e2a782e3ab...     4   18.0   \n",
       "\n",
       "             query  \n",
       "0  virtual reality  \n",
       "1  virtual reality  \n",
       "2  virtual reality  \n",
       "3  virtual reality  \n",
       "4  virtual reality  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.search('virtual reality').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>531</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2bd32e83b0475...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.628859</td>\n",
       "      <td>virtual reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>471</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2570759f7e945...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.583411</td>\n",
       "      <td>virtual reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>542</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/274b2a531957e...</td>\n",
       "      <td>2</td>\n",
       "      <td>2.575140</td>\n",
       "      <td>virtual reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2548</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/274b2a531957e...</td>\n",
       "      <td>3</td>\n",
       "      <td>2.575140</td>\n",
       "      <td>virtual reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>606</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/28a235c24570f...</td>\n",
       "      <td>4</td>\n",
       "      <td>2.567425</td>\n",
       "      <td>virtual reality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                                              docno  rank  \\\n",
       "0   1    531  https://www.bibsonomy.org/bibtex/2bd32e83b0475...     0   \n",
       "1   1    471  https://www.bibsonomy.org/bibtex/2570759f7e945...     1   \n",
       "2   1    542  https://www.bibsonomy.org/bibtex/274b2a531957e...     2   \n",
       "3   1   2548  https://www.bibsonomy.org/bibtex/274b2a531957e...     3   \n",
       "4   1    606  https://www.bibsonomy.org/bibtex/28a235c24570f...     4   \n",
       "\n",
       "      score            query  \n",
       "0  2.628859  virtual reality  \n",
       "1  2.583411  virtual reality  \n",
       "2  2.575140  virtual reality  \n",
       "3  2.575140  virtual reality  \n",
       "4  2.567425  virtual reality  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.search('virtual reality').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/21d2cad7623d1...</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>makerspaces in libraries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/21b40218abc28...</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>makerspaces in libraries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2b80b466b358b...</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>makerspaces in libraries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2c1f2498fcee3...</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>makerspaces in libraries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/27b26f179035f...</td>\n",
       "      <td>4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>makerspaces in libraries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                                              docno  rank  score  \\\n",
       "0   1     20  https://www.bibsonomy.org/bibtex/21d2cad7623d1...     0   24.0   \n",
       "1   1     11  https://www.bibsonomy.org/bibtex/21b40218abc28...     1   21.0   \n",
       "2   1      5  https://www.bibsonomy.org/bibtex/2b80b466b358b...     2   15.0   \n",
       "3   1     15  https://www.bibsonomy.org/bibtex/2c1f2498fcee3...     3   15.0   \n",
       "4   1     16  https://www.bibsonomy.org/bibtex/27b26f179035f...     4   14.0   \n",
       "\n",
       "                      query  \n",
       "0  makerspaces in libraries  \n",
       "1  makerspaces in libraries  \n",
       "2  makerspaces in libraries  \n",
       "3  makerspaces in libraries  \n",
       "4  makerspaces in libraries  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.search('makerspaces in libraries').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2b80b466b358b...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.725281</td>\n",
       "      <td>makerspaces in libraries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/26307bb287f16...</td>\n",
       "      <td>1</td>\n",
       "      <td>8.900658</td>\n",
       "      <td>makerspaces in libraries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2398705cf6922...</td>\n",
       "      <td>2</td>\n",
       "      <td>8.771493</td>\n",
       "      <td>makerspaces in libraries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/28a2b925ea75f...</td>\n",
       "      <td>3</td>\n",
       "      <td>8.389140</td>\n",
       "      <td>makerspaces in libraries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/27b26f179035f...</td>\n",
       "      <td>4</td>\n",
       "      <td>8.321789</td>\n",
       "      <td>makerspaces in libraries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                                              docno  rank  \\\n",
       "0   1      5  https://www.bibsonomy.org/bibtex/2b80b466b358b...     0   \n",
       "1   1      8  https://www.bibsonomy.org/bibtex/26307bb287f16...     1   \n",
       "2   1      9  https://www.bibsonomy.org/bibtex/2398705cf6922...     2   \n",
       "3   1     68  https://www.bibsonomy.org/bibtex/28a2b925ea75f...     3   \n",
       "4   1     16  https://www.bibsonomy.org/bibtex/27b26f179035f...     4   \n",
       "\n",
       "      score                     query  \n",
       "0  9.725281  makerspaces in libraries  \n",
       "1  8.900658  makerspaces in libraries  \n",
       "2  8.771493  makerspaces in libraries  \n",
       "3  8.389140  makerspaces in libraries  \n",
       "4  8.321789  makerspaces in libraries  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.search('makerspaces in libraries').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function for own `tf_idf` calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Die **Termfrequenz** (term frequency, Termhäufigkeit) tf<sub>t,d</sub> eines Terms t in Dokument d ist die *Häufigkeit von t in d*:\n",
    "    * Um die Wirkung der Termfrequenz zu dämpfen, wird häufig mit der logarithmierten Termfrequenz gearbeitet: $w_{t,d} = 1 + log_{10}(tf_{t,d})$\n",
    "        * $tf_{t,d}$ = Häufigkeit des Terms $t$ in Dokument $d$\n",
    "* df ist die **Dokumentfrequenz** für t: Die Anzahl der Dokumente, die t enthält.\n",
    "    * Wir definieren idf (**inverse Dokumentfrequenz**) von t als: $idf_t = log_{10}(N/df_t)$\n",
    "        * $df$ = Anzahl der Dokumente, die $t$ enthält\n",
    "        * $t$ = Term\n",
    "        * $N$ = Anzahl Dokumente\n",
    "* Die **tf-idf-Gewichtung** von Termen ist das Produkt der tf- und idf-Werte: tf-idf<sub>td</sub>:\n",
    "    * $tf-idf_{t,d} = tf_{t,d} * log_{10}(N/df_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def get_idf_for_term(term, index):\n",
    "    \"\"\"\n",
    "    Calculate the inverse document frequency (IDF) for a given term in a specified index.\n",
    "\n",
    "    The function first stems the provided term to its root form using the PorterStemmer \n",
    "    algorithm. It then checks if the stemmed term is present in the lexicon of the index. \n",
    "    If not present, the function returns None.\n",
    "\n",
    "    If the term is in the lexicon, it calculates the IDF. The IDF is determined by dividing \n",
    "    the total number of documents in the index by the document frequency of the term (i.e., \n",
    "    the number of documents containing the term). The function then applies a logarithmic \n",
    "    scale (base 10) to the IDF value to dampen its effect.\n",
    "\n",
    "    Parameters:\n",
    "    - term (str): The term for which the IDF is to be calculated.\n",
    "    - index: An object representing the index. This object must have the following methods:\n",
    "        - getLexicon(): Returns a dictionary representing the lexicon of the index.\n",
    "        - getCollectionStatistics(): Returns an object with a property numberOfDocuments \n",
    "        indicating the total number of documents in the index.\n",
    "\n",
    "    Returns:\n",
    "    - float: The log-scaled inverse document frequency of the term. If the term is not in \n",
    "      the lexicon, returns None.\n",
    "\n",
    "    Examples:\n",
    "    >>> index = SomeIndex()  # Assuming SomeIndex is a pre-defined index object\n",
    "    >>> get_idf_for_term(\"example\", index)\n",
    "    2.373635  # Hypothetical output\n",
    "    \"\"\"\n",
    "    # Stem search term\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_term = stemmer.stem(term)\n",
    "    \n",
    "    # Get lexicon of all searchable terms and return if stemmed_term is not found\n",
    "    lexicon = index.getLexicon()\n",
    "    if not stemmed_term in lexicon:\n",
    "        return\n",
    "    \n",
    "    # Get document_frequency for stemmed_term\n",
    "    df = lexicon[stemmed_term].getDocumentFrequency()\n",
    "    N = index.getCollectionStatistics().numberOfDocuments\n",
    "    \n",
    "    # Calculate inverse_document_frequency\n",
    "    idf = N / df\n",
    "    \n",
    "    # Apply log10 to dampen the effect\n",
    "    log_idf = np.emath.log10(idf)\n",
    "    \n",
    "    return log_idf\n",
    "\n",
    "#define your own tf_idf method here:\n",
    "#remember that tfidf is the product of two components\n",
    "#hint: the tf model search result contains tf frequencies  \n",
    "def calc_tf_idf_for_document(query, docno, index, search_engine):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        query (_type_): _description_\n",
    "        docno (_type_): _description_\n",
    "        index (_type_): _description_\n",
    "        search_engine (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    sum_tf, sum_log_idf = 0, 0\n",
    "    terms = query.split()\n",
    "    \n",
    "    for term in terms:\n",
    "        # Get tf for _query_ term in document _docno_\n",
    "        df_result = search_engine.search(term)\n",
    "        \n",
    "        try:\n",
    "            tf = df_result.query('docno == @docno')['score'].iloc[0]\n",
    "            print(f'tf for \"{term}\" in {docno}:\\n\\t--> {tf}')\n",
    "            # log_tf = 1 + np.emath.log10(tf)\n",
    "        except:\n",
    "            # term not found in doc\n",
    "            tf = 0\n",
    "            print(f'\"{term}\" not found in {docno}:\\n\\t--> {tf}')\n",
    "            \n",
    "        # Get log_idf for term\n",
    "        log_idf = get_idf_for_term(term, index=index)\n",
    "        \n",
    "        # Sum tf and log_idf for each term\n",
    "        sum_tf += tf\n",
    "        sum_log_idf += log_idf\n",
    "        \n",
    "    # Calculate tf_idf\n",
    "    tf_idf = sum_tf * sum_log_idf\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf for \"makerspace\" in https://www.bibsonomy.org/bibtex/2b80b466b358b345222205aee793721df/bibgreen:\n",
      "\t--> 9.0\n",
      "\"education\" not found in https://www.bibsonomy.org/bibtex/2b80b466b358b345222205aee793721df/bibgreen:\n",
      "\t--> 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24.901123115627563"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_tf_idf_for_document(query='makerspace education',\n",
    "                         docno='https://www.bibsonomy.org/bibtex/2b80b466b358b345222205aee793721df/bibgreen',\n",
    "                         index=index_mult,\n",
    "                         search_engine=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/21d2cad7623d1...</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/21b40218abc28...</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2e033978aa497...</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2b80b466b358b...</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2c1f2498fcee3...</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>makerspace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                                              docno  rank  score  \\\n",
       "0   1     20  https://www.bibsonomy.org/bibtex/21d2cad7623d1...     0   13.0   \n",
       "1   1     11  https://www.bibsonomy.org/bibtex/21b40218abc28...     1   12.0   \n",
       "2   1      0  https://www.bibsonomy.org/bibtex/2e033978aa497...     2   10.0   \n",
       "3   1      5  https://www.bibsonomy.org/bibtex/2b80b466b358b...     3    9.0   \n",
       "4   1     15  https://www.bibsonomy.org/bibtex/2c1f2498fcee3...     4    9.0   \n",
       "\n",
       "        query  \n",
       "0  makerspace  \n",
       "1  makerspace  \n",
       "2  makerspace  \n",
       "3  makerspace  \n",
       "4  makerspace  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.search('makerspace')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.bibsonomy.org/bibtex/2b80b466b358b...</td>\n",
       "      <td>17</td>\n",
       "      <td>4.491605</td>\n",
       "      <td>makerspace education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid  docid                                              docno  rank  \\\n",
       "17   1      5  https://www.bibsonomy.org/bibtex/2b80b466b358b...    17   \n",
       "\n",
       "       score                 query  \n",
       "17  4.491605  makerspace education  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tf_idf.search('makerspace education')\n",
    "result.query('docid == 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.bibsonomy.org/bibtex/2b80b466b358b345222205aee793721df/bibgreen',\n",
       " 'https://www.bibsonomy.org/bibtex/2e033978aa4971fb861cc33beb8c3bb7c/bibgreen',\n",
       " 'https://www.bibsonomy.org/bibtex/2a34c65255fecb9a8e28752676167384d/dblp',\n",
       " 'https://www.bibsonomy.org/bibtex/28a2b925ea75f2550c3f30a09b5b3c9c2/dblp',\n",
       " 'https://www.bibsonomy.org/bibtex/26307bb287f16962ec01c8813ff207dcb/bibgreen']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.search('makerspace')['docno'].to_list()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_search_engine(query, data_dict, search_engine, limit=10):\n",
    "    \"\"\"Query a PyTerrier `pt.BatchRetrieve` and match the result\n",
    "    to the dictionary that `pt.BatchRetrieve` uses to build its index.\n",
    "\n",
    "    Args:\n",
    "        query (`str`): String to query index.\n",
    "        data_dict (`dict`): Dictionary containing all documents of the index.\n",
    "        search_engine (`pt.BatchRetrieve`): Search engine\n",
    "        limit (`int`): Number of query results\n",
    "    \"\"\"\n",
    "    result = search_engine.search(query)\n",
    "    x = 0\n",
    "    for _, row in result.iterrows():\n",
    "        for entry in data_dict:\n",
    "            if entry['docno'] == row['docno']:\n",
    "                print(f\"Entry: \\t\\t {x + 1}\")\n",
    "                print(f\"Title: \\t\\t {entry['text']}\")\n",
    "                if isinstance(entry['author'], list):\n",
    "                    print(f\"Author(s): \\t {', '.join(entry['author'])}\")\n",
    "                else:\n",
    "                    print(f\"Author(s): \\t {entry['author']}\")\n",
    "                print(f\"Abstract: \\t {entry['abstract']}\")\n",
    "                print(f\"Year: \\t\\t {entry['year']}\")\n",
    "                print(f\"URL: \\t\\t {entry['url']}\") if len(entry['url']) > 1 else print(f\"URL: \\t\\t {entry['docno']}\")\n",
    "                print(f\"Score: \\t\\t {row['score']:.2f}\")\n",
    "                print()\n",
    "                x += 1   \n",
    "        if x == limit:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry: \t\t 1\n",
      "Title: \t\t Makerspaces: A practical guide for librarians\n",
      "Author(s): \t John J. Burke, Ellyssa Kroski\n",
      "Abstract: \t This book is a guidebook jam-packed with resources, advice, and information to help you develop and fund your own makerspace from the ground up. Learn what other libraries are making, building, and doing in their makerspaces and how you can, too. Readers are introduced to makerspace equipment, new technologies, models for planning and assessing projects, and useful case studies that will equip them with the knowledge to implement their own library makerspaces. This expanded second edition features eighteen brand new library makerspace profiles providing advice and inspiration for how to create your own library makerspace, over twenty new images and figures illustrating maker tools and trends as well as library makerspaces in action and new lists of actual grant and funding sources for library makerspaces.\n",
      "Year: \t\t 2018\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/2b80b466b358b345222205aee793721df/bibgreen\n",
      "Score: \t\t 9.73\n",
      "\n",
      "Entry: \t\t 2\n",
      "Title: \t\t Makerspaces in Libraries\n",
      "Author(s): \t Theresa Willingham, Jeroen DeBoer\n",
      "Abstract: \t Makerspaces, sometimes also referred to as hackerspaces, hackspaces, and fablabs are creative, DIY spaces where people can gather to create, invent, and learn. Discover how you can create a makerspace within your own library though this step-by-step guidebook.\n",
      "Year: \t\t 2015\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/26307bb287f16962ec01c8813ff207dcb/bibgreen\n",
      "Score: \t\t 8.90\n",
      "\n",
      "Entry: \t\t 3\n",
      "Title: \t\t Makerspaces in practice: Successful models for implementation\n",
      "Author(s): \t None\n",
      "Abstract: \t Makerspaces and maker activities have evolved from a shiny new trend in libraries to an acknowledged and valued conduit for partnering with library patrons in the production process and a potent means to provide STEM and critical thinking skills to people of all ages. In a 2017 Library Journal survey of 7,000 public libraries, it was determined that the vast majority of them---89 percent---currently offer maker programming for their patrons. Makerspaces in Practice: Successful Models for Implementation is an advanced guidebook to library makerspaces written from a perspective derived from years of practical experience. Written nearly half a decade after The Makerspace Librarian's Sourcebook was published, this book strives to be of use not only to librarians who are strategizing how to get started but also to those who are actively running makerspaces and maker programming in their libraries. This handbook offers advice from seasoned practitioners based on what has worked for them as well as which programs and tools don't resonate with library patrons. This essential handbook will answer these questions and more\n",
      "Year: \t\t 2021\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/2398705cf6922120ca2c9f7b28b83a5bb/bibgreen\n",
      "Score: \t\t 8.77\n",
      "\n",
      "Entry: \t\t 4\n",
      "Title: \t\t Makerspaces and boundary work: the role of librarians as educators in public library makerspaces.\n",
      "Author(s): \t Rachel D. Williams, Rebekah Willett\n",
      "Abstract: \t None\n",
      "Year: \t\t 2019\n",
      "URL: \t\t http://dblp.uni-trier.de/db/journals/jolis/jolis51.html#WilliamsW19\n",
      "Score: \t\t 8.39\n",
      "\n",
      "Entry: \t\t 5\n",
      "Title: \t\t Every good adaptation is also an innovation: Poster presented at IFLA WLIC 2019, Athens, Greece\n",
      "Author(s): \t Donatas Kubilius, Vaida Gasiūnaitė\n",
      "Abstract: \t Every good adaptation is also an innovation poster represents Makerspace PATS SAU in National library of Lithuania. It is an open access and free of charge service for schoolers that brings the change to learning experience. Poster represents benefits (advantages) that Makerspace brings to National Library, Public libraries and most importantly to the users. The aim of the poster is to present Makerspace as a bridge that connects Traditional Library and emerging technologies such as 3D printing, Coding, sustainable design, Virtual Reality and etc. Makerspace is a successful service from National Library of Lithuania that proved to be a good way to attract new users and strengthen library‘s community. In almost 3 years‘ time 10000 users have used our services and discovered library in a new and exiting way. IFLA WLIC 2019 main topic “Dialogue for change“ was exactly what we wanted to communicate through bringing our expertise to the conference. Our successful service is a great way to encourage other libraries to take on courageous initiatives, unconventional solutions and open the libraries‘ door to meet technological changes that are forming new way of learning, communicating, sharing and creating. Short movies about Makerspace that were used along with the printed poster: https://www.youtube.com/playlist?list=PLZHTxpVgsgcOTYh3e6OxyQzfjt9Plre\\_9\n",
      "Year: \t\t 2019\n",
      "URL: \t\t http://library.ifla.org/id/eprint/2666\n",
      "Score: \t\t 8.32\n",
      "\n",
      "Entry: \t\t 6\n",
      "Title: \t\t Sustaining the Library Makerspace\n",
      "Author(s): \t Jamie Bair\n",
      "Abstract: \t None\n",
      "Year: \t\t 2021\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/2d163403c2aaf6309d608fd36bba02db8/bibgreen\n",
      "Score: \t\t 8.25\n",
      "\n",
      "Entry: \t\t 7\n",
      "Title: \t\t The future of library makerspaces\n",
      "Author(s): \t Eric Johnson\n",
      "Abstract: \t None\n",
      "Year: \t\t 2017\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/286f4056fc25642ac9c04655e79b6d401/bibgreen\n",
      "Score: \t\t 8.25\n",
      "\n",
      "Entry: \t\t 8\n",
      "Title: \t\t What's next for library makerspaces\n",
      "Author(s): \t Tara M. Radniecki, Rebecca Glasgow, Nick Crowl\n",
      "Abstract: \t None\n",
      "Year: \t\t 2021\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/2f8e12ace7b45468309806279b6df36df/bibgreen\n",
      "Score: \t\t 8.25\n",
      "\n",
      "Entry: \t\t 9\n",
      "Title: \t\t The Right Time for Makerspaces in Nigerian Academic Libraries: Perceived Benefits and Challenges\n",
      "Author(s): \t Kingsley Efe OSAWARU | Angela Ishioma DIME | Emordi Herbert OKONJO\n",
      "Abstract: \t The present study investigates makerspaces in Nigerian academic libraries: perceived benefits and challenges. Four objectives guided the study: what constituted makerspace in academic library, the level of awareness of makerspace by academic library benefits and challenges of adopting makerspace in libraries. The descriptive survey design was adopted and questionnaire was used for data collection. The population of the study comprised of professional and paraprofessional librarians of Ambrose University and University of Benin, from which a sample size of 119 was drawn using total enumeration sampling technique. Out of the 119 copies of questionnaire administered, 94 were retrieved and analyzed using simple percentage and frequency tables. Findings revealed that, the respondents were aware of what constitute makerspace which are library space, 3D printers, computers and projector. It was also discovered that, the respondents had a high level of awareness of makerspace in the library. Some of the benefits associated with the use of makerspace are: it facilitates group interaction, it improves knowledge and provides access to wide varieties of tools and technology. Some of the challenges encountered in the adoption of makerspace are training of academic library staff, security of makerspace gadgets, poor funding, erratic power supply, high cost and maintenance of equipment. It was however recommended that; librarians should make deliberate effort to explore the potentials in makerspace in the enhancement of their services and training should be conducted regularly to enhance librarian’s skills in the use of ICTs Kingsley Efe OSAWARU, Angela Ishioma DIME and Emordi Herbert OKONJO 2020. The Right Time for Makerspaces in Nigerian Academic Libraries: Perceived Benefits and Challenges. International Journal on Integrated Education. 3, 10 (Oct. 2020), 103-115. DOI:https://doi.org/10.31149/ijie.v3i10.694 Pdf Url : https://journals.researchparks.org/index.php/IJIE/article/view/694/654 Paper Url : https://journals.researchparks.org/index.php/IJIE/article/view/694\n",
      "Year: \t\t 2020\n",
      "URL: \t\t https://journals.researchparks.org/index.php/IJIE/article/view/694\n",
      "Score: \t\t 8.23\n",
      "\n",
      "Entry: \t\t 10\n",
      "Title: \t\t Evaluating a Public Library Makerspace.\n",
      "Author(s): \t Pia Margaret Gahagan, Philip James Calvert\n",
      "Abstract: \t None\n",
      "Year: \t\t 2020\n",
      "URL: \t\t http://dblp.uni-trier.de/db/journals/plq/plq39.html#GahaganC20\n",
      "Score: \t\t 8.12\n",
      "\n",
      "Entry: \t\t 11\n",
      "Title: \t\t Challenges of library makerspaces and programs\n",
      "Author(s): \t Wendy Harrop\n",
      "Abstract: \t None\n",
      "Year: \t\t 2021\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/2d3d6de633b6448bea2a92a8f40f191c8/bibgreen\n",
      "Score: \t\t 8.12\n",
      "\n",
      "Entry: \t\t 12\n",
      "Title: \t\t Scaffolding of Learning in Library Makerspaces.\n",
      "Author(s): \t Árni Már Einarsson, Morten Hertzum\n",
      "Abstract: \t None\n",
      "Year: \t\t 2019\n",
      "URL: \t\t http://dblp.uni-trier.de/db/conf/fablearn/fablearn2019.html#EinarssonH19\n",
      "Score: \t\t 8.12\n",
      "\n",
      "Entry: \t\t 13\n",
      "Title: \t\t Sustainability: Keeping the library makerspace alive\n",
      "Author(s): \t Sharona Ginsberg\n",
      "Abstract: \t None\n",
      "Year: \t\t 2017\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/21c020a4a195d8f9dcad4ed295bfbf0e5/bibgreen\n",
      "Score: \t\t 7.98\n",
      "\n",
      "Entry: \t\t 14\n",
      "Title: \t\t Makerspaces: A beneficial new service for academic libraries?\n",
      "Author(s): \t Robert Curry\n",
      "Abstract: \t Purpose The purpose of this paper is to start exploring the possibilities for makerspaces to function as a new learning space within academic library services in higher education (HE). This original research study ask two key questions: How is learning achieved and supported in makerspaces? What can academic library services bring to the effective organisation and support of makerspaces? Design/methodology/approach An extensive literature review is followed by a template analysis (King, 2012) of data from an online forum of three professionals operating makerspaces in academic library services in the USA and a discussion incorporating relevant educational theory and philosophy. Findings The three overarching learning themes found were: experiential learning (Dewey, 1909; Kolb, 1984), communities of practice (Lave and Wenger, 1991) and self-efficacy through social learning (Bandura, 1997). Research limitations/implications The one-week forum of three professional library staff provided detailed and informative data. Substantial field work with students will also be required to see how far this professional lens has provided insight into how students are learning and supported in these and other makerspaces. Social implications The wider cultural implications are examined, including the potential social value of makerspaces as transformative creative spaces empowering communities and individuals. Originality/value This is the first study to date on the potential educational value of makerspaces within HE, and the specific support academic library services can offer if they choose to host a makerspace (including teaching information, digital and critical literacies).\n",
      "Year: \t\t 2017\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/2c1f2498fcee34b6fdc7caa2a97559d72/bibgreen\n",
      "Score: \t\t 7.90\n",
      "\n",
      "Entry: \t\t 15\n",
      "Title: \t\t Makerspaces: a beneficial new service for academic libraries?\n",
      "Author(s): \t Robert Curry\n",
      "Abstract: \t None\n",
      "Year: \t\t 2017\n",
      "URL: \t\t http://www.emeraldinsight.com/doi/10.1108/LR-09-2016-0081\n",
      "Score: \t\t 7.85\n",
      "\n",
      "Entry: \t\t 16\n",
      "Title: \t\t Library makerspaces: The complete guide\n",
      "Author(s): \t Chuck Stephens, Jeroen de Boer, Steve Willingham\n",
      "Abstract: \t The Complete Guide is a road map for libraries of any size, with any budget, seeking to redesign or repurpose space or develop maker style programming. This book covers developing makerspaces, writing grant proposals, and helping staff and administrators learn about the technologies and processes involved.\n",
      "Year: \t\t 2018\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/2a0df1a220a8dafd5c51580602b67eea8/bibgreen\n",
      "Score: \t\t 7.78\n",
      "\n",
      "Entry: \t\t 17\n",
      "Title: \t\t Urban gardening, foodsharing and makerspaces: Best practice in the Stadtbibliothek of Bad Oldesloe, Germany\n",
      "Author(s): \t Tim Schumann\n",
      "Abstract: \t Public libraries in Germany are currently under pressure, with many closures recently due to reduced municipal budgets. Public libraries hence need to demonstrate relevance to their communities, sponsors and other accountable bodies to avoid the continuance of this trend. This paper shows how libraries can link into debates in society and politics by adopting the urban gardening trend as well as developing new services around this green movement. Within the debate on public libraries’ transformation, urban gardening and the idea of the green library provide an opportunity to develop new green and sustainable library services. By combining makerspaces and community building with urban gardening and foodsharing, the public library of Bad Oldesloe developed a series of events that turned the library into a modern and creative learning space. At the same time, the library offered its physical space as a community platform for networking, cooperation and creativity.\n",
      "Year: \t\t 2018\n",
      "URL: \t\t https://www.bibsonomy.org/bibtex/26d02c1e91680130a4860896cd54f0891/bibgreen\n",
      "Score: \t\t 7.76\n",
      "\n",
      "Entry: \t\t 18\n",
      "Title: \t\t MakeAbility: Creating Accessible Makerspace Events in a Public Library.\n",
      "Author(s): \t Tara Brady, Camille Salas, Ayah Nuriddin, Walter Rodgers, Mega Subramaniam\n",
      "Abstract: \t None\n",
      "Year: \t\t 2014\n",
      "URL: \t\t http://dblp.uni-trier.de/db/journals/plq/plq33.html#BradySNRS14\n",
      "Score: \t\t 7.73\n",
      "\n",
      "Entry: \t\t 19\n",
      "Title: \t\t Facilitation in library makerspaces: a prototype for a professional development model.\n",
      "Author(s): \t Leanne Bowler, Tom Akiva, Sharon Colvin, Annie McNamara\n",
      "Abstract: \t None\n",
      "Year: \t\t 2019\n",
      "URL: \t\t http://dblp.uni-trier.de/db/journals/ires/ires24.html#BowlerACM19\n",
      "Score: \t\t 7.73\n",
      "\n",
      "Entry: \t\t 20\n",
      "Title: \t\t Continuing education and professional development of library staff involved with makerspaces.\n",
      "Author(s): \t Jennifer Horton\n",
      "Abstract: \t None\n",
      "Year: \t\t 2019\n",
      "URL: \t\t http://dblp.uni-trier.de/db/journals/lht/lht37.html#Horton19\n",
      "Score: \t\t 7.60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_search_engine(query='library makerspaces', \n",
    "                    data_dict=data_dict,\n",
    "                    search_engine=tf_idf,\n",
    "                    limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df as pickle\n",
    "with open('./bibsonomy_clean_data/makerspace_vr.pkl', 'wb') as fout:\n",
    "    pickle.dump(df_filtered, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Streamlit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in ./venv/lib/python3.8/site-packages (1.30.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in ./venv/lib/python3.8/site-packages (from streamlit) (5.2.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in ./venv/lib/python3.8/site-packages (from streamlit) (1.7.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in ./venv/lib/python3.8/site-packages (from streamlit) (5.3.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in ./venv/lib/python3.8/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata<8,>=1.4 in ./venv/lib/python3.8/site-packages (from streamlit) (7.0.1)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in ./venv/lib/python3.8/site-packages (from streamlit) (1.24.4)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in ./venv/lib/python3.8/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in ./venv/lib/python3.8/site-packages (from streamlit) (2.0.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in ./venv/lib/python3.8/site-packages (from streamlit) (10.2.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in ./venv/lib/python3.8/site-packages (from streamlit) (4.25.2)\n",
      "Requirement already satisfied: pyarrow>=6.0 in ./venv/lib/python3.8/site-packages (from streamlit) (14.0.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in ./venv/lib/python3.8/site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./venv/lib/python3.8/site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in ./venv/lib/python3.8/site-packages (from streamlit) (13.7.0)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in ./venv/lib/python3.8/site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in ./venv/lib/python3.8/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in ./venv/lib/python3.8/site-packages (from streamlit) (4.9.0)\n",
      "Requirement already satisfied: tzlocal<6,>=1.1 in ./venv/lib/python3.8/site-packages (from streamlit) (5.2)\n",
      "Requirement already satisfied: validators<1,>=0.2 in ./venv/lib/python3.8/site-packages (from streamlit) (0.22.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in ./venv/lib/python3.8/site-packages (from streamlit) (3.1.41)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in ./venv/lib/python3.8/site-packages (from streamlit) (0.8.1b0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in ./venv/lib/python3.8/site-packages (from streamlit) (6.4)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in ./venv/lib/python3.8/site-packages (from altair<6,>=4.0->streamlit) (4.21.1)\n",
      "Requirement already satisfied: toolz in ./venv/lib/python3.8/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/lib/python3.8/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.8/site-packages (from importlib-metadata<8,>=1.4->streamlit) (3.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.8/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.8/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.27->streamlit) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.8/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.8/site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\n",
      "Requirement already satisfied: backports.zoneinfo in ./venv/lib/python3.8/site-packages (from tzlocal<6,>=1.1->streamlit) (0.2.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./venv/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./venv/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in ./venv/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (6.1.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in ./venv/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.17.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://134.155.49.114:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n",
      "2024-01-31 11:16:36.733 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 531, in _run_script\n",
      "    self._session_state.on_script_will_rerun(rerun_data.widget_states)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/safe_session_state.py\", line 63, in on_script_will_rerun\n",
      "    self._state.on_script_will_rerun(latest_widget_states)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 504, in on_script_will_rerun\n",
      "    self._call_callbacks()\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 517, in _call_callbacks\n",
      "    self._new_widget_state.call_callback(wid)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 261, in call_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/pyterrier_web_interface.py\", line 23, in search\n",
      "    res = st.session_state[\"engine\"].search(query)[:search_limit]\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/pandas/core/frame.py\", line 3735, in __getitem__\n",
      "    indexer = self.index._convert_slice_indexer(key, kind=\"getitem\")\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 4102, in _convert_slice_indexer\n",
      "    self._validate_indexer(\"slice\", key.stop, \"getitem\")\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 6376, in _validate_indexer\n",
      "    self._raise_invalid_indexer(form, key)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 4152, in _raise_invalid_indexer\n",
      "    raise TypeError(msg)\n",
      "TypeError: cannot do slice indexing on RangeIndex with these indexers [(1, 7)] of type tuple\n",
      "2024-01-31 11:20:43.949 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 531, in _run_script\n",
      "    self._session_state.on_script_will_rerun(rerun_data.widget_states)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/safe_session_state.py\", line 63, in on_script_will_rerun\n",
      "    self._state.on_script_will_rerun(latest_widget_states)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 504, in on_script_will_rerun\n",
      "    self._call_callbacks()\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 517, in _call_callbacks\n",
      "    self._new_widget_state.call_callback(wid)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 261, in call_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/pyterrier_web_interface.py\", line 26, in search\n",
      "    for idx, _, row in enumerate(res.iterrows()):\n",
      "ValueError: not enough values to unpack (expected 3, got 2)\n",
      "2024-01-31 11:24:58.604 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 531, in _run_script\n",
      "    self._session_state.on_script_will_rerun(rerun_data.widget_states)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/safe_session_state.py\", line 63, in on_script_will_rerun\n",
      "    self._state.on_script_will_rerun(latest_widget_states)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 504, in on_script_will_rerun\n",
      "    self._call_callbacks()\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 517, in _call_callbacks\n",
      "    self._new_widget_state.call_callback(wid)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 261, in call_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/pyterrier_web_interface.py\", line 27, in search\n",
      "    score = round(row['score'], 2)\n",
      "TypeError: tuple indices must be integers or slices, not str\n",
      "2024-01-31 11:25:23.206 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 531, in _run_script\n",
      "    self._session_state.on_script_will_rerun(rerun_data.widget_states)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/safe_session_state.py\", line 63, in on_script_will_rerun\n",
      "    self._state.on_script_will_rerun(latest_widget_states)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 504, in on_script_will_rerun\n",
      "    self._call_callbacks()\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 517, in _call_callbacks\n",
      "    self._new_widget_state.call_callback(wid)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 261, in call_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/pyterrier_web_interface.py\", line 26, in search\n",
      "    for s, _, row in enumerate(res.iterrows()):\n",
      "ValueError: not enough values to unpack (expected 3, got 2)\n",
      "2024-01-31 11:32:11.817 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 531, in _run_script\n",
      "    self._session_state.on_script_will_rerun(rerun_data.widget_states)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/safe_session_state.py\", line 63, in on_script_will_rerun\n",
      "    self._state.on_script_will_rerun(latest_widget_states)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 504, in on_script_will_rerun\n",
      "    self._call_callbacks()\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 517, in _call_callbacks\n",
      "    self._new_widget_state.call_callback(wid)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/venv/lib/python3.8/site-packages/streamlit/runtime/state/session_state.py\", line 261, in call_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/thomas/Coding/data-librarian/2023-2024_Data_Librarian_Thomas_Schmidt/Modul_2/pyterrier-tut/pyterrier_web_interface.py\", line 23, in search\n",
      "    res = st.session_state[\"engine\"].search(query)[:search_limit[1]]\n",
      "TypeError: 'int' object is not subscriptable\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run pyterrier_web_interface.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
